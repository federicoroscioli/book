% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{svmono}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
\fi
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{caption}
\usepackage{ragged2e}
\usepackage{fancyhdr}
\usepackage[top=2.5cm, bottom=3cm, left=2.5cm, right=2.5cm, headheight=17pt, includehead, includefoot, heightrounded]{geometry}
\pagestyle{fancy}
\fancyhead[LO,RE]{Introduction to Data Analysis with R}
\fancyhead[LE,RO]{}
\fancyfoot[C]{}
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[LO,RE]{Last updated 22 October, 2023}
\usepackage{float}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Data Analysis with R},
  pdfauthor={Federico Roscioli},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Introduction to Data Analysis with R}
\author{Federico Roscioli}
\date{22 October, 2023}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

The real goal of this manual is not to teach the R language per se, but
to allow the students to manage the basic concepts in order to be able
to explore and analyze data using R. After this course the student will
be capable of clearly understand the R code that someone else wrote and
customize her own code according to the need. This means having the
possibility to explore different and more complex solutions for the
student's problems by exploring new packages and paving the way to
become a statistician!

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

I also created also an ad-hoc \textbf{R playground} accessible
\href{https://federicoroscioli.shinyapps.io/exercises/}{here}. The R
playground allows the student to exercise in order to reinforce her
knowledge of R and data analysis. Exercises are fundamental in order to
fix the knowledge acquired in class. The platform is structured in the
same way as this manual in order to have a linear learning process. I
finally want to remark that the code I provide is ``my best and easiest
version of the solution'', I hope you will appreciate it. In fact, with
R, it is possible to do the same thing in 1000 different ways, and by
looking at the internet you can have a confirmation of it.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

This website is free to use, and is licensed under the GNU General
Public License 2.0. If you'd like a pdf copy of the book, you can
download it
\href{https://github.com/federicoroscioli/book/blob/2b55c0f10dac709bf83a6edc26c7175128c34cd3/docs/_main.pdf}{here}.

\hypertarget{author}{%
\section*{Author}\label{author}}
\addcontentsline{toc}{section}{Author}

\textbf{Federico Roscioli} is a PhD Student in Economics and Finance at the
University of Rome Tor Vergata. He has an international experience as a
consultant for various International Organizations and Research
Institutes and a background as freelance photojournalist. His research
interests include: development issues, environmental sustainability,
poverty, social protection, food security and nutrition.

\href{https://www.federicoroscioli.com/}{www.federicoroscioli.com}

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

This book was written following the teachings of Matteo Mazziotta (ISTAT).

\let\thefootnote\relax\footnotetext{Book cover image by \copyright Federico Roscioli.}

\newpage

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\justifying

\begin{quote}
\emph{I want to do something, not learn how to do everything.}\\
John Carroll, ``Minimal manual''
\end{quote}

Many people think of R as a statistics system. I prefer to consider it
an environment within which statistical techniques are implemented. R
can be extended (easily) via packages allowing to execute various
statistical and graphical techniques, including linear and nonlinear
modeling, classical statistical tests, spatial and time-series analysis,
classification, clustering, and others.

\textbf{If this is your first time coding}, R may be slow and tedious at the
beginning. You will pass a lot of your time trying to understand why R
is giving you an error instead of the solution to your problem. \href{https://stackoverflow.com}{Stack
Overflow} and the R Help will become your
best friends. The software is going to seem to judge your mistakes
harshly. You will read the word ``syntax error'' a lot. And you will
probably grow a kind of hate for the creature that lives inside the
computer and that does not like you. You will think the creature is
value judging your code. You will crash the laptop and loose all your
data\ldots{} Ok, ok! I'm a bit exaggerating\ldots{} I don't want to scare you too
much, but yes, this is the worst case scenario and many of your
colleagues already passed through that, and survived!

\begin{center}\includegraphics[width=0.8\linewidth,]{images/8psue2xkvn811} \end{center}

The fact is that, R is a language and, as such, should be practiced as
much as possible in order to learn it. But R is ``only'' a coding
language, so you will not be not able to speak \emph{Rish} with your friends
(fortunately!) nor write a letter to your pals. The only option you have
is to practice it with your computer (also in group if you prefer). All
the syntax errors you will get do not mean that R thinks you're bad. It
is not a judgment of your ability as a statistician. Syntax errors mean
R is lost and doesn't have better words to tell you that. Beside the
fact that it can allow you to do marvelous things, R understands only
when you talk to him in his language. So, my suggestion when you get an
error is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Read out loud the error and try to make sense of where the problem
  is (usually R suggest you that).
\item
  Check carefully your code, there may be a missing comma or a
  misspelled capital letter.
\item
  Copy paste the error on google and look for answers. This works more
  than what you may think.
\item
  \href{mailto:info@federicoroscioli.com}{Contact me}.
\end{enumerate}

The above steps almost always work, but in general I want you to never
loose your effort. You will figure it out, be sure about it. You only
need to build some experience first. It is like when you learn to ride a
bicycle, at the beginning you have to fall a lot, but in the end you
will love cycling and you will never want to get off your bike.

As a final wish for you: during my first contact with R, I was ``lost'',
``lost'', ``lost'', ``lost'', ``lost'', ``oh wow, I love this!''. And I hope that
all of you will go through that exact same feeling.

So, please, \textbf{have fun, be curious, and exercise a lot!}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

This manual is composed by two main sections divided in chapters. The
first section will introduce the reader to the basic characteristics of
the software required, its installation and configuration, it will then
have a linguistic focus, so it aims at teaching how to use R in order to
visualize, explore, understand, manipulate and create data. It will do
so in a progressive way, starting from simple computations and ending
with complex data manipulation and plotting. The second section,
instead, will be all about applied statistics, from exploratory
analysis, to the introduction of hypothesis testing and its
implications, to bivariate and multivariate analysis methods, such as
principal component analysis and classification and regression trees.

\hypertarget{part-the-r-code}{%
\part{The R code}\label{part-the-r-code}}

\hypertarget{installation}{%
\chapter{Installation}\label{installation}}

~

~

~

\hypertarget{introductory-activities}{%
\section{Introductory activities}\label{introductory-activities}}

First of all we need to install the software that will allow us to work
with the R language (\protect\hyperlink{ref-rcoreteam2022}{R Core Team 2022}). We will need to install two
software: R and R-Studio. The first is a compiler for the R language, we
need to install it but we will never use it directly. The latter is an
interface software that runs on top of R and allows us to have some
facilitation and suggestions while working. Within this manual I will
use R and R-Studio as synonyms, but I will be referring always to the
use of R-Studio.

Additional to R and R-Studio we will need to install some packages. The
packages are extension of the software that bring in additional
functions and/or data.

To install the software, follow all the 9 steps below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download the R installer from \href{https://cran.r-project.org/}{CRAN}.\\
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/cran} 

}

\caption{Select the version of R according to your operating system.}\label{fig:unnamed-chunk-2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Run the installer keeping the default settings. If you do not have admin rights on your computer, please ask you IT Support to give you full permissions to the R directories. Otherwise you will not be able to install packages afterwards.
\item
  Download the R-Studio installer from \href{https://rstudio.com/products/rstudio/download/\#download}{R-Studio}.\\
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/rstudio} 

}

\caption{Select the version of R-Studio according to your operating system.}\label{fig:unnamed-chunk-3}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Once the installation of R is completed (NOT BEFORE), run the R-Studio installer keeping the default settings.
\item
  Run R-Studio. It should open an window like the one in the image below.\\
\end{enumerate}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/rstudio2} 

}

\caption{R-Studio.}\label{fig:unnamed-chunk-4}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  In the left hand window, by the sign ``\textgreater{}'', type ``4+5'' (without the quotes) and hit enter. An output line reading ``\([1]\) 9'' should appear. This means that R and R-Studio are working properly. If this is not successful, please \href{\%5Bmailto:\%20info@federicoroscioli.com\%5D(mailto:\%20info@federicoroscioli.com)\%7B.uri\%7D}{contact me}.
\item
  Go to Tools -\textgreater{} Install Packages and install the packages:``gt'', and ``readxl''. See the image below.\\
\end{enumerate}

\begin{center}\includegraphics[width=0.8\linewidth,]{images/pkg1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Check that the packages are installed by typing ``library(gt)'' (without the quotes) in the prompt and press enter.\\
\end{enumerate}

\begin{center}\includegraphics[width=0.8\linewidth,]{images/pkg2} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Finally type ``sessioninfo()'' (without the quotes) and check that gt has been installed.\\
\end{enumerate}

\begin{center}\includegraphics[width=0.8\linewidth,]{images/pkg3} \end{center}

~

~

~

~

\hypertarget{visualization-suggestions}{%
\section{Visualization suggestions}\label{visualization-suggestions}}

Following some visualization suggestions that you may explore.
Personally, I find them really helpful.

\begin{itemize}
\item
  Setting the work-space:

  \begin{itemize}
  \tightlist
  \item
    View -\textgreater{} Panes -\textgreater{} Panes Layout\\
  \item
    clockwise from top-left you should have: Source, Environment,
    Files, Console
  \end{itemize}
\item
  Setting the color style of the code:

  \begin{itemize}
  \tightlist
  \item
    Tools \textgreater{} Global Options -\textgreater{} Appearance -\textgreater{} Editor Theme -\textgreater{}
    Xcode
  \end{itemize}
\end{itemize}

~

~

~

~

\hypertarget{the-workspace}{%
\section{The workspace}\label{the-workspace}}

\begin{center}\includegraphics[width=0.8\linewidth,]{images/workspace} \end{center}

The \textbf{source} is a text file with extension .R that can be saved and
opened from every version of R and R-Studio. This file will allow us to
run and rerun a bunch of code, modify some details if we made a mistake
or if we want to change something. This will always be our best friend.

The \textbf{Environment} is the place where R saves temporarily all the data
that we tell ``him'' to save. The environment will never be clean or
contain only the essential objects you need (maybe this will happen when
you will be a great programmer, but not for now). We will see there all
your data-sets, variables, vectors, etc\ldots{}

The bottom right part of the screen is devoted to many things.
\textbf{Viewer}, \textbf{Plots} and \textbf{Help} will be activated automatically to
show you the requested output. \textbf{Packages}, instead, is useful only
when we have to install new packages.

The \textbf{Console} is where we can write some code that will not be saved,
if not in the temporary history of the console itself. This space is
also where R give us the feedback of our inputs in the form of results,
warnings and errors.

\newpage

\hypertarget{a-b-c}{%
\chapter{A, B, C}\label{a-b-c}}

~

~

~

\hypertarget{the-first-code}{%
\section{The first code}\label{the-first-code}}

We can start exploring the big potential of R by typing some small basic
mathematical operations. My suggestion is to always work on a source
file\footnote{If you don't know what it means, please, review the \protect\hyperlink{installation}{Installation}
  chapter.} and run your commands line by line using the ``Run'' button (or
Control+Enter on Windows, Command+Enter on Mac).

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\DecValTok{2} \SpecialCharTok{*} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

Now we can create some ``objects''. This means storing numbers or series
of numbers within the memory of our computer (the Environment), and we
will be able to use these objects for our analysis. We can also create a
vector by assigning more than one value to an object. For this purpose,
we will need to use the combine function \texttt{c()} and put between the
parentheses all the values separated by a comma. If what we want to
create is a vector comprising an ordered series of integer numbers,
there is a shortcut: we can use the colon symbol between the starting
and the ending numbers. We can also do operations between values and
objects. \textbf{Remember that if we assign to the same object another
content, we will loose the original one. In R there is no ``undo''
button.}

Below we can see the syntax used for this scope. As you can see, within
the code there are some comments. Comments are a great tool to allow us
to understand what the code stands for, and let people (and us)
understand it better. By placing one (or more) hashtag (\#) within the
code, we will transform the whole line into a comment and R will skip
that line from running.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# = and \textless{}{-} are the same, but I prefer \textless{}{-} }
\NormalTok{x }\OtherTok{=} \DecValTok{3}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{5}

\CommentTok{\# with the == sign you ask R if a condition is true}
\NormalTok{x }\SpecialCharTok{==} \DecValTok{7}

\CommentTok{\# single value assignment}
\NormalTok{x }\OtherTok{\textless{}{-}} \StringTok{"hello"}

\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \DecValTok{5}
\NormalTok{x}

\CommentTok{\# vector assignment}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{y }\SpecialCharTok{==} \DecValTok{4}

\CommentTok{\# character vector}
\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"hello"}\NormalTok{, }\StringTok{"how"}\NormalTok{, }\StringTok{"are you"}\NormalTok{, }\StringTok{"?"}\NormalTok{)}
\NormalTok{t}

\CommentTok{\# oredered integer series}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The notations \texttt{\textless{}-} and \texttt{=} are equivalent in assigning values to an
object, however my suggestion is to always use the former to assign
values, in order to avoid confusion with the equal sign. As you can see,
when we store something in the Environment, R is not showing it to us.
We will need to ``call'' it in order to see what is inside that object.

Read the code below by yourself, and guess the result before running the
code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# operations}
\NormalTok{x }\SpecialCharTok{*} \DecValTok{5}

\CommentTok{\# what will happen here?}
\NormalTok{x }\SpecialCharTok{*}\NormalTok{ y}
\NormalTok{x }\SpecialCharTok{{-}}\NormalTok{ y}
\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{indexing}{%
\section{Indexing}\label{indexing}}

The indexing system allows us to select a specific part of our data. To
do so, we need to first call the name of the object we want to search
(\texttt{y}), and then put the searching instruction between squared
parenthesis \texttt{{[}\ {]}} right after. For example, selecting the first, second
and fourth value \texttt{y{[}c(1,2,4){]}}; or all the values bigger than 3
\texttt{y{[}y\textgreater{}3{]}}, or we can give multiple conditions together \texttt{y{[}y\textless{}3\ \&\ y\textgreater{}10{]}},
and so on. Another common and useful instruction is to select all the
values in one object \(y\) that are present in another object \(z\)
\texttt{y{[}y\ \%in\%\ z{]}}.\footnote{For a complete list of please check within the references of this
  chapter.}

Try the following code by yourself and guess the result before running
it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\SpecialCharTok{:}\DecValTok{70}\NormalTok{)}
\CommentTok{\# partial selection}
\NormalTok{y[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{15}\NormalTok{)]}
\NormalTok{y[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ y[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{\# subtractive selection}
\NormalTok{y[}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]}
\NormalTok{y[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}

\CommentTok{\# assigning values only to a subset of observations}
\NormalTok{y[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{15}\NormalTok{)] }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{y}

\CommentTok{\# equality selection}
\NormalTok{y[y}\SpecialCharTok{==}\DecValTok{3}\NormalTok{]}
\NormalTok{y[y}\SpecialCharTok{==}\DecValTok{3} \SpecialCharTok{\&}\NormalTok{ y}\SpecialCharTok{==}\DecValTok{45}\NormalTok{]}
\NormalTok{y[y}\SpecialCharTok{==}\DecValTok{3} \SpecialCharTok{|}\NormalTok{ y}\SpecialCharTok{==}\DecValTok{45}\NormalTok{]}

\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\SpecialCharTok{:}\DecValTok{50}\NormalTok{)}
\NormalTok{y[y }\SpecialCharTok{\%in\%}\NormalTok{ z]}
\end{Highlighting}
\end{Shaded}

I recognize that the indexing system is not immediate and may result
confusing to the majority. My suggestion is to exercise a lot with it.
Don't be shy in creating objects of all sorts and select whatever you
want. Later in this chapter and in \protect\hyperlink{advanced-data-manipulation-and-plotting}{Advanced Data Manipulation and
Plotting} we will see two more friendly methods for sub-setting (the
\texttt{subset()} function and the package \texttt{dplyr}), but, please, \textbf{do not
underestimate the power of the indexing system}.

Now that we master numeric vectors, is time to learn that R is able to
manage many different types of data. More precisely a vector can be:
\emph{character}, \emph{numeric} (real or decimal), \emph{integer}, \emph{logical}, or
\emph{complex}. The most used vectors are however, \emph{character} (like the one
called \emph{t} in our first code chunk), \emph{numeric} and \emph{logical} (TRUE or
FALSE). Another interesting type is the \emph{factor} vector, but we will see
it in action in \protect\hyperlink{data-cleaning}{Data Cleaning}. \textbf{Be aware that if a vector is composed
by both characters and numbers, it will be considered automatically as a
character vector! And this applies also if our numbers have a comma
instead of a dot before decimals!}

~

~
~

\hypertarget{the-first-function}{%
\section{The first function}\label{the-first-function}}

The first R function of our life is the mean. Before running it I
suggest to explore the Help documentation for this function by typing in
the console \texttt{?mean}. Within the help page, that will appear in the right
side of our work-space, we will find all the instructions for how to use
the function. Those ``options'' specified within the normal parenthesis
\texttt{()} are called the arguments. Some of them have a default value and
some others are, instead needed in order to run the function. In this
case only \texttt{x} is needed: the object of which we want to compute the
mean. The help provides an exhaustive explanation of how each function
works, plus it cites some references and examples. Most importantly, the
help will be available for any function or package, and, in 99\% of the
cases, it will provide you with all the above mentioned information.

Functions works by calling them (be aware that R is case sensitive) and
putting between normal parenthesis the required arguments.

Try the following code by yourself and try to guess the result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(y)}
\FunctionTok{mean}\NormalTok{(z)}

\CommentTok{\# what will happen here?}
\FunctionTok{mean}\NormalTok{(y[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}

\CommentTok{\# calculate the mean manually}
\FunctionTok{sum}\NormalTok{(y)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(y)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(p)}
\CommentTok{\# why is it NA?}

\FunctionTok{mean}\NormalTok{(p, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# check in the help what na.rm does!}
\FunctionTok{mean}\NormalTok{(p[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(p)])}

\CommentTok{\# we can see how many values p has, and how many of them are NA (Please note that }
\CommentTok{\#Â for NA we do not use the normal equality expressions)}
\FunctionTok{length}\NormalTok{(p)}
\FunctionTok{length}\NormalTok{(p[}\FunctionTok{is.na}\NormalTok{(p)])}

\CommentTok{\# the function class tells us which is the type of data in the vector}
\FunctionTok{class}\NormalTok{(y)}
\FunctionTok{class}\NormalTok{(t)}
\end{Highlighting}
\end{Shaded}

What happens when we have to compute the mean of a vector with some
missing observations (NA)? Missing observations are not zeros, so the
sum cannot be computed. For this reason, if we add the \texttt{na.rm=TRUE}
argument to our mean function, R strips the missing observations before
applying the function, and gives us a result. \textbf{Be aware that by
stripping the missing observation, the vector will have a smaller
length, this, may have some implications if we want the per-capita
average for example.}

~

~
~

\hypertarget{dataset-exploration}{%
\section{Dataset Exploration}\label{dataset-exploration}}

A dataset is a matrix \(r*c\), with \(r\) rows representing the observations
and \(c\) columns representing the variables. In other words, a dataset is
composed by \(r\) horizontal vectors, or \(c\) longitudinal vectors. Having
this in mind, we can now load a dataset and start exploring its
dimensions.

For this scope we will use a dataset embedded into R that is called
\texttt{mtcars}. In order to better understand what \texttt{mtcars} refers to, type
\texttt{?mtcars} in the Console and read carefully what the help says about it.

I know that working with data about car design and performances
belonging to 1973 is not your favorite hobby, but this R dataset is
cleaned, ready to use, and has an amazing documentation attached, so
perfect for our learning purposes.

The following code presents us some exploratory activities that should
be carried out when we get our hands on some new data in order to
understand its shape.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading an internal dataset}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}

\CommentTok{\# structure of the dataset}
\FunctionTok{str}\NormalTok{(mtcars)}

\CommentTok{\# visualizing the first 6 rows and the last 6}
\FunctionTok{head}\NormalTok{(mtcars)}
\FunctionTok{tail}\NormalTok{(mtcars)}

\CommentTok{\# variables names}
\FunctionTok{colnames}\NormalTok{(mtcars)}

\CommentTok{\# number of rows and number of columns}
\FunctionTok{nrow}\NormalTok{(mtcars)}
\FunctionTok{ncol}\NormalTok{(mtcars)}
\FunctionTok{dim}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

We first explore the internal structure of the dataset (\texttt{str(mtcars)}),
meaning: which kind of dataset it is (yes, there are multiple types),
the number of observations (rows) and variables (columns), and the name
and type of each variable with a small preview. A dataset could be a
``matrix'', a ``data frame'', a ``thible'', etc\ldots{} I will not go through the
details of each type of dataset, my suggestion is to always use a data
frame, as this is one of the most flexible and complete form of
bi-dimensional data. In order to convert to data frame whatever kind of
dataset, use the \texttt{as.data.frame()} function.\footnote{This is not applicable to mtcars, as it is already a data frame,
  but we will see this function in action later on in the book.}

Another important skill to acquire is the capability to create a dataset
from zero. Again, there exist millions of ways to do it. Below some
examples to create the same dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# by specifying it columnwise}
\NormalTok{people }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{, }\StringTok{"Mike"}\NormalTok{, }\StringTok{"Greg"}\NormalTok{),}
                     \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{44}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{46}\NormalTok{),}
                     \AttributeTok{IQ =} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{110}\NormalTok{))}

\CommentTok{\# by creating some vectors and then binding them togheter column{-}wise}
\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{, }\StringTok{"Mike"}\NormalTok{, }\StringTok{"Greg"}\NormalTok{)}
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{44}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{46}\NormalTok{)}
\NormalTok{IQ }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{110}\NormalTok{)}
\NormalTok{people2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(name, age, IQ))}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{subsetting}{%
\section{Subsetting}\label{subsetting}}

Now that we can create complex objects, we need to be able to ``destroy''
them, meaning split them into subsets according to some interesting
characteristics (\protect\hyperlink{ref-james2021}{James et al. 2021}). Sub-setting is the heart of data
manipulation and the basis for data analysis. Via subsetting I can
analyze the data of only one group of observations within my dataset
according to some interesting characteristics we define. As an example,
the consumption (mpg) of manual cars vs automatic cars.

To do so, we will use the same indexing expressions we used in the
\protect\hyperlink{indexing}{Indexing} chapter, but this time we will have to specify two dimensions
(rows and columns) separated by a comma within the squared brackets
\texttt{{[}rows,columns{]}}. \textbf{Remember that if we do not specify either rows or
columns, R will consider the whole set of the corresponding dimension.}

Explore the code below. Use the \texttt{str()} function, specifying the new
object created in each line of code, in order to understand what
happened.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to subset we use [rows,columns]}
\NormalTok{a }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{]}
\FunctionTok{str}\NormalTok{(a)}

\CommentTok{\# what will happen here?}
\NormalTok{b }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[}\DecValTok{3}\NormalTok{,]}
\FunctionTok{str}\NormalTok{(b)}

\CommentTok{\# and here?}
\NormalTok{c }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[,}\DecValTok{3}\NormalTok{]}
\FunctionTok{str}\NormalTok{(c)}

\CommentTok{\# we want to see the first 5 rows and the column from 3 to 8}
\NormalTok{mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

If we want to subset one single column from a dataset we can use the \$
sign.\footnote{This is equivalent to subset using the index number corresponding
  to the same column (see the code below).} This opens up opportunities for specifying some peculiar
characteristics that we want to retain in our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to call a singe variable we use the $ sign}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl}
\CommentTok{\# it is the same as}
\NormalTok{mtcars[,}\DecValTok{2}\NormalTok{]}

\CommentTok{\# equality subsetting: we want all the data of the cars with 4 cylinders}
\NormalTok{mtcars[mtcars}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{==} \DecValTok{4}\NormalTok{,]}

\CommentTok{\# we want the values of the firt clumn (mpg) of cars with 4 cylinders}
\NormalTok{mtcars[mtcars}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{==} \DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\CommentTok{\# it is the same as}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{==} \DecValTok{4}\NormalTok{] }\CommentTok{\# I prefer this}

\CommentTok{\# subsetting consumption per type of transmission}
\CommentTok{\# automatic cars}
\NormalTok{mpg.at }\OtherTok{\textless{}{-}}\NormalTok{ mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{am }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}
\CommentTok{\# manual cars}
\NormalTok{mpg.mt }\OtherTok{\textless{}{-}}\NormalTok{ mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{am }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

There is also an easier (more discursive) way: the \texttt{subset()} function.
This function takes 3 arguments: the data frame we want subsetted, the
rows corresponding to the condition by which we want it subsetted, and
the columns we want returned. The argument \texttt{drop=TRUE} allows us to drop
the row names and have a vector as final output. Of course we can input
multiple conditions and select multiple columns.

The code below leads to the same result as in the last lines of the
previous code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subsetting consumption per type of transmission}
\NormalTok{mpg.at2 }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(mtcars, am }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\AttributeTok{select =} \StringTok{"mpg"}\NormalTok{, }\AttributeTok{drop=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{mpg.mt2 }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(mtcars, am }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\AttributeTok{select =} \StringTok{"mpg"}\NormalTok{, }\AttributeTok{drop=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# multiple conditions and columns}
\NormalTok{mix }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(mtcars, am}\SpecialCharTok{==}\DecValTok{0} \SpecialCharTok{\&}\NormalTok{ cyl}\SpecialCharTok{==}\DecValTok{4}\NormalTok{, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"hp"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{importing-and-exporting-data}{%
\section{Importing and exporting data}\label{importing-and-exporting-data}}

Of course none expects us to work only with internal datasets, nor to
keep the data only on our computer, so we need to know how to import and
export files containing data. While R is a super powerful tool for
cleaning the data, it is important that the data we import follow at
least the basic rule of one observation per row and one variable per
column (meaning that there cannot be variables such as: ``income2021'',
``income2020'', etc; but one column for the years and one for the income).
More rules on how to clean the data are available in \protect\hyperlink{data-cleaning}{Data Cleaning}.

The first thing to do when working with external files is to set the
working directory within the code (finction \texttt{setwd()}). This will tell R
where to look for files and where to put the new ones. Imaging that we
will have to share our code with someone else, that person will have
only to change the working directory path before running our code
successfully.

In order to import external data into R, we should go to File -\textgreater{} Import
Dataset and select the format of the dataset to import. R can read by
itself the most common data formats (text based like csv or txt, Excel,
Stata, SAS, SPSS)(\protect\hyperlink{ref-wickham2022}{Wickham and Bryan 2022}), but through the use of packages we can
extend its importing capacity to spacial data, or other formats . After
having selected the appropriate format, a new window will appear asking
to select the file and set the options you need in order to have it read
properly. Finally, my suggestion is to copy the code that will appear in
the Console and paste it to the Source. This way we will be able to
reload the file anytime without losing time into windows and clicks.

We have seen in the \protect\hyperlink{installation}{Installation} chapter how to install packages,
however, in order to avoid overloading our computers, R activates only a
small set of default packages when we open it. This means that before
using any content (function, data, object) of an additional package, we
need to activate it using the function \texttt{library()}. Once the package has
been activated, it will remain active for the whole session of work, so
there is no need to call it again.

During my teaching career, I have seen many students having a huge list
of packages called at the beginning of their code (most of them useless
for their purposes). This method allowed them to avoid remembering what
was the purpose of each package, but also overcharged their computers
and often leaded to software crashes and data losses. So, please, avoid
it!

Download the \href{http://www.federicoroscioli.com/wp-content/uploads/2021/02/Wine.csv}{wine dataset}, change the directory below with the folder where you put the downloaded
file, and try to import it (it is a text based csv file). Try the same with the \href{http://www.federicoroscioli.com/wp-content/uploads/2022/03/Village.xlsx}{village dataset}, be aware that this one is in Excel format.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# setting the working directory}
\FunctionTok{setwd}\NormalTok{(}\StringTok{"/Users/federicoroscioli/Desktop"}\NormalTok{)}
\CommentTok{\# or setting the working directory as the same where the Source file is stored}
\FunctionTok{setwd}\NormalTok{(}\FunctionTok{dirname}\NormalTok{(rstudioapi}\SpecialCharTok{::}\FunctionTok{getActiveDocumentContext}\NormalTok{()}\SpecialCharTok{$}\NormalTok{path))}

\CommentTok{\# for text based datasets (csv)}
\NormalTok{wine }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Wine.csv"}\NormalTok{)}

\CommentTok{\# for excel datasets you need the readxl package}
\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{village }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"Village.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To end our first chapter we have to talk also about how to save the
final output of our work. The most common way is to export our dataframe
as an excel file. In order to do so, we need to use the package
``xlsx'' (\protect\hyperlink{ref-dragulescu2020}{Dragulescu and Arendt 2020}). \textbf{Note that it is always important to set the
working directory in order to tell R where to put our exported file}.
If we did it at the beginning of our code, there is no need to repeat
it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# exporting to excel}
\FunctionTok{library}\NormalTok{(}\StringTok{"xlsx"}\NormalTok{)}
\FunctionTok{write.xlsx}\NormalTok{(mtcars, }\AttributeTok{file =} \StringTok{"mtcars.xlsx"}\NormalTok{)}

\CommentTok{\# exporting to csv}
\FunctionTok{write.csv}\NormalTok{(mtcars, }\AttributeTok{file =} \StringTok{"mtcars.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{itemize}
\tightlist
\item
  James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2021). An
  Introduction to Statistical Learning (Vol. 103). Springer New York.
  \href{https://www.statlearning.com}{Available here}. Chapter 2.4 exercises 8, 9, and 10.
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground}, A, B, C
\end{itemize}

\newpage

\hypertarget{data-cleaning}{%
\chapter{Data Cleaning}\label{data-cleaning}}

\begin{quote}
\emph{Happy families are all alike; every unhappy family is unhappy in its
own way.}\\
Lev Tolstoy
\end{quote}

It is often said that 80\% of data analysis is spent on the process of
cleaning and preparing the data. Data cleaning is not just one step, but
must be repeated many times over the course of analysis as new problems
arise or new data is collected. The tidy data standard has been designed
to facilitate initial exploration and analysis of the data, and to
simplify the development of data analysis tools that work well together.

A dataset is a collection of values, usually either numbers (if
quantitative) or strings (if qualitative). Most statistical datasets are
rectangular tables made up of rows and columns. The columns are almost
always labeled and the rows are sometimes labeled. Values are organized
in two ways. Every value belongs to a variable (\textbf{column}) and an
observation (\textbf{row}). A variable contains all values that measure the
same underlying attribute (like height, temperature, duration) across
units. An observation contains all values measured on the same unit
(like a person, or a day, or a race) across attributes.

In tidy data (\protect\hyperlink{ref-wickham2014}{Wickham 2014}):

1. Each variable forms a column.

2. Each observation forms a row.

3. Each type of observational unit forms a table.

Tidy data makes it easy for an analyst or a computer to extract needed
variables because it provides a standard way of structuring a dataset.
They are also particularly well suited for vectorized programming
languages like R, because the layout ensures that values of different
variables from the same observation are always paired. While the order
of variables and observations does not affect analysis, a good ordering
makes it easier to scan the raw values.

The five most common problems with messy datasets are:

\begin{itemize}
\item
  Column names are values or codes and not variable names.
\item
  Multiple variables are stored in one column.
\item
  Variables are stored in both rows and columns.
\item
  Multiple types of observational units are stored in the same table
  (i.e.~people and household).
\item
  A single observational unit is stored in multiple tables.
\item
  Variables types are miss-specified due to inputational errors (comma
  before decimals, or written comments within numeric columns).
\end{itemize}

~

~

~

\hypertarget{variable-names}{%
\section{Variable Names}\label{variable-names}}

Working with external code or data, I often see variable names or object
names such as \(X, y, xs, x1, x2, tp, tn, clf, reg, xi, yi, ii\). This
creates a problem when we need to analyze a specific variable, or
understand the results we get. In fact, we will have to go back to the
code-book to understand the content of each variable. To put it frankly,
people (myself included) are terrible at naming objects, especially when
we need to invent a lot of names, but a bigger effort is needed
(\protect\hyperlink{ref-koehrsen2019}{Koehrsen 2019}).

There are three fundamental ideas to keep in mind when naming variables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The variable name must describe the information represented by the
  variable. A variable name should tell us specifically in words what
  the variable stands for.}
\item
  \textbf{Our code will be read by others or us in the future. So we should
  prioritize how easy it is understood, rather than how quickly it is
  written.}
\item
  \textbf{Adopt standard conventions for naming so we can make one global
  decision instead of multiple local decisions throughout the
  process.}
\end{enumerate}

In order to change the names of the variables we can use the following
code. Basically the function \texttt{colnames()} retrieves the names of the
variables as a character vector and we can change it as we have seen in
\protect\hyperlink{indexing}{Indexing}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# retreiving variable names}
\FunctionTok{colnames}\NormalTok{(mtcars)}

\CommentTok{\# changing the name of the variable mpg to consumption}
\FunctionTok{colnames}\NormalTok{(mtcars)[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"consumption"}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{variable-types}{%
\section{Variable Types}\label{variable-types}}

Variable types are important because they tell R how to interpret a
variable. As an example, we never want R to treat a numeric variable as
a character one because we would not be able to use that variable for
calculations.

Fortunately, when we import a dataset R-Studio automatically recognizes
the type of each variable. However, sometimes there are some inputation
mistakes that the software cannot overcome by itself. I refer to cases
such as a number with a comma instead of a dot before decimals, or a
cell with some comments in a numeric variable, etc. In all these cases R
will read the variable as a character variable. The function \texttt{str()},
allows us to recognize the possible problem by listing all the variables
and their types. If we see a missclassified variable we can immediately
explore it via \texttt{summary()} and/or \texttt{table()}, find the error, and correct
it. Finally we will need to tell R which is the right type for that
variable, if we don't get any error, it means that we fixed it (look at
the example below).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating an age vector}
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{12}\NormalTok{, }\StringTok{"didn\textquotesingle{}t answer"}\NormalTok{, }\DecValTok{30}\NormalTok{)}

\CommentTok{\# we need it numeric, but it is character (chr)}
\FunctionTok{str}\NormalTok{(age)}

\CommentTok{\# we find the problem: there is a comment!}
\FunctionTok{table}\NormalTok{(age)}

\CommentTok{\# we change the value where there was the comment and input a missing value}
\NormalTok{age[age}\SpecialCharTok{==}\StringTok{"didn\textquotesingle{}t answer"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\CommentTok{\# we change the vector type to numeric}
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(age)}
\end{Highlighting}
\end{Shaded}

Remember that all data types in R have an apposite function that
converts the data to its type. To list the most used: \texttt{as.character()},
\texttt{as.numeric()}, \texttt{as.factor()}, \texttt{as.data.frame()}, etc\ldots{}

~

\hypertarget{factor-variables}{%
\subsection{Factor variables}\label{factor-variables}}

In some cases (especially when plotting data), it is important that
factor variables are recognized by R and treated accordingly. Factor
variables are dichotomous (female and male, or 1 and 0, or yes and no)
or categorical (hair color, rooftop type, vehicle type, etc\ldots).

A dummy variable is a variable that indicates whether an observation has
a particular characteristic. A dummy variable can only assume the values
0 and 1, where 0 indicates the absence of the property, and 1 indicates
the presence of the same. The values 0/1 can be seen as no/yes or
off/on.

The following code creates a character variable composed by ten random
``yes'' and ``no''. It, then, turns the variable into a factor variable and
specifies the levels. By comparing the summary of the variables, we see
immediately how differently R behaves when it is dealing with a factor
as compared to a character variable. The \texttt{relevel()} function reorders
the variable as we want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yesno }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\NormalTok{), }\AttributeTok{size=}\DecValTok{10}\NormalTok{, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{str}\NormalTok{(yesno)}
\FunctionTok{summary}\NormalTok{(yesno)}

\CommentTok{\# transform the variable in factor}
\NormalTok{yesnofac }\OtherTok{=} \FunctionTok{factor}\NormalTok{(yesno, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\StringTok{"yes"}\NormalTok{,}\StringTok{"no"}\NormalTok{))}
\FunctionTok{str}\NormalTok{(yesnofac)}
\FunctionTok{summary}\NormalTok{(yesnofac)}
\FunctionTok{levels}\NormalTok{(yesnofac)}

\CommentTok{\# relevel the variable}
\FunctionTok{relevel}\NormalTok{(yesnofac, }\AttributeTok{ref=}\StringTok{"no"}\NormalTok{)}
\FunctionTok{as.numeric}\NormalTok{(yesnofac)}
\end{Highlighting}
\end{Shaded}

~

\hypertarget{dates-and-times}{%
\subsection{Dates and times}\label{dates-and-times}}

With R we can do calculations using dates and times too. This may sound
wired, but sometimes we need to subtract 180 days to a date variable and
without this functionality this would be quite a complex task. Dates,
however, are not automatically recognized by the software and there is
the need for an external package called \texttt{lubridate()}(\protect\hyperlink{ref-yarberry2021}{Yarberry 2021}).

Following a small example. The functions \texttt{today()} and \texttt{now()} retrieve
the current date in two different levels of detail. The function ymd()
transforms a numeric or character variables containing year, month, and
day into a date variable. Note that the \texttt{lubridate()} package has a long
list of functions according to the format of the date needed (check the
\href{https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf}{Lubridate cheatsheet}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}
\CommentTok{\# today\textquotesingle{}s date}
\FunctionTok{today}\NormalTok{()}
\FunctionTok{now}\NormalTok{()}

\CommentTok{\# creating a numeric vector of dates with year, month and day}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20220321}\NormalTok{, }\DecValTok{20220322}\NormalTok{, }\DecValTok{20220320}\NormalTok{)}
\FunctionTok{str}\NormalTok{(x)}

\CommentTok{\# transforming it in a date type vector }
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ymd}\NormalTok{(x)}
\FunctionTok{str}\NormalTok{(y)}

\CommentTok{\# subtracting 180 days to y and x}
\NormalTok{x}\DecValTok{{-}180}
\NormalTok{y}\DecValTok{{-}180}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{row-names}{%
\section{Row Names}\label{row-names}}

Row names are often not important. In most of the cases we will have a
dataset with a column representing the id of the observation. However,
when it comes to some \protect\hyperlink{multivariate-analysis}{Multivariate Analysis} methods, we will need to
have the id of the observation not in one column, but as row name. In
this way R will be able to plot the data automatically referring to the
name of the row.

The code below gives us a way to copy the first column of a dataset to
the row names and then delete the column itself, in order to have a
fully numeric dataset. We will use the same dataset we created in
\protect\hyperlink{dataset-exploration}{Dataset Exploration}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creatng the dataset}
\NormalTok{people }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{, }\StringTok{"Mike"}\NormalTok{, }\StringTok{"Greg"}\NormalTok{),}
                     \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{44}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{46}\NormalTok{),}
                     \AttributeTok{IQ =} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{110}\NormalTok{))}

\CommentTok{\# inputting rownames from the column "name"}
\FunctionTok{rownames}\NormalTok{(people) }\OtherTok{\textless{}{-}}\NormalTok{ people}\SpecialCharTok{$}\NormalTok{name}

\CommentTok{\# deleting the clumn "name"}
\NormalTok{people }\OtherTok{\textless{}{-}}\NormalTok{ people[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{advanced-data-manipulation-and-plotting}{%
\chapter{Advanced Data Manipulation and Plotting}\label{advanced-data-manipulation-and-plotting}}

It is time to become a pro! Well, becoming a ``pro'' in R doesn't strictly
that we will have to do complex things, or become some crazy freaks not
leaving our laptop and squeaking like lab rats. Becoming a pro means
being able to do things in another way. Sometimes an easier way, other
times a more complex way that allows us to reach a higher level of
output.

Now you will be asking: ``If there are easier ways, why did we learn the
complex part at the beginning?'' Well, Michael Jordan would answer you:
``Get the fundamentals down and the level of everything you do will
rise.'' So, basically, we need the grammar in order to write a sentence.

Now, put your ``pro'' hat on!

~

~

~

\hypertarget{ifelse}{%
\section{Ifelse}\label{ifelse}}

The \texttt{ifelse()} function allows us to give R multiple inputs according to
one or more conditions. This function is used a lot when cleaning data
and when we want to create new variables.

As an example, we want to generate a variable equal to 1 if the car
consumes less than 20, and equal to 0 in all the other cases. Or we can
create the same variable with more than one condition. Or we want to
create a new variable with more than two levels. Finally, we may want to
correct some values according to our conditions.

Within the \texttt{ifelse()} function we are required to specify the
condition(s), comma, the value to give if the condition(s) is true,
comma, the value to give if the condition(s) is false.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating a new var conditionally}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{newvar }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg }\SpecialCharTok{\textless{}} \DecValTok{20}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# creating a new var with more conditions}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{newvar }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg }\SpecialCharTok{\textgreater{}} \DecValTok{20} \SpecialCharTok{\&}\NormalTok{ mtcars}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{\textless{}} \DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# creating a new var with more and more conditions in nested way}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{newvar }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg }\SpecialCharTok{\textgreater{}} \DecValTok{20}\NormalTok{, }\DecValTok{100}\NormalTok{,}
                        \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg }\SpecialCharTok{\textless{}} \DecValTok{17}\NormalTok{, }\DecValTok{10}\NormalTok{ ,}
                                \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{==} \DecValTok{6}\NormalTok{, }\ConstantTok{NA}\NormalTok{ , mtcars}\SpecialCharTok{$}\NormalTok{mpg )))}
\CommentTok{\# correctin a variable}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{mpg }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl}\SpecialCharTok{\textgreater{}}\DecValTok{5}\NormalTok{, }\ConstantTok{NA}\NormalTok{, mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\CommentTok{\#this is the same as}
\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{cyl}\SpecialCharTok{\textgreater{}}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

In the case in which we have a categorical variable with multiple
categories (i.e.~the color of the car), and/or we want to create dummies
for multiple categorical variables at once (i.e.~the variables: red,
green, blue, etc), I suggest to use the package
\texttt{fastDummies}(\protect\hyperlink{ref-kaplan2022}{Kaplan 2022}). Note that in the argument \texttt{select\_columns}
you can specify multiple columns using \texttt{c()}. The argument
\texttt{remove\_selected\_columns} removes the original categorical variable
after the transformation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fastDummies)}
\FunctionTok{dummy\_cols}\NormalTok{(mtcars, }\AttributeTok{select\_columns =} \StringTok{"cyl"}\NormalTok{, }\AttributeTok{remove\_selected\_columns =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{the-apply-family}{%
\section{The Apply family}\label{the-apply-family}}

The \texttt{apply()} function is a great facilitator. it applies a function of
our choice to the whole dataset by row, or by column
(\protect\hyperlink{ref-sovanskywinter2022}{Sovansky Winter 2022}). This means that, using this function, we can
compute the mean (or a more complex calculus) on all the variables of
the data! So we can create our own summary of the data.

Within the \texttt{apply()} function we have to impute the data we want to
manipulate, comma, if we want it to apply the function row-wise (1) or
column-wise (2), comma, the function we want to apply to the data.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\# apply}
\CommentTok{\# calculate the mean per each column}
\FunctionTok{apply}\NormalTok{(mtcars, }\DecValTok{2}\NormalTok{, mean)}

\CommentTok{\# filter out rows where the mean of the row is lower than 20}
\NormalTok{mtcars[}\SpecialCharTok{{-}}\FunctionTok{apply}\NormalTok{(mtcars, }\DecValTok{1}\NormalTok{, mean) }\SpecialCharTok{\textless{}} \DecValTok{20}\NormalTok{,]}

\CommentTok{\# create statistical summary}
\NormalTok{Stat}\OtherTok{\textless{}{-}}\FunctionTok{cbind}\NormalTok{(}
  \FunctionTok{apply}\NormalTok{(mtcars[,}\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"cyl"}\NormalTok{, }\StringTok{"disp"}\NormalTok{, }\StringTok{"hp"}\NormalTok{,}\StringTok{"drat"}\NormalTok{)],}\DecValTok{2}\NormalTok{,mean), }
  \FunctionTok{apply}\NormalTok{(mtcars[,}\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"cyl"}\NormalTok{, }\StringTok{"disp"}\NormalTok{, }\StringTok{"hp"}\NormalTok{,}\StringTok{"drat"}\NormalTok{)],}\DecValTok{2}\NormalTok{,sd),}
  \FunctionTok{apply}\NormalTok{(mtcars[mtcars}\SpecialCharTok{$}\NormalTok{am}\SpecialCharTok{==}\DecValTok{0}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"cyl"}\NormalTok{, }\StringTok{"disp"}\NormalTok{, }\StringTok{"hp"}\NormalTok{,}\StringTok{"drat"}\NormalTok{)],}\DecValTok{2}\NormalTok{,mean),}
  \FunctionTok{apply}\NormalTok{(mtcars[mtcars}\SpecialCharTok{$}\NormalTok{am}\SpecialCharTok{==}\DecValTok{1}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\StringTok{"mpg"}\NormalTok{, }\StringTok{"cyl"}\NormalTok{, }\StringTok{"disp"}\NormalTok{, }\StringTok{"hp"}\NormalTok{,}\StringTok{"drat"}\NormalTok{)],}\DecValTok{2}\NormalTok{,mean)}
\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(Stat)}\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mean"}\NormalTok{, }\StringTok{"s.d."}\NormalTok{, }\StringTok{"Mean Automatic"}\NormalTok{, }\StringTok{"Mean Manual"}\NormalTok{)}
\FunctionTok{round}\NormalTok{(Stat,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{dplyr}{%
\section{Dplyr}\label{dplyr}}

Following on powerful things, we have the \texttt{dplyr} package
(\protect\hyperlink{ref-wickham2022}{Wickham and Bryan 2022}). Well, this is one of my favorite packages in R. In fact,
\texttt{dplyr} allows us to do almost all that we have seen until now in an
easier and, sometimes, more intuitive way.

\texttt{Dplyr} is a grammar of data manipulation, providing a consistent set of
verbs that help us solve the most common data manipulation challenges.
Basically, \texttt{dplyr} allows us to execute multiple functions a cascade. By
using the symbol \texttt{\%\textgreater{}\%} at the end of each line, we tell R that the code
continues with another function (in fact R gives us an indent in the
following line)(\protect\hyperlink{ref-wickham2022}{Wickham and Bryan 2022}).

The code below allows us to see at glance the big potential of this
package. First we filter the Star Wars characters by skin color;
secondly we compute the Body Mass Index (bmi) of all the characters, and
select only some columns from the original data; finally for each
species, we compute the number of characters and their average mass, and
we filter only the groups with more than one character and an average
mass above 50.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading the package and data}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(starwars)}

\CommentTok{\# subsetting by skin color}
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{filter}\NormalTok{(skin\_color}\SpecialCharTok{==}\StringTok{"light"}\NormalTok{)}

\CommentTok{\# creating a new variable and subsetting some columns}
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{bmi =}\NormalTok{ mass}\SpecialCharTok{/}\NormalTok{((height}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{select}\NormalTok{(name}\SpecialCharTok{:}\NormalTok{mass, bmi)}

\CommentTok{\# creating a new dataset, summarizing the data grouped by species and filtering}
\CommentTok{\# the most interesting information}
\NormalTok{prova }\OtherTok{\textless{}{-}}\NormalTok{ starwars }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }
                  \AttributeTok{mass =} \FunctionTok{mean}\NormalTok{(mass, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{, }
\NormalTok{               mass }\SpecialCharTok{\textgreater{}} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

So, using \texttt{dplyr}, with one command, we can ask R to group the data by a
categorical variable (\texttt{group\_by()}), subset the data according to our
conditions (\texttt{filter()}), select only few variables to be retrieved
(\texttt{select()}), manipulate the current variables or create some new ones
(\texttt{mutate()}), summarize the data (\texttt{summarize()}), etc\ldots{} Of course, all
of the above can be alternatively done using the indexing system and
multiple functions, but I live to you the choice.

~

~

~

\hypertarget{merging-datasets}{%
\section{Merging datasets}\label{merging-datasets}}

In R we can load multiple dataset and data formats in the memory, and
use them together. However, sometimes we want to have data from
different sources within the same object. This entails merging datasets
together. We may want to outer join, left join, cross join. And more
often, we may not be sure about what we want! My suggestion is to first
clear our mind by reading the following options:

\begin{itemize}
\item
  \textbf{Outer join} merges the two datasets by a specified variable and
  keeps all the observations \texttt{(all\ =\ TRUE)}.
\item
  \textbf{Left outer} merges all the observations of object \(y\) on \(x\) only
  if they have a corresponding \(id\) variable (corresponding means that
  the variable has the same name and type).
\item
  \textbf{Right outer} merges all the observations of \(x\) on \(y\) only if
  they have a corresponding \(id\) variable (corresponding means that
  the variable has the same name and type).
\item
  \textbf{Cross} or cartesian product is one kind of join where each row of
  one dataset is joined with other. So if we have a dataset of size
  \(m\) and if we join with other dataset with of size \(n\), we will get
  a dataset with \(m*n\) number of rows.
\end{itemize}

Within the \texttt{merge()} function we are asked to specify the dataset \(x\),
comma the dataset \$y\$, comma, the name of the \(id\) variable, comma,
additional arguments depending on which merge we are interested in.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Outer join: }
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"CustomerId"}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Left outer: }
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"CustomerId"}\NormalTok{, }\AttributeTok{all.x =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Right outer: }
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \StringTok{"CustomerId"}\NormalTok{, }\AttributeTok{all.y =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Cross join: }
\FunctionTok{merge}\NormalTok{(}\AttributeTok{x =}\NormalTok{ df1, }\AttributeTok{y =}\NormalTok{ df2, }\AttributeTok{by =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{melting-vs-transposing}{%
\section{Melting vs Transposing}\label{melting-vs-transposing}}

I wrote in the previous chapter that the most important rule is that, in
a dataset, each row has to be an observation and each column has to
represent a variable. Now it is time to break this rule! In fact,
sometimes we need a different shape of dataset, usually when we want to
plot multidimensional data.

Melting data sounds strange, however this is what we want to do if we
have a wide dataset (a lot of variables) and we want to transform it in
a ``long'' dataset (only a few variables), but retaining all the
information.

Please remember: melting is not transposing! Transposing (\texttt{t()}
function) is useful when we have data in columns that we need to
rearrange in rows, while melting (\texttt{melt()}) creates a ``long'' dataset
with more variables in the same column. Note that the \texttt{melt()} function
belongs to the package \texttt{reshape2}(\protect\hyperlink{ref-anderson2022}{Anderson 2022}).

The code below generates a dataset, then transposes it, and melts it.
The \texttt{melt()} function requires us to specify the data we want to melt,
comma, the name of the variable(s) we want to retain as they are. We
will see in the following chapter how to use melted data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{people }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{, }\StringTok{"Mike"}\NormalTok{, }\StringTok{"Greg"}\NormalTok{),}
                     \AttributeTok{surname =} \FunctionTok{c}\NormalTok{(}\StringTok{"Wilson"}\NormalTok{, }\StringTok{"Jones"}\NormalTok{, }\StringTok{"Smith"}\NormalTok{),}
                     \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{44}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{46}\NormalTok{),}
                     \AttributeTok{IQ =} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{110}\NormalTok{))}

\CommentTok{\# transpose}
\FunctionTok{t}\NormalTok{(people)}

\CommentTok{\# melt}
\FunctionTok{library}\NormalTok{(reshape2)}
\FunctionTok{melt}\NormalTok{(people, }\AttributeTok{id =} \StringTok{"name"}\NormalTok{)}

\CommentTok{\# for two id variables}
\FunctionTok{melt}\NormalTok{(people, }\AttributeTok{id =} \FunctionTok{c}\NormalTok{(}\StringTok{"name"}\NormalTok{, }\StringTok{"surname"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{ggplot2}{%
\section{Ggplot2}\label{ggplot2}}

\texttt{Ggplot2} is one of the most famous packages in R, and probably also one
of the most used. It allows us to draw almost all those beautiful graphs
for which R is known worldwide (\protect\hyperlink{ref-wickham2016}{Wickham 2016}; \protect\hyperlink{ref-chang2018}{Chang 2018}). The main
advantage of \texttt{ggplot2} is its deep underlying grammar, which lets us
create a simple, expressive and descriptive code for plotting. Plots can
be built up iteratively, and edited in a second moment.

The \texttt{ggplot()} function tells R to generate the Cartesian axes where we
will place the graph, but this is yet not a graph yet. Within this line
we usually specify the dataset from which ggplot has to take the data,
and the variables corresponding to the Cartesian axes (using \texttt{aes()}).
After having specified the \texttt{ggplot()} function we can put a \texttt{+} sign
that will express our intention to put an additional layer on top of the
Cartesian space using the dedicated functions: \texttt{geom\_point()} for a
scatter plot, \texttt{geom\_smooth()} for a regression line, \texttt{geom\_bar()} or
\texttt{geom\_col()} for a bar chart, just to mention a few of them (for a
description of the meaning of the above mentioned graph refer to the
\protect\hyperlink{data-visualization}{Data visualization} chapter).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\# No Plot Yet!}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp))}

\CommentTok{\# First scatterplot}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}

\CommentTok{\# we can assign our graph to an object}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We can add more and more layers to the graph, and personalize all the
things we want. The code below generates two types of regression lines
(more details in {[}\protect\hyperlink{linear-regression}{Linear Regression}{]}), and divides data in plots
according to a variable (\texttt{facets\_grid()}). \emph{As you can see, I tend not
to assign my plot to an object, unless it is strictly necessary. This is
my way of working, but you are free to do as you prefer.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\# Adding More Layers: regression line}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{() }\CommentTok{\# Generalized additive models}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\CommentTok{\#linear regression}

\CommentTok{\# Adding More Layers: Facets}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{hp)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(. }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-33-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-33-2} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-33-3} \caption{From top-left clockwise: Regression line GAM; Linear regression line; Facets.}\label{fig:unnamed-chunk-33}
\end{figure}

Following an example application of the need for melting data for the
scope of plotting multidimensional phenomenons. In this case, we have a
dataset with some students and we want to plot their scores, but we want
to be able to clearly see a trend for each one of them? A solution could
be the paired (specifying \texttt{position\ =\ \textquotesingle{}dodge\textquotesingle{}}) bar chart below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{people }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Mary"}\NormalTok{, }\StringTok{"Mike"}\NormalTok{, }\StringTok{"Greg"}\NormalTok{),}
                     \AttributeTok{surname =} \FunctionTok{c}\NormalTok{(}\StringTok{"Wilson"}\NormalTok{, }\StringTok{"Jones"}\NormalTok{, }\StringTok{"Smith"}\NormalTok{),}
                     \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{44}\NormalTok{, }\DecValTok{52}\NormalTok{, }\DecValTok{46}\NormalTok{),}
                     \AttributeTok{IQ =} \FunctionTok{c}\NormalTok{(}\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{110}\NormalTok{))}

\CommentTok{\# melting data}
\FunctionTok{library}\NormalTok{(reshape2)}
\NormalTok{melted\_people }\OtherTok{\textless{}{-}} \FunctionTok{melt}\NormalTok{(people, }\AttributeTok{id =} \FunctionTok{c}\NormalTok{(}\StringTok{"name"}\NormalTok{, }\StringTok{"surname"}\NormalTok{))}

\CommentTok{\# plotting}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ melted\_people, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ name, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{_main_files/figure-latex/unnamed-chunk-34-1} 

}

\caption{Paired barchart.}\label{fig:unnamed-chunk-34}
\end{figure}

We are able to personalize almost everything as we like using \texttt{ggplot2}.
The \href{https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf}{Ggplot2 Cheatsheet} gives us some of the most important options. I strongly suggest to look
also at the \href{https://www.r-graph-gallery.com}{R Graph Gallery} which
includes advanced graphs that are done using \texttt{ggplot2} or similar
packages, and provides the code for replicating each one of them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# labeled points}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hp, }\AttributeTok{y =}\NormalTok{ mpg, }\AttributeTok{size =}\NormalTok{ wt)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(cyl)), }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{row.names}\NormalTok{(mtcars)), }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{nudge\_y =} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_family =} \StringTok{"Times"}\NormalTok{)}

\CommentTok{\# Violin plot}
\FunctionTok{data}\NormalTok{(}\StringTok{"InsectSprays"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(InsectSprays, }\FunctionTok{aes}\NormalTok{(spray, count, }\AttributeTok{fill=}\NormalTok{spray)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{(}\AttributeTok{colour=}\StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Type of spray"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Insect count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-35-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-35-2} \caption{From top-left clockwise: Labeled scatterplot; Violin plot.}\label{fig:unnamed-chunk-35}
\end{figure}

Finally, I want to show you what to do if we have some outliers in our
data, but we need to plot the main trend excluding them. Below some
different versions of ggplot code that put a limit to the y axes (but it
could be done for the x axes too using \texttt{xlim()} instead of \texttt{ylim()}).
Try the code and see the difference.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{testdat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }
                      \AttributeTok{y =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{))}
\NormalTok{testdat[}\DecValTok{20}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{100}  \CommentTok{\# Outlier!}

\CommentTok{\# normal plot}
\FunctionTok{ggplot}\NormalTok{(testdat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_line}\NormalTok{()}

\CommentTok{\# excluding the values outside the ylim range}
\FunctionTok{ggplot}\NormalTok{(testdat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
        \FunctionTok{ylim}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}

\CommentTok{\# zooming in a portion of the graph}
\FunctionTok{ggplot}\NormalTok{(testdat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+} 
        \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
        \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

Exercises:

\begin{itemize}
\tightlist
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground} -
  Advanced Data Manipulation
  \newpage
\end{itemize}

\hypertarget{part-statistical-analysis}{%
\part{Statistical Analysis}\label{part-statistical-analysis}}

\hypertarget{exploratory-data-analysis}{%
\chapter{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\justifying

From this point forward, we will stop focusing on R, and we will finally
start acting as statisticians. We will perform various level of data
analysis by applying different methodologies. Of course, R will be our
tool, our facilitator, but it will be no more at the center of the
stage. We will fully concentrate on the statistical methods, their
application and interpretation.

Following the six steps of data analysis. This section explores the
steps from 4 to 6, given that the first two are out of the scope of this
book and the third has been treated in the previous section.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/Schermata 2022-03-28 alle 10.36.15} 

}

\caption{The six steps of data analysis.}\label{fig:unnamed-chunk-37}
\end{figure}

Exploratory data analysis is usually performed when we have our first
contact with \ul{clean} new data. Of course, the first
impression is not always the right one, but if we give it a proper
chance can save us a lot of work and some headaches later on. During
this phase we will perform some summary statistics (central tendency and
variability measures), basic plotting, inequality measures, and
correlations. Exploratory analysis differs from the \protect\hyperlink{dataset-exploration}{dataset
exploration} we performed in Chapter 1, because the latter focuses on
the shape of the data (type, dimensions, etc) while the former focuses
on the content.

~

~

~

\hypertarget{central-tendency-measures}{%
\section{Central Tendency Measures}\label{central-tendency-measures}}

A measure of central tendency is a single value that attempts to
describe a set of data by identifying the central position within that
set of data. The mean, median and mode are all valid measures of central
tendency, but under different conditions, some measures of central
tendency become more appropriate to use than others (\protect\hyperlink{ref-lane2003}{Lane et al. 2003}; \protect\hyperlink{ref-manikandan2011}{Manikandan 2011}; \protect\hyperlink{ref-laerdstatistics}{Laerd Statistics, n.d.}).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{arithmetic} \textbf{mean} (or \textbf{average}) is the most popular and
well known measure of central tendency. The mean is equal to the sum of
all values divided by the number of observations (Equation
\eqref{eq:mean}).

\begin{equation}
\bar x = \frac{\sum{x}}{n}
\label{eq:mean}
\end{equation}

One of its important properties is that it minimizes error in the
prediction of any value in your data set, given a big enough sample size
(more details on this property will be discussed in {[}The Normal
Distribution{]} chapter). Another important property of the mean is that
it includes every value in your data set as part of the calculation.
This implies, also, that it is sensitive to the presence of outliers or
skewed distributions (i.e., the frequency distribution for our data is
skewed). In those cases the median would be a better indicator of
central location.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{geometric mean} is defined as the \(n^{th}\) root of the product of
\(n\) values (Equation \eqref{eq:geomean}).

\begin{equation}
\bar x_{g} = \sqrt[n^{th}]{x_1*x_2*...*x_n}
\label{eq:geomean}
\end{equation}

The geometric mean cannot be used if the values considered are lower
or equal to zero. It is often used for a set of numbers whose values are
meant to be multiplied together or are exponential in nature, such as a
set of growth figures: values of the human population, or interest rates
of a financial investment over time.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{median} is the middle score for a set of data that has been
arranged in order of magnitude. This means, also, that the median is
equal to the value corresponding to the 50th percentile of he
distribution. If the number of observations is even, then the median is
the simple average of the middle two numbers.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{mode} is the most frequent score in our data set. On a histogram
it represents the highest point (we will see more about it later in this
chapter). You can, therefore, sometimes consider the mode as being the
most popular option. This implies that the mode is not always a unique
value, in fact (particularly in continuous data) we can have bi-modal,
tri-modal, or multi-modal distributions. Moreover, the mode will not
provide us with a very good measure of central tendency when the most
common mark is far away from the rest of the data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{middle value} is a less used measure. It represents the value
exactly at the center between the minimum and the maximum values
(Equation \eqref{eq:mv}), regardless of the distribution of the data Thus it is
sensitive to outliers.

\begin{equation}
\frac{max-min}{2}
\label{eq:mv}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The function \texttt{summary()} gives us a brief overview of the minimum,
maximum, first and third quantile, median and mean of each variable, or
of a single variable. Of course these measures can be also computed
independently using dedicated functions.

The following code computes summary statistics, arithmetic mean,
geometric mean (there is not a dedicated function for it), median, mode
(the function from \texttt{DescTools} allows us to retrieve also multi-modal
values)(\protect\hyperlink{ref-signorelli2021}{Signorelli 2021}), and middle value. For more options about all
the functions below, please, look in the help.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to do statistical summaries of the whole dataset or of a single variable}
\FunctionTok{summary}\NormalTok{(mtcars)}
\FunctionTok{summary}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)}

\CommentTok{\# arithmetic mean}
\FunctionTok{mean}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# geometric mean}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{mean}\NormalTok{(}\FunctionTok{log}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)))  }

\CommentTok{\# median}
\FunctionTok{median}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# mode}
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{Mode}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\#middle value}
\NormalTok{(}\FunctionTok{max}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg))}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{variability-measures}{%
\section{Variability Measures}\label{variability-measures}}

The terms variability, spread, and dispersion are synonyms, and refer to
how disperse a distribution is. These measures complete the information
given by central tendency measures, in order to better understand the
data we are analyzing. The example below gives us an idea of how unequal
distributions may have the same central tendency measures, and thus,
considering only them would lead to mistaken evaluations.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0972}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1250}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1250}}@{}}
\caption{Central tendency measures of the samples a and b.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
distributions
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
median
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
mode
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
distributions
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
median
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
mode
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
a & 3 & 3 & 3 \\
b & 3 & 3 & 3 \\
\end{longtable}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-39-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-39-2} \caption{From left: Histograms of the samples a and b.}\label{fig:unnamed-chunk-39}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{range} is the simplest measure of variability to calculate, and
one you have probably encountered many times in your life. The range is
the maximum value minus the minimum value (Equation \eqref{eq:range}).

\begin{equation}
range=max-min
\label{eq:range}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{interquartile range} (IQR) is the range of the central 50\% of the
values in a distribution (Equation \eqref{eq:iqrange}).

\begin{equation}
IQR=75^{th} percentile - 25^{th} percentile
\label{eq:iqrange}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

But variability can also be defined in terms of how close the values in
the distribution are to the middle of the distribution. Using the mean
as the measure for referencing to the middle of the distribution, the
\textbf{variance} is defined as the average squared difference of the scores
from the mean (Equation \eqref{eq:var}).

\begin{equation}
\sigma^2=\frac{\sum{(X-\bar x)^2}}{N}
\label{eq:var}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{standard deviation} is simply the square root of the variance
(Equation \eqref{eq:sd}). It is an especially useful measure of variability when
the distribution is normal or approximately normal (see {[}The Normal
Distribution{]}) because the proportion of the distribution within a given
number of standard deviations from the mean can be calculated.

\begin{equation}
\sigma=\sqrt{\sigma^2}
\label{eq:sd}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{coefficient of variation} (CV) represents the ratio of the
standard deviation to the mean (Equation \eqref{eq:cv}), and it is a useful
statistic for comparing the degree of variation of data relative to
different unit of measures. In fact, the CV is the only variability
measure (of those mentioned here) that is standardized, thus does not
depend on its unit of measure.

\begin{equation}
CV=\frac{\sigma}{\bar{x}}
\label{eq:cv}
\end{equation}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The following code computes the above mentioned variability measures for
the consumption variable of the mtcars dataset. In the last lines you
can find also the code to compile a frequency table with the function
\texttt{table()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# range}
\FunctionTok{max}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# quantile distribution}
\FunctionTok{quantile}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# interquantile range}
\FunctionTok{quantile}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{75}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{quantile}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{25}\NormalTok{)}

\CommentTok{\# variance}
\FunctionTok{var}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# standard deviation}
\FunctionTok{sd}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# coefficent of variation}
\FunctionTok{sd}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# frequency tables}
\FunctionTok{table}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)}
\FunctionTok{table}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl, mtcars}\SpecialCharTok{$}\NormalTok{am)}
\end{Highlighting}
\end{Shaded}

Below a simple code that allows to create a dataframe, which is also a
way to create a summary table. The \texttt{data.frame()} function has to be
filled column wise, specifying the column/variable name and then the
content. You can play with it in order to create a personalized table.
The \texttt{stargazer()} function, instead presents the same statistics as the
\texttt{summary()} function, but in a cooler style (\protect\hyperlink{ref-torres-reyna2014}{Torres-Reyna 2014}; \protect\hyperlink{ref-hlavac2022}{Hlavac 2022}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a table storing your values}
\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{obs =} \FunctionTok{c}\NormalTok{(}\StringTok{"Miles per Gallon"}\NormalTok{, }\StringTok{"Number of Cylinders"}\NormalTok{),}
           \AttributeTok{mean =} \FunctionTok{c}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg), }\FunctionTok{mean}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)),}
           \AttributeTok{median =} \FunctionTok{c}\NormalTok{(}\FunctionTok{median}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg), }\FunctionTok{median}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl)),}
           \AttributeTok{sd =} \FunctionTok{c}\NormalTok{(}\FunctionTok{sd}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg), }\FunctionTok{sd}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl))}
\NormalTok{           )}

\CommentTok{\# create a table storing basic summary statistics}
\FunctionTok{library}\NormalTok{(stargazer)}
\FunctionTok{stargazer}\NormalTok{(mtcars, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{, }\AttributeTok{digits=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{inequality-measures}{%
\section{Inequality Measures}\label{inequality-measures}}

Another set of analysis methods that are related to variability are
inequality measures. Inequality can be defined as: ``a scalar numerical
representation of the interpersonal differences in income (for example)
within a given population.'' The use of the word ``scalar'' implies that
all the different features of inequality are compressed into a single
number or a single point on a scale.

But inequality of what? The strict statistical rule says that it must be
a quantitative variable transferable among the population of interest,
such as income, apples, cars, etc\ldots{} However, in economics, inequality
measurements are used also for other items, like carbon emissions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{Gini} coefficient, or Gini Index, is the most widely used measure
of inequality in policy-related discussions. It is a measure of
statistical dispersion most prominently used as a measure of inequality
of income distribution or inequality of wealth distribution. It is
defined as a ratio with values between 0 and 1. Thus, a low Gini
coefficient indicates more equal income or wealth distribution, while a
high Gini coefficient indicates more unequal distribution. Zero
corresponds to perfect equality (everyone having exactly the same
income) and 1 corresponds to perfect inequality (where one person has
all the income, while everyone else has zero income). \emph{Note that the
Gini coefficient requires that no one have a negative net income or
wealth.}

In geometrical terms, the Gini coefficient can be thought of as the
ratio of the area that lies between the line of perfect equality (the
diagonal) and the Lorenz curve over the total area under the line of
equality (see Figure \ref{fig:gini}). Here the formula used to compute the Gini
coefficient:

\begin{equation}
G(x)=1-\frac{2}{N-1}\sum^{N-1}_{i=1}{q_i}
\label{eq:gini}
\end{equation}

where \emph{N} is the population size, and \emph{q} is the cumulative relative
income.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{images/gini} 

}

\caption{Graphical representation of the Gini index.}\label{fig:gini}
\end{figure}

Run the code below and calculate the Gini coefficient for both the x and
y objects. You can either use the function from the package \texttt{DescTools}
(\protect\hyperlink{ref-signorelli2021}{Signorelli 2021}), or the function from the package \texttt{labstatR}
(\protect\hyperlink{ref-iacus2020}{Iacus and Masarotto 2020}) (remember that R is case-sensitive!). Did you expect this
value of Gini?

Try to understand how the function \texttt{rep()} works using the help.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# generate vector (of incomes)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{541}\NormalTok{, }\DecValTok{1463}\NormalTok{, }\DecValTok{2445}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{4437}\NormalTok{, }\DecValTok{5401}\NormalTok{, }\DecValTok{6392}\NormalTok{, }\DecValTok{8304}\NormalTok{, }\DecValTok{11904}\NormalTok{, }\DecValTok{22261}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{541}\NormalTok{, }\DecValTok{1463}\NormalTok{, }\DecValTok{2445}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{3438}\NormalTok{, }\DecValTok{11904}\NormalTok{, }\DecValTok{22261}\NormalTok{)}

\CommentTok{\# compute Gini coefficient}
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{Gini}\NormalTok{(x)}
\FunctionTok{Gini}\NormalTok{(y)}
\FunctionTok{Gini}\NormalTok{(z)}
\CommentTok{\# or}
\FunctionTok{library}\NormalTok{(labstatR)}
\FunctionTok{gini}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

As mentioned, the Gini Index is nowadays a recognized standard,
nevertheless it has some limitations. In fact, Gini is more sensitive to
changes in the middle of the distribution, than to the tails. In
economics, when we study inequality, we are often more interested in the
tails behavior (namely the top and bottom 10\% of the distribution).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{Palma Ratio} is a particular specification within a family of
inequality measures known as inter-decile ratios (\protect\hyperlink{ref-cobham2016}{Cobham, SchlÃ¶gl, and Sumner 2016}). More
specifically, it is the ratio of national income shares of the top 10\%
of households to the bottom 40\%. It, thus, tells us how many times the
income of the top 10\% of the population is higher than that of the
bottom 40\% (Equation \eqref{eq:palma}).

\begin{equation}
Palma = \frac {top10}{bottom40}
\label{eq:palma}
\end{equation}

The code below computes the Palma Ratio for the \(x\) and \(z\)
distributions created previously. This has to be done manually, by
computing the top 10\% income and the bottom 40\% income with the
cumulative frequencies computed by the function \texttt{gini()} of the package
\texttt{labstatR} (\protect\hyperlink{ref-iacus2020}{Iacus and Masarotto 2020}). Confront the Gini Index and the Palma Ratio, do
you find different results? Why?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(labstatR)}
\CommentTok{\# extracting the cumulative frequencies from the function gini by labstatR}
\NormalTok{q }\OtherTok{\textless{}{-}} \FunctionTok{gini}\NormalTok{(x)}\SpecialCharTok{$}\NormalTok{Q}

\CommentTok{\# computing the Palma Ratio on the cumulative frequencies}
\NormalTok{(}\FunctionTok{quantile}\NormalTok{(q, }\AttributeTok{probs =} \DecValTok{1}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{quantile}\NormalTok{(q, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{9}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{))}\SpecialCharTok{/}
  \FunctionTok{quantile}\NormalTok{(q, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{4}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}

\CommentTok{\# extracting the cumulative frequencies from the function gini by labstatR}
\NormalTok{q2 }\OtherTok{\textless{}{-}} \FunctionTok{gini}\NormalTok{(y)}\SpecialCharTok{$}\NormalTok{Q}

\CommentTok{\# computing the Palma Ratio on the cumulative frequencies}
\NormalTok{(}\FunctionTok{quantile}\NormalTok{(q2, }\AttributeTok{probs =} \DecValTok{1}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{quantile}\NormalTok{(q2, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{9}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{))}\SpecialCharTok{/}
  \FunctionTok{quantile}\NormalTok{(q2, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{4}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}

\CommentTok{\# extracting the cumulative frequencies from the function gini by labstatR}
\NormalTok{q3 }\OtherTok{\textless{}{-}} \FunctionTok{gini}\NormalTok{(z)}\SpecialCharTok{$}\NormalTok{Q}

\CommentTok{\# computing the Palma Ratio on the cumulative frequencies}
\NormalTok{(}\FunctionTok{quantile}\NormalTok{(q3, }\AttributeTok{probs =} \DecValTok{1}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{quantile}\NormalTok{(q3, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{9}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{))}\SpecialCharTok{/}
  \FunctionTok{quantile}\NormalTok{(q3, }\AttributeTok{probs =}\NormalTok{ .}\DecValTok{4}\NormalTok{, }\AttributeTok{type=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{data-visualization}{%
\section{Data visualization}\label{data-visualization}}

Visualizing data is another way to explore the data, or better, a
complement to the quantitative exploratory analysis carried out above.
In fact, as we saw in the example in Figure \ref{fig:fig1}, a simple histogram can
tell us whether the arithmetic mean can give us a realistic
representation of the data, or if more analysis is needed. In this
section we will not see the beautiful graphs for which R is recognized
worldwide (for those I invite you to explore \protect\hyperlink{ggplot2}{Ggplot2}), but fast and
dirty graphs have their potential too, at least for the exploratory
analysis phase.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \textbf{histogram} is a plot that allows the inspection of the data for its
underlying distribution (e.g., normal distribution), outliers, skewness,
etc. If the bars of the histogram are equally spaced bars, the height of
the bin reflects the frequency of each value of the distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \textbf{box plot}, also called a ``box and whisker plot'', is a way to show
the spread and centers of a data set. A box plot is a way to show a five
number summary in a chart. The main part of the chart (the ``box'') shows
where the middle portion of the data is: the interquartile range. At the
ends of the box, you'' find the first quartile (the 25\% mark) and the
third quartile (the 75\% mark). The far bottom of the chart is the
minimum (the smallest number in the set) and the far top is the maximum
(the largest number in the set). Finally, the median
\ul{(}\textbf{not the mean!}\ul{)} is represented by an
horizontal bar in the center of the box.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \textbf{scatter plot} is a bi-dimensional plot in which the dots represent
values for two different numeric variables in the Cartesian space. The
position of each dot indicates each individual data point with respect
to the two variables selected. Scatter plots are used to observe
relationships between two numeric variables (we will deepen their use
later in this chapter).

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig1-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig1-2} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig1-3} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig1-4} \caption{Plot examples. From top-left clowise: Histogram; Density plot; Scatterplot; Boxplot.}\label{fig:fig1}
\end{figure}

The code provided for this part may look a bit more complex than what we
have seen so far. We first draw an histogram and a density plot (which
is a linear representation of the same distribution). Then, we draw some
scatter plots, we add some ``stilish'' (but still basic) arguments and a
linear regression line. Note that the formula used in the \texttt{abline()}
function is something that we will explore better in \protect\hyperlink{linear-regression}{Linear
Regression}, for now only notice that \texttt{y\ \textasciitilde{}\ x\ +\ z\ +\ e} stands for
\texttt{y\ =\ x\ +\ z\ +\ e} in a classic linear expression algebra.\footnote{In order to type the symbol \textasciitilde{} (tilde) it is needed a different
  combination of keys according to the operating system and the
  keyboards.} We then
subset consumption (mpg) data for automatic and manual cars from the
\texttt{mtcars} dataset in order to study the differences among them with a box
plot. We can see how manual cars (1 in the plot) present higher miles
per gallon (thus lower consumption) than automatic cars (0 in the plot),
and the two distributions overlaps only on their tails. In the following
graph, we appreciate how a continuous variable (horse power) is
distributed among cars grouped by a discrete variable (cylinders). In
this way, we are studying a relationship between variable of mixed
types. Note that, in order to tell R that the variable is discrete, we
used the function \texttt{as.factor()} (more details on it in \protect\hyperlink{factor-variables}{Factor
variables}). Moreover, we specified the \(x\) and \(y\) axes labels using
the \texttt{xlab} and \texttt{ylab} arguments.

Try the code, personalize it, and check in the help for supplementary
options.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# histogram}
\FunctionTok{hist}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# density plot}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{density}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg), }\AttributeTok{type =} \StringTok{"l"}\NormalTok{)}

\CommentTok{\# scatterplot of cyl vs hp}
\FunctionTok{plot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl, mtcars}\SpecialCharTok{$}\NormalTok{hp)}

\CommentTok{\# why do we get an error here?}
\FunctionTok{plot}\NormalTok{(mpg, hp)}

\CommentTok{\# stylish scatterplot}
\FunctionTok{plot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp,}
\NormalTok{     mtcars}\SpecialCharTok{$}\NormalTok{mpg,}
     \CommentTok{\# adding titles and axes names}
     \AttributeTok{ylab =} \StringTok{"Miles per Gallon"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Horse Power"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Is consumption related to Horse Power?"}\NormalTok{)}

\CommentTok{\# adding a regression line on the scatterplot}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp, }\AttributeTok{data =}\NormalTok{ mtcars), }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}

\CommentTok{\# subset automatic cars consumption}
\NormalTok{mpg.at }\OtherTok{\textless{}{-}}\NormalTok{ mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{am }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}
\CommentTok{\# and manual cars consumption}
\NormalTok{mpg.mt }\OtherTok{\textless{}{-}}\NormalTok{ mtcars}\SpecialCharTok{$}\NormalTok{mpg[mtcars}\SpecialCharTok{$}\NormalTok{am }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\CommentTok{\# boxplot}
\FunctionTok{boxplot}\NormalTok{(mpg.mt, mpg.at)}

\CommentTok{\# boxplot with a categorical variable}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl), mtcars}\SpecialCharTok{$}\NormalTok{hp, }
     \CommentTok{\# adding axes names}
     \AttributeTok{xlab=}\StringTok{"Cylinders"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Horse Power"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{scaling-data}{%
\section{Scaling data}\label{scaling-data}}

Scaling, or standardizing, data is a useful technical trick that
statisticians and economists use in order to better use data. Some of
the most common situations where scaling is required are visualization,
interpretation, but most of all comparison.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Log scaling} is typically used for plotting and regression analysis.
It is a way of displaying numerical data over a very wide range of
values in a compact way. In fact, typically, when log scaling is needed,
the largest numbers in the data are hundreds or even thousands of times
larger than the smallest numbers. As an example, often exponential
growth curves are displayed on a log scale, otherwise they would
increase too quickly to fit within a small graph and allow a complete
analysis of its variation. R has the function \texttt{log()}to do that.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{4000}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{1000}\NormalTok{,}\DecValTok{350000}\NormalTok{,}\DecValTok{25000}\NormalTok{)}

\CommentTok{\# plotting data}
\FunctionTok{plot}\NormalTok{(x)}

\CommentTok{\# log scaling}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(x)}

\CommentTok{\# plotting scaled data}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{log}\NormalTok{(scaled\_x))}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Ranking} replaces the value assumed by each unit, with the order
number (rank) by which the unit is placed on the list according to a
specific indicator. If two or more units assume the same value, then
they will give the average rank of the positions that they would have
had in case of different values. The transformation into ranks purifies
the indicators from the unit of measure, but it does not preserve the
relative distance between the different units.

The basic form of the \texttt{rank()} function produces a vector that contains
the rank of the values in the vector that was evaluated such that the
lowest value would have a rank of 1 and the second-lowest value would
have a rank of 2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\CommentTok{\# scale ranking data}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}} \FunctionTok{rank}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Relative indices with respect to the range (\textbf{Min-Max}) purifies the
data from their unit of measure such that the features are within a
specific range (i.e.~\(0, 1\))(Equation \eqref{eq:MM}). Min-Max scaling is important when we want
to retain the distance between the data points.

\begin{equation}
r_{ij}=\frac{x_{ij}-\min_ix_{ij}}{\max_ix_{ij}-\min_ix_{ij}}
\label{eq:MM}
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\CommentTok{\# min{-}max scaling data}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}}\NormalTok{ (x}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(x))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{max}\NormalTok{(x)}\SpecialCharTok{{-}}\FunctionTok{min}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The point of \textbf{z-score standardization} is to change your data so that
it can be described as a normal distribution (Equation \eqref{eq:zscore}). Normal distribution, or
Gaussian distribution, is a specific statistical distribution where
equal observations fall above and below the mean, the mean and the
median are the same, and there are more observations closer to the mean
(for more details see {[}The Normal Distribution{]}).

\begin{equation}
z_{ij}=\frac{x_{ij}-\bar x_j}{\sigma_j}
\label{eq:zscore}
\end{equation}

The \texttt{scale()} function, with default settings, will calculate the mean
and standard deviation of the entire vector, then normalize each element
by those values by subtracting the mean and dividing by the standard
deviation (Equation \eqref{eq:zscore}). The resulting distribution will have mean
equal to 0 and standard deviation equal to 1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\CommentTok{\# z{-}score scaling data}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(x)}
\FunctionTok{mean}\NormalTok{(scaled\_x)}
\FunctionTok{sd}\NormalTok{(scaled\_x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In \textbf{index numbers}, the value assumed by each unit is divided by a
reference value belonging to the same distribution or calculated on it
(generally the mean or maximum)(Equation \eqref{eq:in}). This normalization allows
to delete the unit of measure and to keep the relative distance among
the units. If the denominator is the maximum than we obtain values less
or equal to 100.

\begin{equation}
I_{ij}=\frac{x_{ij}}{x^*_{oj}}*100
\label{eq:in}
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\CommentTok{\# indexing with mean}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{/}\FunctionTok{mean}\NormalTok{(x)}

\CommentTok{\# indexing with maximum}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{/}\FunctionTok{max}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In the \textbf{percentage} transformation the value of each unit is divided
by the sum of the values (Equation \eqref{eq:perc}). The sum of the normalized
values are equal to 100.

\begin{equation}
p_{ij}=\frac{x_{ij}}{\sum^n_{i=1}x_{ij}}*100
\label{eq:perc}
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating data}
\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}

\CommentTok{\# percentage indexing}
\NormalTok{scaled\_x }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{probability-sampling}{%
\section{Probability Sampling}\label{probability-sampling}}

Sampling allows statisticians to draw conclusions about a whole by
examining a part. It enables us to estimate characteristics of a
population by directly observing a portion of the entire population.
Researchers are not interested in the sample itself, but in what can be
learned from the survey---and how this information can be applied to the
entire population.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Simple Random sampling} is a powerful tool, we can use it to random
subset data, or generate random distributions with precise
characteristics. Each member of a population has an equal chance of
being included in the sample. Also, each combination of members of the
population has an equal chance of composing the sample. Those two
properties are what defines simple random sampling. Simple random
sampling can be done \emph{with or without replacement}. A sample with
replacement means that there is a possibility that the sampled
observation may be selected twice or more. Usually, the simple random
sampling approach is conducted without replacement because it is more
convenient and gives more precise results.

Simple random sampling is the easiest method of sampling and it is the
most commonly used. Advantages of this technique are that it does not
require any additional information on the sampled unit, nor on the
context (i.e.~the geographic area, gender, age, etc\ldots). Also, since
simple random sampling is a simple method and the theory behind it is
well established, standard formulas exist to determine the sample size,
the estimates and so on, and these formulas are easy to use.

On the other hand, by making no use of context information this method
may sometimes result less efficient in equally representing some strata
of the population, particularly for smaller samples. Finally, if we are
planning a survey and not just sampling already collected data, simple
random sampling may result an expensive and unfeasible method for large
populations because all elements must be identified and labeled prior to
sampling. It can also be expensive if personal interviewers are required
since the sample may be geographically spread out across the population.

We can calculate the probability of a given observation being selected.
Since we know the sample size (\(n\)) and the total population (\(N\)),
calculating the probability of being included in the sample becomes a
simple matter of division (Equation \eqref{eq:srs}).

\begin{equation}
p=\frac{n}{N}*100
\label{eq:srs}
\end{equation}

In R we will use the \texttt{sample()} function. This function requires us to
specify the total population to be sampled (i.e.~a vector between 1 and
1 million), comma, the sample size we are interested in, comma, if we
want replacement to happen (\texttt{TRUE}) or not (\texttt{FALSE}).

In order to have a reproducible code (this is one of the main reasons
for using R as our working tools), we want to be able to select the same
random sample over and over. It may sound complex, but it is not. If we
are not able to select the same random sample today and tomorrow, our
analysis will change slightly every time we run the code. In order to
solve this problem, we will use the \texttt{set.seed()} function at the
beginning of our work. In fact by setting the seed, we tell R which set
of random number generator to use. \textbf{Please be aware that different
versions of the Random Number Generator will select different samples
given the same seed. If you collaborate with other people, specify the
same version of it} (see the code below)\textbf{.}

In the code below, run the first line multiple times and you will see
different samples drawn. If instead, you run the set.seed line plus the
sampling, you will have always the same sample. The last line of code,
gives us an example of how to randomly select 4 rows out of the mtcars
dataset (32 rows) using a combination of the indexing system and the
\texttt{sample()} function. In fact, between square brackets there are: a
vector of row numbers (generated by the \texttt{sample()} function), comma
nothing (which means that we want to keep all the columns of the
dataset). You can run just the sample function first to see the
resulting vector (\texttt{sample(1:32,\ 4,\ replace=FALSE)}), after this run the
whole line (\texttt{samp\_data\ \textless{}-\ mtcars{[}sample(1:32,\ 4,\ replace=FALSE),{]}}). The
code subsets the mtcars lines randomly sampled.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# random sampling 4 numbers out of the series from 1 to 32}
\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{32}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# set seed allows us to reproduce the random operations we do locally}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{32}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# specifying the Random Number Generator version, allows everyone to have the same sample. }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{, }\AttributeTok{kind =} \StringTok{"Mersenne{-}Twister"}\NormalTok{, }\AttributeTok{normal.kind =}  \StringTok{"Inversion"}\NormalTok{)}

\CommentTok{\# sampling the random observations selected}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{samp\_data }\OtherTok{\textless{}{-}}\NormalTok{ mtcars[}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{32}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In \textbf{stratified sampling}, the population is divided into homogeneous,
mutually exclusive groups called strata, and then independent samples
are selected from each stratum. Why do we need to create strata? There
are many reasons, the main one being that it can make the sampling
strategy more efficient. In fact, it was mentioned earlier that we need
a larger sample to get a more accurate estimation of a characteristic
that varies greatly from one unit to the other than for a characteristic
that does not. For example, if every person in a population had the same
salary, then a sample of one individual would be enough to get a precise
estimate of the average salary.

This is the idea behind the efficiency gain obtained with
stratification. If we create strata within which units share similar
characteristics (e.g., income) and are considerably different from units
in other strata (e.g., occupation, type of dwelling) then we would only
need a small sample from each stratum to get a precise estimate of total
income for that stratum. Then we could combine these estimates to get a
precise estimate of total income for the whole population. If we were to
use a simple random sampling approach in the whole population without
stratification, the sample would need to be larger than the total of all
stratum samples to get an estimate of total income with the same level
of precision.

Any of the sampling methods mentioned in this section (and others that
exist) can be used to sample within each stratum. The sampling method
and size can vary from one stratum to another, since each stratum
becomes an independent population. When simple random sampling is used
to select the sample within each stratum, the sample design is called
\textbf{stratified simple random sampling}. A population can be stratified by
any variable that is available for all units on the sampling frame prior
to sampling (i.e., age, gender, province of residence, income, etc.).

The code below applies a Stratified Simple Random Sampling to the Star
Wars dataset. Using the \texttt{Dplyr} package (\protect\hyperlink{ref-wickham2022}{Wickham and Bryan 2022}), we are able to
stratify the data by one variable (eye color), and sample randomly 40\%
of the available population in each stratum (function \texttt{sample\_frac()}),
or 2 observations per stratum (function \texttt{sample\_n()}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading the package and data}
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# stratifying by eye color}
        \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# setting the sample size}
        \FunctionTok{sample\_frac}\NormalTok{(.}\DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        ungroup}

\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# stratifying by eye color}
        \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# setting the sample size}
        \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        ungroup}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\begin{itemize}
\tightlist
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground},
  Exploratory data analysis
\end{itemize}

\newpage

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

Hypothesis testing is a vital process in inferential statistics where
the goal is to use \textbf{sample data} to draw conclusions about an entire
population. A hypothesis test evaluates two mutually exclusive
statements about a population to determine which statement is best
supported by the sample data. These two statements are called the null
hypothesis and the alternative hypothesis.

Why do we need a test? As mentioned, almost always in statistics, we are
dealing with a sample instead of the full population. There are huge
benefits when working with samples because it is usually impossible to
collect data from an entire population. However, the trade off for
working with a manageable sample is that we need to account for the
\textbf{sampling error}. The sampling error is the gap between the sample
statistic and the population parameter. Unfortunately, the value of the
population parameter is not only unknown but usually unknowable.
\textbf{Hypothesis tests provide a rigorous statistical framework to draw
conclusions about an entire population based on a representative
sample.}

As an example, given a sample mean of 350USD and a hypothetical
population mean of 300USD, we can estimate how much is the probability
that the population mean of 300USD is true (\textbf{p-value}), thus the
sampling error, by using the \textbf{T distribution} (which is similar to the
Normal Distribution, but with fattier tails). We will follow these four
steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First, we define the null (\(\bar i_n = \bar i_N\)) and alternative
  hypotheses (\(\bar i_n \neq \bar i_N\)). Much of the emphasis in
  classical statistics focuses on testing a single \textbf{null
  hypothesis}, such as \emph{H\textsubscript{0}:} \emph{the average income between two groups
  is the same}. Of course, we would probably like to discover that
  there is a difference between the average income in the two groups.
  But for reasons that will become clear, we construct a null
  hypothesis corresponding to \textbf{no difference}.
\item
  Next, we construct a test statistic that summarizes the strength of
  evidence against the null hypothesis.
\item
  We then compute a \textbf{p-value} that quantifies the probability of
  having obtained a comparable or more extreme value of the test
  statistic under the \(H_0\) (\protect\hyperlink{ref-lee2019}{Lee 2019}).
\item
  Finally, based on a predefined significance level that we want to
  reach (\textbf{alpha}), we will decide whether to reject or not \(H_0\)
  that the population mean is 300.
\end{enumerate}

Hypothesis tests are not 100\% accurate because they use a random sample
to draw conclusions about entire populations. When we perform a
hypothesis test, there are two types of errors related to drawing an
incorrect conclusion.

\begin{itemize}
\item
  Type I error: rejects a null hypothesis that is true (a false
  positive).
\item
  Type II error: fails to reject a null hypothesis that is false (a
  false negative).
\end{itemize}

A \textbf{significance level}, also known as alpha, is an evidentiary
standard that a researcher sets before the study. It defines how
strongly the sample evidence must contradict the null hypothesis before
we can reject the null hypothesis for the entire population. The
strength of the evidence is defined by the probability of rejecting a
null hypothesis that is true (\protect\hyperlink{ref-frost}{Frost, n.d.}). In other words, it is the
probability that you say there is an effect when there is no effect. In
practical terms, \(alpha=1-pvalue\). For instance, a p-value lower than
0.05 signifies 95\% chances of detecting a difference between the two
values. Higher significance levels require stronger sample evidence to
be able to reject the null hypothesis. For example, to be statistically
significant at the 0.01 requires more substantial evidences than the
0.05 significance level.

As mentioned, the significance level is set by the researcher. However
there are some standard values that are conventionally applied in
different scientific context, and we will follow these conventions.

\begin{longtable}[]{@{}llll@{}}
\caption{Significal levels by convention}\tabularnewline
\toprule\noalign{}
\(\alpha\) & \(p-value\) & symbol & scientific field \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\(\alpha\) & \(p-value\) & symbol & scientific field \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
80\% & \textless0.2 & & social science \\
90\% & \textless0.1 & * & social science \\
95\% & \textless0.05 & ** & all \\
99\% & \textless0.01 & *** & all \\
99.9\% & \textless0.001 & & physics \\
\end{longtable}

Following we will see different type of tests, and their implementation
using R. My suggestion, when working with really small numbers (such as
the p-values), is to suppress the scientific notation in order to have a
clearer sense of the coefficient scale. Use the following code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#suppress scientific notation}
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{9999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~
~

\hypertarget{probability-distributions}{%
\section{Probability Distributions}\label{probability-distributions}}

All \textbf{probability distributions} can be classified as discrete
probability distributions or as continuous probability distributions,
depending on whether they define probabilities associated with discrete
variables or continuous variables. With a discrete probability
distribution, each possible value of the discrete random variable can be
associated with a non-zero probability. Two of the most famous
distributions associated with categorical data are the Binomial and the
Poisson Distributions.

If a random variable is a continuous variable, its probability
distribution is called a continuous probability distribution. This
distribution differs from a discrete probability distribution because
the probability that a continuous random variable will assume a
particular value is zero, and the function that shows the density of the
values of our data is called Probability Density Function, sometimes
abbreviated pdf. Basically, this function represents the outline of the
histogram. The probability that a random variable assumes a value
between \(a\) and \(b\) is equal to the area under the density function
bounded by \(a\) and \(b\) (\protect\hyperlink{ref-damodaran}{Damodaran, n.d.}).

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/pval-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/pval-2} \caption{From left: A density distribution as outline of the hisogram; The probability that the random variable $X$ is higher than or equal to 1.5.}\label{fig:pval}
\end{figure}

In Figure \ref{fig:pval}, the shaded area in the graph represents the probability
that the random variable \(X\) is higher than or equal to \(1.5\). This is a
cumulative probability, also called \textbf{p-value} (please, remember this
definition, as this will be key in order to understand all the remaining
of this chapter). However, the probability that \(X\) is exactly equal to
\(1.5\) would be zero.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{normal distribution}, also known as the Gaussian distribution, is
the most important probability distribution in statistics for
independent, continuous, random variables. Most people recognize its
familiar bell-shaped curve in statistical reports (Figure \ref{fig:pval}).

\emph{The normal distribution is a continuous probability distribution that
is symmetrical around its mean, most of the observations cluster around
the central peak, and the probabilities for values further away from the
mean taper off equally in both directions.}

Extreme values in both tails of the distribution are similarly unlikely.
While the normal distribution is symmetrical, not all symmetrical
distributions are normal. It is the most important probability
distribution in statistics because it accurately describes the
distribution of values for many natural phenomena (\protect\hyperlink{ref-frost}{Frost, n.d.}). As with any
probability distribution, the parameters for the normal distribution
define its shape and probabilities entirely. The normal distribution has
two parameters, the \textbf{mean} and \textbf{standard deviation}.

Despite the different shapes, all forms of the normal distribution have
the following characteristic properties.

\begin{itemize}
\item
  They're all symmetric bell curves. The Gaussian distribution cannot
  be skewed.
\item
  The mean, median, and mode are all equal.
\item
  One half of the population is less than the mean, and the other half
  is greater than the mean.
\item
  The \textbf{Empirical Rule} allows you to determine the proportion of
  values that fall within certain distances from the mean.
\end{itemize}

When we have normally distributed data, the standard deviation becomes
particularly valuable. In fact, we can use it to determine the
proportion of the values that fall within a specified number of standard
deviations from the mean. For example, in a normal distribution, 68\% of
the observations fall within +/- 1 standard deviation from the mean
(Figure \ref{fig:rule}). This property is part of the Empirical Rule, which
describes the percentage of the data that fall within specific numbers
of standard deviations from the mean for bell-shaped curves. For this
reason, in statistics, when dealing with large-enough dataset, the
normal distribution is assumed, even if it is not perfect.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/Senza titolo} 

}

\caption{The Empirical Rule.}\label{fig:rule}
\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{standard normal distribution} is a special case of the normal
distribution where the mean is 0 and the standard deviation is 1. This
distribution is also known as the Z-distribution. A value on the
standard normal distribution is known as a standard score, or a Z-score.
A standard score represents the number of standard deviations above or
below the mean that a specific observation falls. A standard normal
distribution is also the result of the z-score standardization process
(see \protect\hyperlink{scaling-data}{Scaling data})(\protect\hyperlink{ref-frost}{Frost, n.d.}). So, if we have to compare the means of the
distributions of two elements (the weight of apple and pears), by
standardizing we can do it (Figure \ref{fig:apples}).

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/apples-1} 

}

\caption{The comparison between Apples and Pears standardized.}\label{fig:apples}
\end{figure}

~

~

~

\hypertarget{shapiro-wilk-test}{%
\section{Shapiro-Wilk Test}\label{shapiro-wilk-test}}

The Shapiro-Wilk Test is a way to tell if a random sample comes from a
normal distribution. The test gives us a W value and a p-value. The null
hypothesis is that our population is normally distributed. Thus, if we
get a significant p-value (\textless0.1), we will have to reject \(H_0\), and we
will consider the distribution as no normal. The test has limitations,
most importantly it has a bias by sample size. The larger the sample,
the more likely we will get a statistically significant result.

Why is it important to know if a random sample comes from a normal
distribution? Because, if this hypothesis does not hold, we are not able
to use the characteristics of the t-distribution in order to make some
inference and, instead of using a T-Test, we will have to use a Wilcoxon
Rank Sum test or Mann-Whitney test.

In order to perform a Shapiro-Wilk Test in R, we need to use the
following code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Shapiro Test of normality}
\CommentTok{\# H0: the variable is normally distributed}
\CommentTok{\# Ha: the variable is not normally distributed}
\FunctionTok{shapiro.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\FunctionTok{hist}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\end{Highlighting}
\end{Shaded}

~

~
~

\hypertarget{one-sample-t-test}{%
\section{One-Sample T-Test}\label{one-sample-t-test}}

One-Sample T-Test is used to compare the mean of one sample to a
theoretical/hypothetical mean (as in our previous example). We can apply
the test according to one of the three different type of hypotheses
available, depending on our research question:

\begin{itemize}
\tightlist
\item
  The null hypothesis (\(H_0\)) is that the sample mean is equal to the
  known mean, and the alternative (\(H_a\)) is that they are not
  (argument ``two.sided'').
\item
  The null hypothesis (\(H_0\)) is that the sample mean is lower-equal
  to the known mean, and the alternative (\(H_a\)) is that it is bigger
  (argument ``greater'').
\item
  The null hypothesis (\(H_0\)) is that the sample mean is bigger-equal
  to the known mean, and the alternative (\(H_a\)) is that it is smaller
  (argument ``less'').
\end{itemize}

In the code below, given an average consumption of 20.1 miles per
gallon, we want to test if the cars' consumption is on average lower
than 22 miles per gallon (\(H_a\)). And this is in fact true, because we
retrieve a significant p-value (\textless0.05), thus we reject \(H_0\). By using
the package \texttt{gginfernece} (\protect\hyperlink{ref-bratsas2020}{Bratsas, Foudouli, and Koupidis 2020}), we can also plot this test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# H0: cars consumption is on average 22 mpg or higher}
\CommentTok{\# Ha: cars consumption is on average lower than 22 mpg}
\FunctionTok{t.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{mu =} \DecValTok{22}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\CommentTok{\#We reject H0. The average car consumption is significantly lower than 22 mpg.}

\CommentTok{\# plot the t{-}test }
\FunctionTok{library}\NormalTok{(gginference)}
\FunctionTok{ggttest}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{mu =} \DecValTok{22}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"less"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-56-1} 

}

\caption{Single Sample T-Test as plotted by gginference package.}\label{fig:unnamed-chunk-56}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{images/Schermata 2022-03-29 alle 16.14.37} 

}

\caption{Single Sample T-Test output example.}\label{fig:unnamed-chunk-57}
\end{figure}

The output of the test in R gives us many information. The most
important are: the data that we are using (\texttt{mtcars\$mpg}), the t
statistics (if it is lower than 2, we have a significant p-value), the
p-value, the alternative hypothesis (\(H_a\)), the 95\% confidence interval
(so the boundaries of 95\% of the possible averages), and the sample
estimate of the mean.

~

~
~

\hypertarget{unpaired-two-sample-t-test}{%
\section{Unpaired Two Sample T-Test}\label{unpaired-two-sample-t-test}}

One of the most common tests in statistics is the Unpaired Two Sample
T-Test, used to determine whether the means of two groups are equal to
each other (\protect\hyperlink{ref-ziliak2008}{Ziliak 2008}). The assumption for the test is that both
groups are sampled from normal distributions with equal variances. The
null hypothesis (\(H_0\)) is that the two means are equal, and the
alternative (\(H_a\)) is that they are not.

As an example, we can compare the average income between two group of
100 people. First, we explore some descriptive statistics about the two
groups in order to have an idea. And we see that the difference between
the two groups is only 0.69. Note, that we set the seed because we
generate our data randomly from an Normal distribution using the
function \texttt{rnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{income\_a }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean=}\DecValTok{20}\NormalTok{, }\AttributeTok{sd=}\DecValTok{3}\NormalTok{)}
\NormalTok{income\_b }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean=}\FloatTok{18.8}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{)}

\FunctionTok{boxplot}\NormalTok{(income\_a, income\_b)}
\FunctionTok{mean}\NormalTok{(income\_a)}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(income\_b)}
\end{Highlighting}
\end{Shaded}

Before proceeding with the test, we should verify the assumption of
normality by doing a Shapiro-Wilk Test as explained before. However, as
the data are normally distributed by construction, we will skip this
step.

We will then state our hypotheses, and run the Unpaired Two Sample
T-Test. The R function is the same as for the One-Sample T-Test, but
specifying different arguments. We can again plot our test using the
package \texttt{gginfernece} (\protect\hyperlink{ref-bratsas2020}{Bratsas, Foudouli, and Koupidis 2020}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# H0: the two groups have the same income on average}
\CommentTok{\# Ha: the two groups have the different income on average}
\FunctionTok{t.test}\NormalTok{(income\_a, income\_b) }\CommentTok{\# reject H0}
\CommentTok{\# We accept Ha. There is a SIGNIFICANT mean income difference (0.67)}
\CommentTok{\# between the two groups.}

\FunctionTok{library}\NormalTok{(gginference)}
\FunctionTok{ggttest}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(income\_a, income\_b))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-60-1} 

}

\caption{Single Sample T-Test as plotted by gginference package.}\label{fig:unnamed-chunk-60}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{images/Schermata 2022-03-29 alle 16.36.10} 

}

\caption{Two sample T-test output example.}\label{fig:unnamed-chunk-61}
\end{figure}

As for the One-Sample T-Test, R gives us a set of information about the
test. The most important are: the data that we are using, the t
statistics (if it is lower than 2, we have a significant p-value), the
p-value, the alternative hypothesis (\(H_a\)), the 95\% confidence interval
(so the boundaries of 95\% of the possible difference between the
averages of the two groups), and the sample estimates of the mean. Note
that we will have to compute the difference between the two averages ``by
hand'', in case we needed.

If we want to extract the coefficients, the p-value or any other
information given by the t-test in order to build a table, we must
assign the test to an object first, and then extract the data. In the
code below we run the Unpaired Two Sample T-Test for manual and
automatic on the average consumption and weight of the cars. We then
create a table with the average difference and the p-value of both
tests. \emph{Note that the t-test formula is written in a different way than
in the code chunk above, however the meaning of the two expressions is
the same.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a table}
\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ am, }\AttributeTok{data=}\NormalTok{mtcars)}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(wt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ am, }\AttributeTok{data=}\NormalTok{mtcars)}

\NormalTok{taboft }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"coef"}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"difference"}\NormalTok{, }\StringTok{"p{-}value"}\NormalTok{),}
                     \StringTok{"mpg"}\OtherTok{=} \FunctionTok{c}\NormalTok{(t}\SpecialCharTok{$}\NormalTok{estimate[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{t}\SpecialCharTok{$}\NormalTok{estimate[}\DecValTok{2}\NormalTok{], t}\SpecialCharTok{$}\NormalTok{p.value),}
                     \StringTok{"wt"} \OtherTok{=} \FunctionTok{c}\NormalTok{(s}\SpecialCharTok{$}\NormalTok{estimate[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{s}\SpecialCharTok{$}\NormalTok{estimate[}\DecValTok{2}\NormalTok{], s}\SpecialCharTok{$}\NormalTok{p.value))}
\NormalTok{taboft}
\end{Highlighting}
\end{Shaded}

~

~
~

\hypertarget{mann-whitney-u-test}{%
\section{Mann Whitney U Test}\label{mann-whitney-u-test}}

In the case the variable we want to compare are not normally distributed
(aka: the Shapiro test gives us a significant p-value), we can run a
Mann Whitney U Test, also known as Wilcoxon Rank Sum test (\protect\hyperlink{ref-dewinter}{Winter 2013}).
It is used to test whether two samples are likely to derive from the
same population (i.e., that the two populations have the same shape).
Some scholars interpret this test as comparing the medians between the
two populations. Recall that the Unpaired Two Sample T-Test compares the
means between independent groups. In contrast, the research hypotheses
for this test are stated as follows:

\(H_0\): The two populations are equal versus

\(H_a\): The two populations are not equal.

The procedure for the test involves pooling the observations from the
two samples into one combined sample, keeping track of which sample each
observation comes from, and then ranking lowest to highest.

In order to run it, the only thing we need to change is the name of the
formula of the test, as written in the code below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(mpg.at, mpg.mt)}
\end{Highlighting}
\end{Shaded}

~

~
~

\hypertarget{paired-sample-t-test}{%
\section{Paired Sample T-Test}\label{paired-sample-t-test}}

The Paired Sample T-Test is a statistical procedure used to determine
whether the mean difference between two sets of observations is zero.
Common applications of the Paired Sample T-Test include case-control
studies or repeated-measures designs (time series). In fact, each
subject or entity must be measured twice, resulting in \textbf{pairs} \textbf{of
observations}. As an example, suppose we are interested in evaluating
the effectiveness of a company training program. We would measure the
performance of a sample of employees before and after completing the
program, and analyze the differences using a Paired Sample T-Test to see
its impact.

The code below generates a dataset with some before and after values. We
then compute the difference between the two measures and run a
Shapiro-Wilk Test on them in order to verify the normality of the data
distribution. Finally, we can run the proper Paired Sample T-Test on our
data (or a Paired Mann Whitney U Test, if data are not normally
distributed).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\StringTok{"before"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{200.1}\NormalTok{, }\FloatTok{190.9}\NormalTok{, }\FloatTok{192.7}\NormalTok{, }\DecValTok{213}\NormalTok{, }\FloatTok{241.4}\NormalTok{, }\FloatTok{196.9}\NormalTok{, }
                              \FloatTok{172.2}\NormalTok{, }\FloatTok{185.5}\NormalTok{, }\FloatTok{205.2}\NormalTok{, }\FloatTok{193.7}\NormalTok{),}
                 \StringTok{"after"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{392.9}\NormalTok{, }\FloatTok{393.2}\NormalTok{, }\FloatTok{345.1}\NormalTok{, }\DecValTok{393}\NormalTok{, }\DecValTok{434}\NormalTok{, }\FloatTok{427.9}\NormalTok{, }\DecValTok{422}\NormalTok{, }
                             \FloatTok{383.9}\NormalTok{, }\FloatTok{392.3}\NormalTok{, }\FloatTok{352.2}\NormalTok{))}

\CommentTok{\# compute the difference}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{difference }\OtherTok{\textless{}{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{before }\SpecialCharTok{{-}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{after}
\CommentTok{\# H0: the variables are normally distributed}
\FunctionTok{shapiro.test}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{difference) }\CommentTok{\# accept H0}
\FunctionTok{hist}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{difference)}

\FunctionTok{t.test}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{before, df}\SpecialCharTok{$}\NormalTok{after, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{exercises-3}{%
\section{Exercises}\label{exercises-3}}

\newpage

\hypertarget{bivariate-analysis}{%
\chapter{Bivariate Analysis}\label{bivariate-analysis}}

~

~

~

\hypertarget{correlation}{%
\section{Correlation}\label{correlation}}

We will now point our imaginary boat towards the Relationship Islands.
When we see one thing vary, we perceive it changing in some regard, as
the sun setting, the price of goods increasing, or the alternation of
green and red lights at an intersection. Therefore, when two things
covary there are two possibilities. One is that the change in the first
is concomitant with the change in the second, as the change in a child's
age covaries with his height. The older, the taller. When higher
magnitudes on one thing occur along with higher magnitudes on another
and the lower magnitudes on both also co-occur, then the things vary
together positively, and we denote this situation as positive
covariation or \textbf{positive correlation}. The second possibility is that
two things vary inversely or oppositely. That is, the higher magnitudes
of one thing go along with the lower magnitudes of the other and vice
versa. Then, we denote this situation as negative covariation or
\textbf{negative correlation}. This seems clear enough, but in order to be
more systematic about correlation more definition is needed.

We start with the concept of \textbf{covariance}, which represents the
direction of the linear relationship between two variables. By direction
we mean if the variables are directly proportional or inversely
proportional to each other. Thus, if increasing the value of one
variable we have a positive or a negative impact on the value of the
other variable. The values of covariance can be any number between the
two opposite infinities. It's important to mention that the covariance
only \ul{measures the direction of the relationship between two variables
and not its magnitude}, for which the correlation is used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simple covariance}
\FunctionTok{cov}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp, mtcars}\SpecialCharTok{$}\NormalTok{mpg)}
\end{Highlighting}
\end{Shaded}

In probability theory and statistics, \textbf{correlation}, also called
correlation coefficient, indicates the strength and direction of a
linear relationship between two random variables (\protect\hyperlink{ref-davis2021}{Davis 2021}; \protect\hyperlink{ref-madhavan2019}{Madhavan 2019}; \protect\hyperlink{ref-wilson2014}{Wilson 2014}). In general statistical usage, correlation
or co-relation refers to the departure of two variables from
independence. In this broad sense there are several coefficients,
measuring the degree of correlation, adapted to the nature of data. The
best known is the \textbf{Pearson} product-moment correlation coefficient
(\(\rho\)), which is used for linearly related variables and is obtained
by dividing the covariance of the two variables (\(\sigma_{xy}\)) by the
product of their standard deviations (Equation \eqref{eq:pears}.

\begin{equation}
\rho=\frac{\sigma_{xy}}{\sigma_x*\sigma_y}
\label{eq:pears}
\end{equation}

A second measure is the \textbf{Spearman}'s rank correlation coefficient
(\(\rho_{R(x),R(y)}\)) which is a nonparametric measure of rank
correlation defined as the Pearson correlation coefficient between rank
variables (Equation \eqref{eq:spear}).

\begin{equation}
\rho_{R(x),R(y)}=\frac{cov{(R(x),R(y))}}{\sigma_{R(x)}*\sigma_{R(y)}}
\label{eq:spear}
\end{equation}

While Pearson's correlation assesses linear relationships, Spearman's
correlation assesses monotonic relationships (whether linear or not). It
is, thus, fundamental a theoretical assumption taken before choosing the
right measure. Two variables \(x\) and \(y\) are positively correlated if
when \(x\) increases \(y\) increases too and when \(x\) decreases \(y\)
decreases too. The correlation is instead negative when the variable \(x\)
increases and the variable \(y\) decreases and vice-versa. The sign of
\(\rho\) depends only on the covariance (\(\sigma_{xy}\)). The correlation
coefficient varies between -1 (perfect negative linear correlation) and
1 (perfect positive linear correlation), and if it is equal to zero the
variables are independent.

The code below allows you to compute the correlation coefficient between
two variables, and then a correlation matrix, which is a table showing
the correlation coefficients between each pair of variables. \emph{Note that
the first line of code suppresses the scientific notation, allowing the
results of the future calculation to be expressed in decimals even if
they are really really small (or big) values.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# suppress scientific notation}
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{9999}\NormalTok{)}

\CommentTok{\# Correlation coefficients}
\FunctionTok{cor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, mtcars}\SpecialCharTok{$}\NormalTok{disp, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\FunctionTok{cor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, mtcars}\SpecialCharTok{$}\NormalTok{disp, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}

\CommentTok{\# Pearson correlation matrix}
\FunctionTok{cor}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

Because a correlation matrix may result dispersive and difficult to
study, especially when we have a high number of variables, there is the
possibility to visualize it. In fact, if the variables are correlated,
the ``scatter'' points have a trend very known: if the trend is linear the
correlation is linear.

The code below provides you with some examples. The function \texttt{pairs()}
is internal to R and gives us the most basic graph, while the function
\texttt{corrplot()} belongs to the \texttt{corrplot} package and provides a more
stylish and customizable graph (\protect\hyperlink{ref-wei2021}{Wei and Simko 2021}). Finally, the most interesting
(but advanced) version of a pair plot is offered by the \texttt{GGally} package
with the function \texttt{ggpairs()}(\protect\hyperlink{ref-emerson2012}{Emerson et al. 2012}). This function allows us to
draw a correlation matrix that can include whatever kind of value or
graph we want inside of each cell.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# basic pair plot}
\FunctionTok{pairs}\NormalTok{(mtcars)}

\CommentTok{\# the corrplot version}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(mtcars))}

\CommentTok{\# the ggally version}
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{ggpairs}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

The last measure of relationship we will talk about is the \textbf{chi-squared
test} (also known as \(\chi^2\) test). This is a hypothesis test
statistics that comes into play when dealing with contingency tables and
relatively large sample sizes. In simpler terms, the chi-squared test is
primarily employed to assess whether there's a relationship between two
\ul{categorical variables} in terms of their impact on the test
statistic. The purpose of the test is to evaluate how likely the
observed frequencies would be assuming the null hypothesis (\(H_0\)) is
true. Test statistics that follow a \(\chi^2\) distribution occur when the
observations are independent.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# H0: The two variables are independent.}
\CommentTok{\# H1: The two variables relate to each other.}
\FunctionTok{chisq.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl, mtcars}\SpecialCharTok{$}\NormalTok{am)}
\end{Highlighting}
\end{Shaded}

The code above computes the \(\chi^2\) test for the number of cilinders of
a car and the presence of manual transmission. Since we get a p-Value
less than the significance level of 0.05, we reject the null hypothesis
and conclude that the two variables are in fact dependent.

\textbf{CramÃ©r's V} (sometimes referred to as Cramer's \(\phi\)). This is a
measure of association between two categorical variables (or
categorical) based on Pearson's \(\chi^2\) statistic and it was published
by Harald CramÃ©r in 1946. CramÃ©r's V, gives us a method that can be used
when we want to study the intercorrelation between two discrete
variables, and may be used with variables having two or more levels. It
varies from 0 (corresponding to no association between the variables) to
1 (complete association) and can reach 1 only when each variable is
completely determined by the other. Thus \ul{it does not tell us the
direction of the association}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{CramerV}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl, mtcars}\SpecialCharTok{$}\NormalTok{am)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{linear-regression}{%
\section{Linear Regression}\label{linear-regression}}

Linear regression examines the relation of a dependent variable
(response variable) to specified independent variables (explanatory
variables). The mathematical model of their relationship is the
regression equation. The dependent variable is modelled as a random
variable because of uncertainty as to its value, given only the value of
each independent variable. A regression equation contains estimates of
one or more hypothesized regression parameters (``constants''). These
estimates are constructed using data for the variables, such as from a
sample. The estimates measure the relationship between the dependent
variable and each of the independent variables. They also allow
estimating the value of the dependent variable for a given value of each
respective independent variable.

Uses of regression include curve fitting, prediction (including
forecasting of time-series data), modelling of causal relationships, and
testing scientific hypotheses about relationships between variables.
However, we must always keep in mind that a \textbf{correlation does not imply
causation}. In fact, the study of causality is as concerned with the
study of potential causal mechanisms as it is with variation amongst the
data (\protect\hyperlink{ref-Imbens2015}{Imbens and Rubin 2015}).

The difference between correlation and regression is that whether in the
first \(x\) and \(y\) are on the same level, in the latter one \(x\) affect
\(y\), but not the other way around. This has important theoretical
implications in the selection of \(x\) and \(y\). The general form of a
\textbf{simple linear regression} is \eqref{eq:slr}:

\begin{equation}
y=\beta_0+\beta_1x+e
\label{eq:slr}
\end{equation}

where \(\beta_0\) is the intercept, \(\beta_1\) is the slope, and\(e\) is the
error term, which picks up the unpredictable part of the dependent
variable \(y\). We will sometimes describe 1.1 by saying that we are
regressing \(y\) on \(x\). The error term \(e\) is usually posited to be
normally distributed. The \(x\)'s and \(y\)'s are the data quantities from
the sample or population in question, and \(\beta_0\) and \(\beta_1\) are
the unknown parameters (``constants'') to be estimated from the data.
Estimates for the values of \(\beta_0\) and \(\beta_1\) can be derived by
the method of \ul{ordinary least squares}. The method is called
``least squares'', because estimates of \(\beta_0\) and \(\beta_1\) minimize
the sum of squared error estimates for the given data set (Equation \eqref{eq:rss}), thus
minimizing:

\begin{equation}
RSS=e_1^2+e_2^2+...+e_n^2
\label{eq:rss}
\end{equation}

The estimates are often denoted by \(\hat\beta_0\) and \(\hat\beta_1\) or
their corresponding Roman letters. It can be shown that least squares
estimates are given by

\begin{equation}
\hat\beta_1=\frac{\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^N(x_i-\bar{x})^2} \\
\hat\beta_0=\bar{y}-\hat\beta_1 \bar{x}
\end{equation}

where \(\bar{x}\) is the mean (average) of the \(x\) values and \(\bar{y}\)
is the mean of the \(y\) values.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-70-1} 

}

\caption{Plot of the residuals from a regression line.}\label{fig:unnamed-chunk-70}
\end{figure}

As we said, before building our first regression model, it is important
to have an idea about the theoretical relationship between the variable
we want to study. In the case of the dataset \texttt{cars}, we know that speed
has an impact on the breaking distance of a car (variable dist) from our
physics studies. We can thus say that dist is our dependent variable
(\(y\)), and speed is our independent variable (\(x\)).

We can fit the model in the following way. Inside the \texttt{lm()} function
place the dependent variable \textasciitilde{} independent variable, comma the dataset
in which they are contained. \emph{Note that the formula 1.1 in the \texttt{lm()}
function is written as \texttt{y\ \textasciitilde{}\ x}.}\footnote{In order to type the symbol \textasciitilde{} (tilde) it is needed a different
  combination of keys according to the operating system and the
  keyboards.} The code chunk below draws a
summary and the plot of the relationship between the speed of a car and
the breaking distance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# linear regression}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(dist }\SpecialCharTok{\textasciitilde{}}\NormalTok{ speed, }\AttributeTok{data =}\NormalTok{ cars)}
\FunctionTok{summary}\NormalTok{(fit)}

\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(cars, }\FunctionTok{aes}\NormalTok{(dist, speed))}\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}
        \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output of the summary of our regression model (Figure \ref{fig:lm}) must be
read in the following way in order to have a brief idea on the model
that we built. The first step is to look at the Multiple R-squared, this
number tells us which percentage of the data is explained by the model
(65.1\% in this case), and thus how significant our model is. If the
model has an appreciable power to explain our data, we then analyze the
coefficients' estimates and their significance level. We see here that
speed has a p-value lower than 0.001 (thus highly significant\footnote{For a discussion on the meaning of p-value go back to the previous
  chapter.}) and
that one unit increase in speed means 3.9 units of increase in distance.
However, there is also a slightly significant intercept, \(\beta_0\),
which means that there are factors which are not present in the model
and that could further explain the behavior of our dependent variable.
From a theoretical perspective this makes sense, in fact, the type of
tires, the weight of the car, the weather conditions (etc..) are some
additional factors that are missing in our model and could improve our
understanding of the phenomenon.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{images/Schermata 2022-07-07 alle 17.24.44} 

}

\caption{Linear regression model summary output.}\label{fig:lm}
\end{figure}

Whether simple linear regression is a useful approach for predicting a
response on the basis of one single independent variable, we often have
more than one independent variable (\(x\)) that influence the dependent
variable (\(y\)). Instead of fitting a separate simple linear regression
model for each \(x\), a better approach is to extend the simple linear
regression model. We can do this by giving each independent variable
(\(x\)) a separate slope coefficient in a single model. In general,
suppose that we have \(p\) distinct independent variables (\(x\)). Then the
\textbf{multiple linear regression model} takes the form \eqref{eq:mlr}:

\begin{equation}
y\approx\beta_0+\beta_1x_1+\beta_2x_2...+\beta_px_p+e
\label{eq:mlr}
\end{equation}

where \(x_p\) represents the \(p^{th}\) independent variable, and \(\beta_p\)
quantifies the association between that variable and the dependent
variable \(y\). We interpret \(\beta_p\) as the average effect on \(y\) of a
one unit increase in \(x_p\), \ul{holding all other independent variables
fixed}. In other words, we will still have the effect of the
increase of one independent variable (\(x\)) over our dependent variable
(\(y\)), but ``controlling'' for other factors.

In order to include more independent variables into our model, we use
the plus sign. The formula in R will then be \texttt{y\ \textasciitilde{}\ x1\ +\ x2\ +\ x3}. If we
want to use all the variables present in our dataset as independent
variables, the formula in R will be \texttt{y\ \textasciitilde{}\ .} where the dot stands for
``everything else''. Another possibility is to have an interaction term.
An interaction effect exists when the effect of an independent variable
on a dependent variable changes, depending on the value(s) of one or
more other independent variables (i.e.~\(y=x*z\)). However, interaction
terms are out of the scope of this manual.

To make an example, we can use the dataset \texttt{swiss}, which reports Swiss
fertility and socioeconomic data from the year 1888. Following some
models with a different number of variables used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# multiple linear regression}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Fertility }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ swiss)}
\FunctionTok{summary}\NormalTok{(fit2)}

\NormalTok{fit3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Fertility }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Education }\SpecialCharTok{+}\NormalTok{ Agriculture, }\AttributeTok{data =}\NormalTok{ swiss)}
\FunctionTok{summary}\NormalTok{(fit3)}
\end{Highlighting}
\end{Shaded}

When we perform multiple linear regression, we are usually interested in
answering a few important questions in order to reach our goal: find the
model, with the lower number of independent variables that best explains
the outcome. The questions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Is at least one of the \(x_1, x_2, . . . ,x_p\) useful in explaining
  the independent variable \(y\)?
\item
  Do all the \(x_1, x_2, . . . ,x_p\) help to explain \(y\), or is only a
  subset of them sufficient?
\item
  How well does the model fit the data?
\end{enumerate}

In order to answer the questions above we need to do a comparative
analysis between the models. Analysis of Variance (\textbf{ANOVA}) consists
of calculations that provide information about levels of variability
within a regression model and form a basis for tests of significance. We
can thus use the function \texttt{anova()} in order to compare multiple
regression models. When ANOVA is applied in practice, it actually
becomes a variable selection method: if the full model is significantly
different (null hypothesis rejected), the variable/s added by the full
model is/are considered useful for prediction. \emph{Statistic textbooks
generally recommend to test every predictor for significance.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ANOVA testing}
\FunctionTok{anova}\NormalTok{(fit2, fit3)}
\end{Highlighting}
\end{Shaded}

~

~

~

\hypertarget{logistic-regressions}{%
\section{Logistic Regressions}\label{logistic-regressions}}

Whether independent variables can be either continuous or categorical,
if our dependent variable that is logical (1,0 or TRUE,FALSE), we will
have to run a different kind of regression model: the logistic model (or
logit model). This model gives us the probability of one event (out of
two alternatives) taking place by having the log-odds (the logarithm of
the odds) for the event be a linear combination of one or more
independent variables.

Using the \texttt{glm()} function (Generalized Linear Models), and the argument
\texttt{family="binomial"}, we can fit our logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit4 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(am }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mpg, }\AttributeTok{family=}\StringTok{"binomial"}\NormalTok{, mtcars)}
\FunctionTok{summary}\NormalTok{(fit4)}

\CommentTok{\# plotting the logit model}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(mtcars, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{mpg, }\AttributeTok{y=}\NormalTok{am)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"glm"}\NormalTok{, }\AttributeTok{color=}\StringTok{"green"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }
              \AttributeTok{method.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{family=}\NormalTok{binomial))}
\end{Highlighting}
\end{Shaded}

Its output (Figure \ref{fig:logi}) must be interpreted as follows. An increase in
miles per gallons (mpg) increases the probability that the car has
automatic transmission by 31\%, and this increase is statistically
significant (p-value\textless0.01). In this case we do not have the Multiple
R-squared to assess the significance of the model, but we will have to
look at the Residual deviance \textbf{tells us how well the response variable
can be predicted by a model with p predictor variables}. The lower the
value, the better the model is able to predict the value of the response
variable.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\linewidth,]{images/Schermata 2022-07-07 alle 18.49.38} 

}

\caption{Logistic regression model summary output.}\label{fig:logi}
\end{figure}

~

~

~

\hypertarget{exercises-4}{%
\section{Exercises}\label{exercises-4}}

\begin{itemize}
\item
  James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2021). An
  Introduction to Statistical Learning (Vol. 103). Springer New York.
  \href{https://www.statlearning.com}{Available here}. Chapter 3.6 and 3.7
  exercises from 8 to 15.
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground},
  section 7 - T-tests and Regressions
\end{itemize}

\newpage

\hypertarget{multivariate-analysis}{%
\chapter{Multivariate Analysis}\label{multivariate-analysis}}

Often bivariate analysis is not enough, and this is particularly true in
this historical period, when data availability is not an issue anymore.
One of the most common problems instead is how to analyze big (huge)
dataset. Multivariate analysis methods give us the possibility to
somehow reduce the dimensionality of the data, allowing a clearer
understanding of the relationships present in it.

~

~

~

\hypertarget{cluster-analysis}{%
\section{Cluster Analysis}\label{cluster-analysis}}

Cluster analysis is an exploratory data analysis tool for solving
classification problems. Its objective is to sort observations into
groups, or clusters, so that the degree of association is strong between
members of the same cluster and weak between members of different
clusters. Each cluster thus describes, in terms of the data collected,
the class to which its members belong; and this description may be
abstracted through use from the particular to the general class or type.
Cluster analysis is thus a tool of discovery. It may reveal associations
and structure in data which, though not previously evident, nevertheless
are sensible and useful once found. The results of cluster analysis may
contribute to the definition of a formal classification scheme, such as
a taxonomy for related animals, insects or plants; or suggest
statistical models with which to describe populations; or indicate rules
for assigning new cases to classes for identification and diagnostic
purposes; or provide measures of definition, size and change in what
previously were only broad concepts; or find exemplars to represent
classes. Whatever business you're in, the chances are that sooner or
later you will run into a classification problem.

Cluster analysis includes a broad suite of techniques designed to find
groups of similar items within a data set. Partitioning methods divide
the data set into a number of groups predesignated by the user.
Hierarchical cluster methods produce a hierarchy of clusters from small
clusters of very similar items to large clusters that include more
dissimilar items (\protect\hyperlink{ref-abdi2010}{Abdi and Williams 2010}). Both clustering and PCA seek to simplify
the data via a small number of summaries, but their mechanisms are
different: PCA looks to find a low-dimensional representation of the
observations that explain a good fraction of the variance; clustering
looks to find homogeneous subgroups among the observations.

As mentioned, when we cluster the observations of a data set, we seek to
partition them into distinct groups so that the observations within each
group are quite similar to each other, while observations in different
groups are quite different from each other. Of course, to make this
concrete, we must define what it means for two or more observations to
be similar or different.

In order to define the similarity between observations we need a
``metric''. The Eucludean distance is the most common metric used in
cluster analysis, but many others exist (see the help in the \texttt{dist()}
function for more detail). We also need to choose which algorithm we
want to apply in order for the computer to assign the observations to
the right cluster.

Remember that Clustering has to be performed on \ul{continuous scaled
data}. If the variables you want to analyze are categorical,
you should use scaled dummies.

~

\hypertarget{hierarchical-clustering}{%
\subsection{Hierarchical Clustering}\label{hierarchical-clustering}}

Hierarchical methods usually produce a graphical output known as a
dendrogram or tree that shows this hierarchical clustering structure.
Some hierarchical methods are divisive, that progressively divide the
one large cluster comprising all of the data into two smaller clusters
and repeat this process until all clusters have been divided. Other
hierarchical methods are agglomerative and work in the opposite
direction by first finding the clusters of the most similar items and
progressively adding less similar items until all items have been
included into a single large cluster. Hierarchical methods are
particularly useful in that they are not limited to a predetermined
number of clusters and can display similarity of samples across a wide
range of scales.

Bottom-up or agglomerative clustering is the most common type of
hierarchical clustering, and refers to the fact that a dendrogram is
built starting from the leaves and combining clusters up to the trunk.
As we move up the tree, some leaves begin to fuse into branches. These
correspond to observations that are similar to each other. As we move
higher up the tree, branches themselves fuse, either with leaves or
other branches. The earlier (lower in the tree) fusions occur, the more
similar the groups of observations are to each other. On the other hand,
observations that fuse later (near the top of the tree) can be quite
different. \textbf{The height of this fusion, as measured on the vertical
axis, indicates how different the two observations are.}

Hierarchical clustering allows also to select the method you want to
apply. \emph{Ward's} minimum variance method aims at finding compact,
spherical clusters. The \emph{complete linkage} method finds similar
clusters. The \emph{single linkage} method (which is closely related to the
minimal spanning tree) adopts a `friends of friends' clustering
strategy. The other methods can be regarded as aiming for clusters with
characteristics somewhere between the single and complete link methods.

The first step in order to proceed with hierarchical cluster analysis is
to compute the ``distance matrix'', which represents how distance are the
observations among themselves. For this step, as mentioned before, the
Euclidean distance is one of the most common metrics used. We then
properly run the hierarchical cluster analysis (function \texttt{hclust()})
specifying the method for complete linkage. Finally we plot the
dendogram. \emph{It is important that each row of the dataset has a name
assigned (see \protect\hyperlink{row-names}{Row Names})}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Euclidean distance}
\NormalTok{dist }\OtherTok{\textless{}{-}} \FunctionTok{dist}\NormalTok{(swiss, }\AttributeTok{method=}\StringTok{"euclidean"}\NormalTok{)}
\CommentTok{\# Hierarchical Clustering with hclust}
\NormalTok{hc }\OtherTok{\textless{}{-}} \FunctionTok{hclust}\NormalTok{(dist, }\AttributeTok{method=}\StringTok{"complete"}\NormalTok{)}
\CommentTok{\# Plot the result}
\FunctionTok{plot}\NormalTok{(hc, }\AttributeTok{hang=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{cex=}\NormalTok{.}\DecValTok{5}\NormalTok{)}
\FunctionTok{rect.hclust}\NormalTok{(hc, }\AttributeTok{k=}\DecValTok{3}\NormalTok{, }\AttributeTok{border=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=1\linewidth,]{_main_files/figure-latex/fig2-1} \caption{Dendogram plot.}\label{fig:fig2}
\end{figure}

The last line of code adds the rectangles highlighting 3 clusters. The
number of cluster is a personal choice, there is no strict rule about
how to identify them. The common rule of thumb is to look at the height
(vertical axes of the dendogram) and cut it where the highest jump
occurs between the branches. In this case it corresponds to 3 clusters.

Because of its agglomerative nature, clusters are sensitive to the order
in which samples join, which can cause samples to join a grouping to
which it does not actually belong. In other words, if groups are known
beforehand, those same groupings may not be produced from cluster
analysis. Cluster analysis is sensitive to both the distance metric
selected and the criterion for determining the order of clustering.
Different approaches may yield different results. Consequently, the
distance metric and clustering criterion should be chosen carefully. The
results should also be compared to analyses based on different metrics
and clustering criteria, or to an ordination, to determine the
robustness of the results.Caution should be used when defining groups
based on cluster analysis, particularly if long stems are not present.
Even if the data form a cloud in multivariate space, cluster analysis
will still form clusters, although they may not be meaningful or natural
groups. Again, it is generally wise to compare a cluster analysis to an
ordination to evaluate the distinctness of the groups in multivariate
space. \emph{Transformations may be needed to put samples and variables on
comparable scales; otherwise, clustering may reflect sample size or be
dominated by variables with large values.}

~

\hypertarget{k-means-clustering}{%
\subsection{K-Means clustering}\label{k-means-clustering}}

K-means clustering is a simple and elegant approach for partitioning a
data set into K distinct, non-overlapping clusters. To perform K-means
clustering, we must \ul{first specify the desired number of clusters
K}; then the K-means algorithm assigns each observation to
exactly one of the K clusters. The idea behind K-means clustering is
that a good clustering is one for which the within-cluster variation is
as small as possible. The K-Means algorithm, in an iteratively way,
defines a centroid for each cluster, which is a point (imaginary or
real) at the center of a cluster, and adjusts it until there is no
possible change anymore. The metric used is the Squared Sum of Euclidean
distances. \ul{The main limitation of K-means is understanding which is the
right k prior to the analysis}. Also, K-means is an
algorithm that tends to perform well only with spherical clusters, as it
looks for centroids.

The function \texttt{kmeans()} allows to run K-Means clustering given the
preferred number of clusters (centers). The results can be appreciated
by plotting the clusters using the \texttt{fviz\_cluster()} function from the
package factoextra (\protect\hyperlink{ref-kassambara}{Kassambara and Mundt, n.d.}). \emph{Note that in order to plot the
clusters from K-means the function automatically reduces the
dimensionality of the data via PCA and selects the first two
components.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the k{-}means for the preferred number of clusters}
\NormalTok{kc }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(swiss, }\AttributeTok{centers=}\DecValTok{3}\NormalTok{)}
\FunctionTok{library}\NormalTok{(factoextra) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{data=}\NormalTok{swiss, }\AttributeTok{cluster=}\NormalTok{kc}\SpecialCharTok{$}\NormalTok{cluster))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{_main_files/figure-latex/fig3-1} 

}

\caption{K-means clustering.}\label{fig:fig3}
\end{figure}

~

\hypertarget{the-silhouette-plot}{%
\subsection{The silhouette plot}\label{the-silhouette-plot}}

Silhouette analysis can be used to study the separation distance between
the resulting clusters. This analysis is usually carried out prior to
any clustering algorithm (\protect\hyperlink{ref-syakur2018}{Syakur et al. 2018}). In fact, the silhouette plot
displays a measure of how close each point in one cluster is to points
in the neighboring clusters and thus provides a way to assess parameters
like number of clusters visually. This measure has a range of \(-1, 1\).
The value of \emph{k} that maximizes the silhouette width is the one
minimizing the distance within the clusters and maximizing the distance
between them. However, it is important to remark that \ul{the silhouette
plot analysis provides just a rule of thumb for cluster
selection}.

In the case of the \texttt{swiss} dataset, the silhouette plot suggests the
presence of only two clusters both using hierarchical clustering and
K-Means. As we have seen previously this is not properly true.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#silhouette method}
\FunctionTok{library}\NormalTok{(factoextra) }
\FunctionTok{fviz\_nbclust}\NormalTok{(swiss, }\AttributeTok{FUN =}\NormalTok{ hcut)}
\FunctionTok{fviz\_nbclust}\NormalTok{(swiss, }\AttributeTok{FUN =}\NormalTok{ kmeans)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig4-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/fig4-2} \caption{From left: Hierarchical clustering silhouette plot; K-means clustering silhouette plot.}\label{fig:fig4}
\end{figure}

~

~

~

\hypertarget{heatmap}{%
\section{Heatmap}\label{heatmap}}

A Heatmap is a two-way display of a data matrix in which the individual
cells are displayed as colored rectangles. The color of a cell is
proportional to its position along a color gradient. Usually, the
columns (variables) of the matrix are shown as the columns of the heat
map and the rows of the matrix are shown as the rows of the heat map, as
in the example below. The order of the rows is determined by performing
hierarchical cluster analyses of the rows (it is even possible to
appreciate the corresponding dendogram on the side of the heatmap). This
tends to position similar rows together on the plot. The order of the
columns is determined similarly. Usually, a clustered Heatmap is made on
variables that have similar scales, such as scores on tests. If the
variables have different scales, the data matrix must first be scaled
using a standardization transformation such as z-scores or proportion of
the range.

In the heatmap you can see that V. of Geneve is a proper outlier in
terms of Education and share of people involved in the agricultural
sector. For more advanced Heatmaps, please \href{https://www.datanovia.com/en/lessons/heatmap-in-r-static-and-interactive-visualization/}{visit this link.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#heatmap}
\NormalTok{dataMatrix }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(swiss)}
\FunctionTok{heatmap}\NormalTok{(dataMatrix, }\AttributeTok{cexCol=}\NormalTok{.}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{_main_files/figure-latex/unnamed-chunk-75-1} 

}

\caption{Heatmap.}\label{fig:unnamed-chunk-75}
\end{figure}

~

~

~

\hypertarget{principal-component-analysis}{%
\section{Principal Component Analysis}\label{principal-component-analysis}}

Principal Component Analysis (PCA) is a way of identifying patterns in
data, and expressing the data in such a way as to highlight their
similarities and differences Jolliffe and Cadima (\protect\hyperlink{ref-jolliffe2016}{2016}). Since patterns
in data can be hard to find in data of high dimension, where the luxury
of graphical representation is not available, PCA is a powerful tool for
analysing data. The other main advantage of PCA is that once we have
found these patterns in the data, we compress the data (ie. by reducing
the number of dimensions) without much loss of information.

\ul{The goal of PCA is to reduce the dimensionality of the data while
retaining as much as possible of the variation present in the
dataset.}

PCA is:

\begin{itemize}
\tightlist
\item
  a statistical technique used to examine the interrelations among a
  set of variables in order to identify the underlying structure of
  those variables.
\item
  a non-parametric analysis and the answer is unique and independent
  of any hypothesis about data distribution.
\end{itemize}

These two properties can be regarded as weaknesses as well as strengths.
Since the technique is non-parametric, no prior knowledge can be
incorporated. Moreover, PCA data reduction often incurs a loss of
information.

The assumptions of PCA:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ul{Linearity}. Assumes the data set to be linear
  combinations of the variables.
\item
  \ul{The importance of mean and covariance}. There is no
  guarantee that the directions of maximum variance will contain good
  features for discrimination.
\item
  \ul{That large variances have important dynamics}. Assumes
  that components with larger variance correspond to interesting
  dynamics and lower ones correspond to noise.
\end{enumerate}

The first principal component can equivalently be defined as a direction
that maximizes the variance of the projected data. The second will
represent the direction that maximizes the variance of the projected
data, given the first component, and thus it will be uncorrelated with
it. And so on for the other components. Once we have computed the
principal components, we can plot them against each other in order to
produce low-dimensional views of the data. More generally, we are
interested in knowing the proportion of variance explained by each
principal component and analyse the ones that maximize it.

It is important to remember that PCA has to be performed on \ul{continuous
scaled data}. If the variables we want to analyze are
categorical, we should use scaled dummies or correspondence analysis.
Another fundamental aspect is that each row of the dataset must have a
name assigned to it, otherwise we will not see the names corresponding
to each observation in the plot. See \protect\hyperlink{scaling-data}{Scaling data} and \protect\hyperlink{row-names}{Row Names} for
more information on the procedure.

Using the codes below, we are able to reduce the dimensionality in the
\texttt{swiss} dataset. This dataset presents only percentage values, thus all
the variables are already continuous and in the same scale. Moreover,
each observation (village) has its row named accordingly, so we do not
need to do any transformation prior to the analysis. One we are sure
about these two aspect, we can start our analysis by studying the
correlation between the different variables that compose the dataset. We
do so because we know the PCA works best when we have correlated
variables that can be ``grouped'' within the same principal component by
the algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Correlation Matrix}
\FunctionTok{cor}\NormalTok{(swiss)}
\end{Highlighting}
\end{Shaded}

The next step is to properly run the PCA's algorithm and assign it to an
object. If the values are not on the same scale, it is better to set the
argument scale equal to TRUE. This argument sets the PCA to work on the
correlation matrix, instead of on the covariance matrix, allowing to
start from values all centered around 0 and with the same scale. The
object created by \texttt{prcomp()} is a ``list''. A list can contain dataframes,
vectors, variables, etc\ldots{} In order to explore what is inside of a list
you can use the \texttt{\$} sign or the \texttt{{[}{]}} (nested square brackets). The
summary and the scree plot (command \texttt{fviz\_eig()} from the package
\texttt{factoextra}) are the first thing to look at because they tell us how
much of the variance is explained by each component (\protect\hyperlink{ref-kassambara}{Kassambara and Mundt, n.d.}). The
higher are the first components, the more accurate our PCA will be. In
this case, the first two components retain 73.1\% of the total
variability within the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\CommentTok{\#running the PCA}
\NormalTok{pca\_swiss }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(swiss, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pca\_swiss)}

\CommentTok{\#visualizing the PCA}
\FunctionTok{fviz\_eig}\NormalTok{(pca\_swiss)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{_main_files/figure-latex/unnamed-chunk-78-1} 

}

\caption{Screeplot.}\label{fig:unnamed-chunk-78}
\end{figure}

The final step is to plot the graph of the variables, where positively
correlated variables point to the same side of the plot, while
negatively correlated variables point to opposite sides of the graph. We
can see how Education is positively correlated with the PC2, while
Fertility and Catholic are negatively correlated with the same dimension
and thus also with Education. This result confirms what we already saw
in the correlation matrix above.

The graph of individuals, instead, tells us how the observations (the
villages in this case) are related to the components. Thus, we can
conclude by saying that V. de Geneve has some peculiar characteristics
as compared with the other villages, in fact it has the highest
education level and lowest fertility and share of catholic people.

The biplot overlays the previous two graphs allowing a more immediate
interpretation. However if we have many variables and observations, this
plot can be do messy to be analyzed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Graph of variables}
\FunctionTok{fviz\_pca\_var}\NormalTok{(}
\NormalTok{        pca\_swiss,}
        \AttributeTok{col.var =} \StringTok{"contrib"}\NormalTok{,}
        \AttributeTok{repel =} \ConstantTok{TRUE}     \CommentTok{\# Avoid text overlapping}
\NormalTok{)}

\CommentTok{\# Graph of individuals}
\FunctionTok{fviz\_pca\_ind}\NormalTok{(}
\NormalTok{        pca\_swiss,}
        \AttributeTok{col.ind =} \StringTok{"cos2"}\NormalTok{,}
        \AttributeTok{repel =} \ConstantTok{TRUE}
\NormalTok{)}

\CommentTok{\# Biplot of individuals and variables}
\FunctionTok{fviz\_pca\_biplot}\NormalTok{(pca\_swiss, }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-79-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-79-2} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-79-3} \caption{From top-left clockwise: Graph of variables, positive correlated variables point to the same side of the plot; Graph of individuals, individuals with a similar profile are grouped together; Biplot of individuals and variables.}\label{fig:unnamed-chunk-79}
\end{figure}

As a robustness check, and also to better understand what the algorithm
does, we can compare the rotation of the axis before and after the pca
looking at the pairs plot. In the pair graph after the PCA we expect to
see no relationship between all the principal component, as this is the
aim of the PCA algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pairs before PCA}
\FunctionTok{pairs}\NormalTok{(swiss, }\AttributeTok{panel=}\NormalTok{panel.smooth, }\AttributeTok{col=}\StringTok{"\#6da7a7"}\NormalTok{)}

\CommentTok{\# Pair after PCA}
\FunctionTok{pairs}\NormalTok{(pca\_swiss}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{panel=}\NormalTok{panel.smooth, }\AttributeTok{col=}\StringTok{"\#6da7a7"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics{_main_files/figure-latex/unnamed-chunk-80-1} \includegraphics{_main_files/figure-latex/unnamed-chunk-80-2} \caption{From top: Pairs graph before PCA; Pairs graph after PCA.}\label{fig:unnamed-chunk-80}
\end{figure}

~

~

~

\hypertarget{classification-and-regression-trees}{%
\section{Classification And Regression Trees}\label{classification-and-regression-trees}}

Classification and Regression Trees (CART) are simple and useful methods
for interpretation that allow to understand the underlying relationship
between one dependent variable and multiple independent variables
Temkin et al. (\protect\hyperlink{ref-temkin1995}{1995}). As compared to multiple linear regression
analysis, this set of methods does not retrieve the impact of one
variable on the outcome controlling for a set of other independent
variables. It instead recursively looks at the most significant
relationship between a set of variables, subsets the given data
accordingly, and finally draws a tree. CART are a great tool for
communicating complex relationships thanks to their visual output,
however they have generally a poor predicting performance.

Depending on the dependent variable type it is possible to apply a
Classification (for discrete variables) or Regression (for continuous
variables) Tree. In order to build a \textbf{regression tree}, the algorithm
first uses recursive binary splitting to grow a large tree, stopping
only when each terminal node has fewer than some minimum number of
observations. Beginning at the top of the tree, it splits the data into
2 branches, creating a partition of 2 spaces. It then carries out this
particular split at the top of the tree multiple times and chooses the
split of the features that minimizes the Residual Sum of Squares (RSS).
Then it repeats the procedure for each subsequent split. A
\textbf{classification tree}, instead, is built by predicting which
observation belongs to the most commonly occurring class in the region
to which it belongs. However, in the classification setting, RSS cannot
be used as a criterion for making the binary splits. The algorithm then
uses Classification Error Rate, Gini Index or Cross-Entropy.

In interpreting the results of a classification tree, you are often
interested not only in the class prediction corresponding to a
particular terminal node region, but also in the class proportions among
the training observations that fall into that region. The image below
offers a clear understanding of how a classification tree must be read
(\protect\hyperlink{ref-lee2018}{J. Lee 2018}). We first state our research question. The answer proposed
depend on the variables included in our data. In this case we will
accept the new job offer only if the salary is higher than \$50k, it
takes less than one hour to commute, and the company offers free coffee.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{images/classification-tree_ygvats copia} 

}

\caption{Classification tree explanation. Source Lee (2018).}\label{fig:unnamed-chunk-81}
\end{figure}

The main question is when to stop splitting? Clearly, if all of the
elements in a node are of the same class it does not do us much good to
add another split. Doing so would usually decrease the power of our
model. This is known as \ul{overfitting}. As omniscient
statisticians, we have to be creative with the rules for termination. In
fact, there is no one-size-fits-all-rule in this case, but the algorithm
provides a number of parameters that we can set. This process is called
``pruning'', as if we were pruning a tree to make it smaller and simpler.

\textbf{Manual pruning}, is performed starting from fully grown (over-fitted)
trees and setting parameters such as the minimum number of observations
that must exist in a node in order for a split to be attempted
(\texttt{minsplit}), the minimum number of observations in any terminal node
(\texttt{minbucket}), and the maximum depth of any node of the final tree, with
the root node counted as depth 0 (\texttt{maxdepth}), just to mention the most
important ones. There is no rule for setting these parameters, and here
comes the art of the statistician.

\textbf{Automatic pruning}, instead, is done by setting the complexity
parameter (\texttt{cp}). The complexity parameter is a combination of the size
of a tree and the ability of the tree to separate the classes of the
target variable. If the next best split in growing a tree does not
reduce the tree's overall complexity by a certain amount, then the
process is terminated. The complexity parameter by default is set to
0.01. Setting it to a negative amount ensures that the tree will be
fully grown. But which is the right value for the complexity parameter?
Also in this case, there is not a perfect rule. The rule of thumb is to
set it to zero, and then select the complexity parameter that minimizes
the level of the cross-validated error rate.

In our example below, we will use the dataset \texttt{ptitianic}, from the
package \texttt{rpart.plot}. The dataset provides a list of passengers on board
fo the famous ship Titanic which sank in the North Atlantic Ocean on 15
April 1912. It tells us whether the passenger died or survived, the
passenger class, gender, age, the number of sibling or spouses aboard,
and the number of parents or children aboard. Our aim is to understand
which were the factors allowing the passenger to survive.

The package \texttt{rpart} allows us to run the CART algorithms
(\protect\hyperlink{ref-therneau2022}{Therneau and Atkinson 2022}). The \texttt{rpart()} function needs the specification of the
formula using the same syntax used for multiple linear regressions, the
source of data, and the method (if \texttt{y} is a survival object, then
\texttt{method\ =\ "exp"}, if \texttt{y} has 2 columns then \texttt{method\ =\ "poisson"}, if \texttt{y}
is categorical then \texttt{method\ =\ "class"}, otherwise \texttt{method\ =\ "anova"}).
In the code below, the argument \texttt{method\ =\ "class"} is used given that
the outcome variable is a categorical variable. \emph{It is important to set
the seed before working with rpart if we want to have coherent results,
as it runs some random sampling.}

The \texttt{fit} object is a fully grown tree (\texttt{cp\textless{}0}). We then create \texttt{fit2},
which is a tree manually pruned by setting the parameters mentioned
above. Remember that it is not required to set all the parameters, one
of them could be enough. Finally, \texttt{fit3} is and automatically pruned
tree. The functions \texttt{printcp(fit)} and \texttt{plotcp(fit)}, allow us to
visualize the cross-validated error rate of the fully grown tree
(\texttt{fit}), so that we can select the value for the complexity parameter
that will minimize that value. In this case, I would pick the ``elbow'' of
the graph, thus \texttt{cp=0.094}. In order to apply a new complexity parameter
to the fully grown tree, either we grow the tree again as done for
\texttt{fit}, or we use the function \texttt{prune.rpart()}.

We then plot the tree using \texttt{fancyRpartPlot()} from the package \texttt{rattle}
(\protect\hyperlink{ref-williams2011}{Williams 2011}). The graph produced displays the number of the node on
top of each node, the predicted class (yes or no), the number of
miss-classified observations, the percentage of observations in the
predicted class for this node. As we see here, in this case, pruning
using the automatic method retrieved a poor tree with only one split,
whether the manually pruned tree is richer and allows us to interpret
the result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(rpart.plot)}
\FunctionTok{library}\NormalTok{(rattle)}

\CommentTok{\# set the seed in order to have replicability of the model}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{, }\AttributeTok{kind =} \StringTok{"Mersenne{-}Twister"}\NormalTok{, }\AttributeTok{normal.kind =}  \StringTok{"Inversion"}\NormalTok{)}

\CommentTok{\# fully grown tree}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
        \FunctionTok{as.factor}\NormalTok{(survived) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
        \AttributeTok{data =}\NormalTok{ ptitanic, }
        \AttributeTok{method =} \StringTok{"class"}\NormalTok{,}
        \AttributeTok{cp=}\SpecialCharTok{{-}}\DecValTok{1}
\NormalTok{)}

\CommentTok{\# manually pruned tree}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
        \FunctionTok{as.factor}\NormalTok{(survived) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
        \AttributeTok{data =}\NormalTok{ ptitanic, }
        \AttributeTok{method =} \StringTok{"class"}\NormalTok{,}
        \CommentTok{\# min. n. of obs. that must exist in a node in order for a split }
        \AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }
        \CommentTok{\# the minimum number of observations in any terminal node}
        \AttributeTok{minbucket =} \DecValTok{10}\NormalTok{, }
        \CommentTok{\# max number of splits}
        \AttributeTok{maxdepth =} \DecValTok{5}
\NormalTok{)}

\CommentTok{\# automatically pruned tree}
\CommentTok{\# printing and plotting the cross{-}validated error rate}
\FunctionTok{printcp}\NormalTok{(fit)}
\FunctionTok{plotcp}\NormalTok{(fit)}

\CommentTok{\# pruning the tree accordingly by setting the cp=cpbest}
\NormalTok{fit3 }\OtherTok{\textless{}{-}} \FunctionTok{prune.rpart}\NormalTok{(fit, }\AttributeTok{cp=}\FloatTok{0.094}\NormalTok{)}

\CommentTok{\# plotting the tree}
\FunctionTok{fancyRpartPlot}\NormalTok{(fit2, }\AttributeTok{caption =} \StringTok{"Classification Tree for Titanic pruned manually"}\NormalTok{)}
\FunctionTok{fancyRpartPlot}\NormalTok{(fit3, }\AttributeTok{caption =} \StringTok{"Classification Tree for Titanic pruned automatically"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-83-1} \includegraphics[width=0.5\linewidth,]{_main_files/figure-latex/unnamed-chunk-83-2} \caption{From left: Complexity vs X-val Relative Error; The automatically pruned CART.}\label{fig:unnamed-chunk-83}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,]{_main_files/figure-latex/unnamed-chunk-84-1} 

}

\caption{The manually pruned CART.}\label{fig:unnamed-chunk-84}
\end{figure}

If we want to give an interpretation of the manually pruned tree we can
say the following. The probability of dying on board of the Titanic were
about 83\% if the passenger was a male, older than 9.5 years old, and
this happened for 61\% of the passengers aboard. On the contrary, there
were 93\% of chances of surviving by being a woman with a passenger class
different than the 3rd. This statistic applies to 19\% of the passengers
abroad.

The algorithm for growing a decision tree is an example of recursive
partitioning. The recursive binary splitting approach is top-down and
greedy. Top-down because it begins at the top of the tree (at which
point all observations belong to a single ``region'') and then
successively splits the independent variable' space; each split is
indicated via two new branches further down on the tree. It is greedy
because at each step of the tree-building process, the best split in
terms of minimum RSS is made at that particular step, rather than
looking ahead and picking a split that will lead to a better tree in
some future step. Each node in the tree is grown using the same set of
rules as its parent node.

A much more powerful use of CART (but less interpretable) is when we
have an ensemble of them. An ensemble method is an approach that
combines many simple ``building ensemble block'' models (in this case
trees) in order to obtain a single and potentially very powerful model.
Some examples are Bagging, Random Forest, or Boosting. However, these
methodologies are out of the scope of this book.

~

~

~

\hypertarget{composite-indicators}{%
\section{Composite Indicators}\label{composite-indicators}}

A composite indicator is formed when individual indicators are combined
into a single index, based on an underlying model of the
multidimensional concept that is being measured. The indicators that
make up a composite indicator are referred to as components or component
indicators, and their variability represents the implicit weight the
component has within the final composite indicator Mazziotta and Pareto (\protect\hyperlink{ref-mazziotta2020}{2020}). One of the most common and advanced methods to build a
composite indicator is the use of a scoring system, which is a flexible
and easily interpretable measure Mazziotta and Pareto (\protect\hyperlink{ref-mazziotta2020}{2020}).
It can be expressed as a relative or absolute measure, depending on the
method chosen, and, in both cases, the composite index can be compared
over time. Some examples are the Mazziotta-Pareto Index (relative
measure), the Adjusted Mazziotta-Pareto Index (absolute
measure)(\protect\hyperlink{ref-mazziotta2020}{Mazziotta and Pareto 2020}), the arithmetic mean of the z-scores (relative
measure), or the arithmetic mean of Min-Max (\protect\hyperlink{ref-joint2008handbook}{OECD and JRC 2008}). Scores
can also be clustered or classified to have a ranking of regions.
Furthermore, a scoring system allows to use the components in its
original form, thus keeping the eventual weighting or balancing they
have. Finally, the reference value facilitates the interpretation of the
scores (i.e., a score of 90 given an average of 100 means that the
region is 10 points below the average). The only caveat we can find is
the lack of a unit of measure for the final indicator and thus the
impossibility to have a meaningful single value for the world
aggregation.

~

\hypertarget{mazziotta-pareto-index}{%
\subsection{Mazziotta-Pareto Index}\label{mazziotta-pareto-index}}

The Mazziotta--Pareto index (MPI) is a composite index or summarizing a
set of individual indicators that are assumed to be not fully
substitutable. It is based on a non-linear function which, starting from
the arithmetic mean of the normalized indicators, introduces a penalty
for the units with unbalanced values of the indicators
(\protect\hyperlink{ref-de2011composite}{De Muro, Mazziotta, and Pareto 2011}). The MPI is the best solution for static analysis.

Given the matrix \(Y=y_{ij}\) with \(n\) rows (statistical units) and \(m\)
columns (individual indicators), we calculate the normalized matrix \eqref{eq:normmat}
\(Z=z_{ij}\) as follows:

\begin{equation}
z_{ij}=100\pm\frac{y_{ij}-M_{y_j}}{S_{y_j}}*10
\label{eq:normmat}
\end{equation}

where \(M_{y_j}\) and \(S_{y_j}\) are, respectively, the mean and standard
deviation of the indicator \(j\) and the sign \(\pm\) is the `polarity' of
the indicator \(j\), i.e., the sign of the relation between the indicator
\(j\) and the phenomenon to be measured (\(+\) if the individual indicator
represents a dimension considered positive and \(-\) if it represents a
dimension considered negative).

We then aggregate the normalized data. Denoting with
\(M_{z_i},S_{z_i},cv_{z_i}\), respectively, the mean, standard deviation,
and coefficient of variation of the normalized values for the unit \(i\),
the composite index is given by \eqref{eq:mpi}:
\begin{equation}
MPI^\pm_i= M_{z_i}*(i+cv^2_{z_i})=M_{z_i}\pm S_{z_i}*cv_{z_i}
\label{eq:mpi}
\end{equation}

where the sign \(\pm\) depends on the kind of phenomenon to be
measured. If the composite index is `increasing' or `positive', i.e.,
increasing values of the index correspond to positive variations of the
phenomenon (e.g., socio-economic development), then \(MPI^-\) is used. On
the contrary, if the composite index is `decreasing' or `negative',
i.e., increasing values of the index correspond to negative variations
of the phenomenon (e.g., poverty), then \(MPI^+\) is used. In any cases,
an unbalance among indicators will have a negative effect on the value
of the index.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Compind)}
\CommentTok{\# loading data}
\FunctionTok{data}\NormalTok{(EU\_NUTS1)}

\CommentTok{\# inputting rownames}
\NormalTok{EU\_NUTS1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(EU\_NUTS1)}
\FunctionTok{rownames}\NormalTok{(EU\_NUTS1) }\OtherTok{\textless{}{-}}\NormalTok{ EU\_NUTS1}\SpecialCharTok{$}\NormalTok{NUTS1}

\CommentTok{\# Unsustainable Transport Index (NEG)}

\CommentTok{\# Normalization (roads are negative, trains are positive)}
\NormalTok{data\_norm }\OtherTok{\textless{}{-}} \FunctionTok{normalise\_ci}\NormalTok{(EU\_NUTS1,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{),}
                          \CommentTok{\# roads are negative, railrads positive}
                          \AttributeTok{polarity =} \FunctionTok{c}\NormalTok{(}\StringTok{"NEG"}\NormalTok{,}\StringTok{"POS"}\NormalTok{),}
                          \CommentTok{\# z{-}score method}
                          \AttributeTok{method =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Aggregation using MPI (the index is negative)}
\NormalTok{CI }\OtherTok{\textless{}{-}} \FunctionTok{ci\_mpi}\NormalTok{(data\_norm}\SpecialCharTok{$}\NormalTok{ci\_norm, }
             \AttributeTok{penalty=}\StringTok{"NEG"}\NormalTok{)}

\CommentTok{\# Table containing Top 5 Unsustainable Transport Index}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{AMPI=}\FunctionTok{head}\NormalTok{(}\FunctionTok{sort}\NormalTok{(CI}\SpecialCharTok{$}\NormalTok{ci\_mpi\_est, }\AttributeTok{decreasing =}\NormalTok{ T),}\DecValTok{5}\NormalTok{)), }
       \AttributeTok{caption =} \StringTok{"Top 5 unsustainable Index"}\NormalTok{)}

\CommentTok{\# Table containing Bottom 5 Unsustainable Transport Index (aka most sustainable)}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{AMPI=}\FunctionTok{sort}\NormalTok{(}\FunctionTok{tail}\NormalTok{(}\FunctionTok{sort}\NormalTok{(CI}\SpecialCharTok{$}\NormalTok{ci\_mpi\_est, }\AttributeTok{decreasing =}\NormalTok{ T),}\DecValTok{5}\NormalTok{))), }
       \AttributeTok{caption =} \StringTok{"Bottom 5 unsustainable Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the example above, we usa a dataset present in the same Compind
package (\protect\hyperlink{ref-compind}{Fusco, Vidoli, and Sahoo 2018}). We first input the rownames (this is needed to have
the names of the countries in our final table), we then state the index
and its polarity. We normalize the data using the function
\texttt{normalise\_ci} selecting the variables of interest, their polarity with
respect to the phenomeon of interest, and the method to use (see the
help for the available methods). We finally compute our MPI using the
function \texttt{ci\_mpi} by specifying its penality.

~

\hypertarget{adjusted-mazziotta-pareto-index}{%
\subsection{Adjusted Mazziotta-Pareto Index}\label{adjusted-mazziotta-pareto-index}}

In this study we consider as a composite indicator the Adjusted
Mazziotta Pareto Index (AMPI) methodology. AMPI is a non-compensatory
(or partially compensatory) composite index that allows the
comparability of data between units and over time
(\protect\hyperlink{ref-mazziotta2016generalized}{Mazziotta and Pareto 2016}). It is a variant of the MPI, based on a
rescaling of individual indicators using a Min-Max transformation
(\protect\hyperlink{ref-de2011composite}{De Muro, Mazziotta, and Pareto 2011}).

To apply AMPI, the original indicators are normalized using a Min-Max
methodology with goalposts. As compared to the most common MPI, the
Min-Max normalization technique enables us to compare data over time,
whereas the z-score normalization used in MPI does not. Given the matrix
\(X=\{x_{ij}\}\) with \(n\) rows (units) and \(m\) columns (indicators), we
calculate the normalized matrix \(R=\{r_{ij}\}\) as follows \eqref{eq:normmat2}:
\begin{equation}
r_{ij}=\frac{x_{ij} -\min x_j}{\max x_j - \min x_j} *60+70
\label{eq:normmat2}
\end{equation}

where \(x_{ij}\) is the value of the indicator \(j\) for the unit \(i\) and
\(\min x_j\) and \(\max x_j\) are the `goalposts' for the indicator \(j\). If
the indicator \(j\) has negative polarity, the complement of \((1)\) is
calculated with respect to \(200\). To facilitate the interpretation of
results, the `goalposts' can be fixed so that 100 represents a reference
value (e.g., the average in a given year). A simple procedure for
setting the `goalposts' is the following. Let \(\inf x_j\) and \(\sup x_j\)
be the overall minimum and maximum of the indicator \(j\) across all units
and all time periods considered. Denoting with \(\text{ref } x_j\) the
reference value for the indicator \(j\), the `goalposts' are defined as \eqref{eq:goals}:

\begin{equation*}
    \begin{cases}
      \min x_j= \text{ref } x_j - (\sup x_j - \inf x_j)/2\\
      \max x_j= \text{ref } x_j + (\sup x_j - \inf x_j)/2
    \end{cases}\
\label{eq:goals}
\end{equation*}\textbackslash end\{equation*\}

The normalized values will fall approximately in the range (70; 130),
where 100 represents the reference value.

The normalized indicators can then be aggregated. Denoting with
\(M_{r_i}\) and \(S_{r_i}\), respectively, the mean and standard deviation
of the normalized values of the unit \(i\), the generalized form of AMPI
is given by \eqref{eq:ampi}:

\begin{equation}
AMPI_i^{+/-}=M_{r_i} \pm S_{r_i}*cv_i
\label{eq:ampi}
\end{equation}

where \(cv_i=\frac{S_{r_i}}{M_{r_i}}\) is the coefficient of variation of the
unit \(i\) and the sign \(\pm\) depends on the kind of phenomenon to be
measured. If the composite index is increasing or positive, that is,
increasing the index values corresponds to positive variations in the
phenomenon (for example, well-being), then \(AMPI^-\) is used. Vice versa,
if the composite index is decreasing or negative, that is, increasing
index values correspond to negative variations of the phenomenon (e.g.
waste), then \(AMPI^+\) is used. This approach is characterized by the use
of a function (the product \(S_{r_i}*cv_i\)) to penalize units with
unbalanced values of the normalized indicators. The `penalty' is based
on the coefficient of variation and is zero if all values are equal. The
purpose is to favor the units that, mean being equal, have a greater
balance among the different indicators. Therefore, AMPI is characterized
by the combination of a `mean effect' (\(M_{r_i}\)) and a `penalty effect'
(\(S_{r_i}*cv_i\)) where each unit stands in relation to the `goalposts'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading data}
\FunctionTok{data}\NormalTok{(EU\_2020)}

\CommentTok{\# Sustainable Employment Index (POS)}

\CommentTok{\# subsetting interesting variables}
\NormalTok{data\_test }\OtherTok{\textless{}{-}}\NormalTok{ EU\_2020[,}\FunctionTok{c}\NormalTok{(}\StringTok{"geo"}\NormalTok{,}\StringTok{"employ\_2010"}\NormalTok{,}\StringTok{"employ\_2011"}\NormalTok{,}\StringTok{"finalenergy\_2010"}\NormalTok{,}
                        \StringTok{"finalenergy\_2011"}\NormalTok{)] }

\CommentTok{\# reshaping to long format}
\NormalTok{EU\_2020\_long }\OtherTok{\textless{}{-}} \FunctionTok{reshape}\NormalTok{(data\_test, }
                       \CommentTok{\#our variables}
                      \AttributeTok{varying=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{), }
                      \AttributeTok{direction=}\StringTok{"long"}\NormalTok{, }
                      \CommentTok{\#geographic variable}
                      \AttributeTok{idvar=}\StringTok{"geo"}\NormalTok{, }
                      \AttributeTok{sep=}\StringTok{"\_"}\NormalTok{)}

\CommentTok{\# normalization and aggregation using AMPI}
\NormalTok{CI }\OtherTok{\textless{}{-}} \FunctionTok{ci\_ampi}\NormalTok{(EU\_2020\_long, }
              \CommentTok{\#our variables}
              \AttributeTok{indic\_col=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{),}
              \CommentTok{\#goalposts}
              \AttributeTok{gp=}\FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{), }
              \CommentTok{\#time variable}
              \AttributeTok{time=}\NormalTok{EU\_2020\_long}\SpecialCharTok{$}\NormalTok{time,}
              \CommentTok{\#both variables are positive}
              \AttributeTok{polarity=} \FunctionTok{c}\NormalTok{(}\StringTok{"POS"}\NormalTok{, }\StringTok{"POS"}\NormalTok{),}
              \CommentTok{\#index is positive}
              \AttributeTok{penalty=}\StringTok{"POS"}\NormalTok{)}

\CommentTok{\# Table containing the Sustainable Employment Index scores}
\FunctionTok{pander}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{t}\NormalTok{(CI}\SpecialCharTok{$}\NormalTok{ci\_ampi\_est)), }\AttributeTok{caption =} \StringTok{"AMPI time series"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the example above, we usa a dataset present in the same Compind
package (\protect\hyperlink{ref-compind}{Fusco, Vidoli, and Sahoo 2018}). We reshape data to long format, allowing us to have
a dataset with one variable per column. We then use the function
\texttt{ci\_ampi} which standardize the data and compite the score at the same
time. In this function we set the dataset, the variables of interest,
the values of the corresponding variables to be used as a reference
(goalposts), the time variable, the polarity of the variables and the
penality corresponding to the final indicator.

~

~

~

\hypertarget{exercises-5}{%
\section{Exercises}\label{exercises-5}}

\begin{itemize}
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground},
  section 6 - PCA and Clustering
\item
  \href{https://federicoroscioli.shinyapps.io/exercises/}{R playground},
  section 8 - Classification And Regression Trees
\end{itemize}

\newpage

\hypertarget{final-remarks}{%
\chapter*{Final Remarks}\label{final-remarks}}
\addcontentsline{toc}{chapter}{Final Remarks}

At this point I expect that you are familiar with the \emph{Rish} language
and its dynamics. This manual covered some basic data analysis methods,
but there is much more to do and to learn. Now it is your time to
explore new packages, new ways of writing code, and new statistical
techniques.

Be curious and have fun!

\emph{Federico Roscioli}

\newpage

\hypertarget{bibliography}{%
\chapter*{Bibliography}\label{bibliography}}
\addcontentsline{toc}{chapter}{Bibliography}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-abdi2010}{}}%
Abdi, HervÃ©, and Lynne J. Williams. 2010. {``Principal Component Analysis: Principal Component Analysis.''} \emph{Wiley Interdisciplinary Reviews: Computational Statistics} 2 (4): 433--59. \url{https://doi.org/10.1002/wics.101}.

\leavevmode\vadjust pre{\hypertarget{ref-anderson2022}{}}%
Anderson, Sean C. 2022. {``An Introduction to Reshape2.''} \url{https://seananderson.ca/2013/10/19/reshape/}.

\leavevmode\vadjust pre{\hypertarget{ref-bratsas2020}{}}%
Bratsas, Charalampos, Anastasia Foudouli, and Kleanthis Koupidis. 2020. {``Package {`}Gginference{'}.''} \url{https://cran.r-project.org/web/packages/gginference/gginference.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-chang2018}{}}%
Chang, Winston. 2018. \emph{R Graphics Cookbook: Practical Recipes for Visualizing Data}. Second edition. Beijing ; Boston: O'Reilly. \url{https://r-graphics.org}.

\leavevmode\vadjust pre{\hypertarget{ref-cobham2016}{}}%
Cobham, Alex, Lukas SchlÃ¶gl, and Andy Sumner. 2016. {``Inequality and the Tails: The Palma Proposition and Ratio.''} \emph{Global Policy} 7 (1): 25--36. \url{https://doi.org/10.1111/1758-5899.12320}.

\leavevmode\vadjust pre{\hypertarget{ref-damodaran}{}}%
Damodaran, Aswath. n.d. {``Statistical Distributions.''} \url{http://people.stern.nyu.edu/adamodar/New_Home_Page/StatFile/statdistns.htm}.

\leavevmode\vadjust pre{\hypertarget{ref-davis2021}{}}%
Davis, Brittany. 2021. {``When Correlation Is Better Than Causation.''} \url{https://towardsdatascience.com/when-correlation-is-better-than-causation-1cbfa2708fbb}.

\leavevmode\vadjust pre{\hypertarget{ref-de2011composite}{}}%
De Muro, Pasquale, Matteo Mazziotta, and Adriano Pareto. 2011. {``Composite Indices of Development and Poverty: An Application to MDGs.''} \emph{Social Indicators Research} 104: 1--18.

\leavevmode\vadjust pre{\hypertarget{ref-dragulescu2020}{}}%
Dragulescu, Adrian, and Cole Arendt. 2020. {``Package {`Xlsx'}.''} \url{https://cran.r-project.org/web/packages/xlsx/xlsx.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-emerson2012}{}}%
Emerson, John W., Walton A. Green, Barret Schloerke, Jason Crowley, Dianne Cook, Heike Hofmann, and Hadley Wickham. 2012. {``The Generalized Pairs Plot.''} \emph{Journal of Computational and Graphical Statistics} 22 (1): 79--91. \url{https://doi.org/10.1080/10618600.2012.694762}.

\leavevmode\vadjust pre{\hypertarget{ref-frost}{}}%
Frost, Jim. n.d. {``Normal Distribution in Statistics.''} \url{https://statisticsbyjim.com/basics/normal-distribution/}.

\leavevmode\vadjust pre{\hypertarget{ref-compind}{}}%
Fusco, Elisa, Francesco Vidoli, and Biresh K. Sahoo. 2018. {``Spatial Heterogeneity in Composite Indicator: A Methodological Proposal.''} \emph{Omega} 77: 1--14.

\leavevmode\vadjust pre{\hypertarget{ref-hlavac2022}{}}%
Hlavac, Marek. 2022. {``Stargazer: Beautiful LATEX, HTML and ASCII Tables from r Statistical Output.''} \emph{The Comprehensive R Archive Network}. \url{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-iacus2020}{}}%
Iacus, Stefano M., and Guido Masarotto. 2020. {``Package {`labstatR'}.''} \emph{The Comprehensive R Archive Network}. \url{https://cran.r-project.org/web/packages/labstatR/labstatR.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-Imbens2015}{}}%
Imbens, Guido W, and Donald B Rubin. 2015. \emph{Causal Inference in Statistics, Social, and Biomedical Sciences}. Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-james2021}{}}%
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. \emph{An Introduction to Statistical Learning: With Applications in r}. Second edition. Springer Texts in Statistics. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-jolliffe2016}{}}%
Jolliffe, Ian T., and Jorge Cadima. 2016. {``Principal Component Analysis: A Review and Recent Developments.''} \emph{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences} 374 (2065): 20150202. \url{https://doi.org/10.1098/rsta.2015.0202}.

\leavevmode\vadjust pre{\hypertarget{ref-kaplan2022}{}}%
Kaplan, Jacob. 2022. {``fastDummies.''} \url{https://jacobkap.github.io/fastDummies/}.

\leavevmode\vadjust pre{\hypertarget{ref-kassambara}{}}%
Kassambara, Alboukadel, and Fabian Mundt. n.d. {``Factoextra : Extract and Visualize the Results of Multivariate Data Analyses.''} \url{https://rpkgs.datanovia.com/factoextra/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-koehrsen2019}{}}%
Koehrsen, Will. 2019. {``Data Scientists: Your Variable Names Are Awful. Here's How to Fix Them.''} \url{https://towardsdatascience.com/data-scientists-your-variable-names-are-awful-heres-how-to-fix-them-89053d2855be}.

\leavevmode\vadjust pre{\hypertarget{ref-laerdstatistics}{}}%
Laerd Statistics. n.d. {``Measures of Central Tendency.''} \url{https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php}.

\leavevmode\vadjust pre{\hypertarget{ref-lane2003}{}}%
Lane, David M., David Scott, Mikki Hebl, Rudy Guerra, Dan Osherson, and Heidi Zimmer. 2003. \emph{Introduction to Statistics}. \url{https://onlinestatbook.com/Online_Statistics_Education.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-lee2019}{}}%
Lee. 2019. {``P-Values Explained by Data Scientist.''} \url{https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8}.

\leavevmode\vadjust pre{\hypertarget{ref-lee2018}{}}%
Lee, James. 2018. {``R Decision Trees Tutorial.''} \emph{Datacamp}. \url{https://www.datacamp.com/tutorial/decision-trees-R}.

\leavevmode\vadjust pre{\hypertarget{ref-madhavan2019}{}}%
Madhavan, Archana. 2019. {``Correlation Vs Causation: Understand the Difference for Your Product.''} \url{https://amplitude.com/blog/causation-correlation}.

\leavevmode\vadjust pre{\hypertarget{ref-mahjoobi2008}{}}%
Mahjoobi, J., and A. Etemad-Shahidi. 2008. {``An Alternative Approach for the Prediction of Significant Wave Heights Based on Classification and Regression Trees.''} \emph{Applied Ocean Research} 30 (3): 172--77. \url{https://doi.org/10.1016/j.apor.2008.11.001}.

\leavevmode\vadjust pre{\hypertarget{ref-manikandan2011}{}}%
Manikandan, S. 2011. {``Measures of Central Tendency: Median and Mode.''} \emph{Journal of Pharmacology and Pharmacotherapeutics} 2 (3): 214. \url{https://doi.org/10.4103/0976-500X.83300}.

\leavevmode\vadjust pre{\hypertarget{ref-mazziotta2016generalized}{}}%
Mazziotta, Matteo, and Adriano Pareto. 2016. {``On a Generalized Non-Compensatory Composite Index for Measuring Socio-Economic Phenomena.''} \emph{Social Indicators Research} 127: 983--1003.

\leavevmode\vadjust pre{\hypertarget{ref-mazziotta2020}{}}%
---------. 2020. \emph{Gli Indici Sintetici}. 1st ed. Giappichelli.

\leavevmode\vadjust pre{\hypertarget{ref-joint2008handbook}{}}%
OECD, and JRC. 2008. \emph{Handbook on Constructing Composite Indicators: Methodology and User Guide}. OECD publishing.

\leavevmode\vadjust pre{\hypertarget{ref-Otoiu2021}{}}%
Otoiu, Adrian, Adriano Pareto, Elena Grimaccia, Matteo Mazziotta, and Silvia Terzi. 2021. \emph{Open Issues in Composite Indicators. A Starting Point and a Reference on Some State-of-the-Art Issues}. Roma TrE-Press. \url{https://doi.org/10.13134/979-12-5977-001-1}.

\leavevmode\vadjust pre{\hypertarget{ref-rcoreteam2022}{}}%
R Core Team. 2022. {``R Language Definition.''} \url{https://cran.r-project.org/doc/manuals/r-release/R-lang.html\#Operators}.

\leavevmode\vadjust pre{\hypertarget{ref-signorelli2021}{}}%
Signorelli, Andri. 2021. {``DescTools: Tools for Descriptive Statistics.''} \url{https://andrisignorell.github.io/DescTools/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-sovanskywinter2022}{}}%
Sovansky Winter, Erin. 2022. {``Chapter 4: Apply Functions.''} In. \url{https://ademos.people.uic.edu/Chapter4.html}.

\leavevmode\vadjust pre{\hypertarget{ref-syakur2018}{}}%
Syakur, M A, B K Khotimah, E M S Rochman, and B D Satoto. 2018. {``Integration k-Means Clustering Method and Elbow Method for Identification of the Best Customer Profile Cluster.''} \emph{IOP Conference Series: Materials Science and Engineering} 336 (April): 012017. \url{https://doi.org/10.1088/1757-899X/336/1/012017}.

\leavevmode\vadjust pre{\hypertarget{ref-temkin1995}{}}%
Temkin, Nancy R., Richard Holubkov, Joan E. Machamer, H. Richard Winn, and Sureyya S. Dikmen. 1995. {``Classification and Regression Trees (CART) for Prediction of Function at 1 Year Following Head Trauma.''} \emph{Journal of Neurosurgery} 82 (5): 764--71. \url{https://doi.org/10.3171/jns.1995.82.5.0764}.

\leavevmode\vadjust pre{\hypertarget{ref-therneau2022}{}}%
Therneau, Terry M., and Elizabeth J. Atkinson. 2022. {``An Introduction to Recursive Partitioning Using the RPART Routines.''} \url{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-torres-reyna2014}{}}%
Torres-Reyna, Oscar. 2014. {``Using Stargazer to Report Regression Output and Descriptive Statistics in r.''} \url{https://www.princeton.edu/~otorres/NiceOutputR.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-wei2021}{}}%
Wei, Taiyun, and Viliam Simko. 2021. {``An Introduction to Corrplot Package.''} \emph{The Comprehensive R Archive Network}. \url{https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2014}{}}%
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical Software} 59 (10). \url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2016}{}}%
---------. 2016. \emph{Ggplot2: Elegant Graphics for Data Analysis}. Springer-Verlag New York. \url{https://ggplot2.tidyverse.org/reference/}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2022}{}}%
Wickham, Hadley, and Jennifer Bryan. 2022. {``Readxl: Read Excel Files.''} \url{https://readxl.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-williams2011}{}}%
Williams, Graham J. 2011. \emph{Data Mining with Rattle and r: The Art of Excavating Data for Knowledge Discovery}. Use r! Springer.

\leavevmode\vadjust pre{\hypertarget{ref-wilson2014}{}}%
Wilson, Mark. 2014. {``Hilarious Graphs Prove That Correlation Isn{'}t Causation.''} \url{https://www.fastcompany.com/3030529/hilarious-graphs-prove-that-correlation-isnt-causation}.

\leavevmode\vadjust pre{\hypertarget{ref-dewinter}{}}%
Winter, J. C. F. de. 2013. {``Using the Student's t-Test with Extremely Small Sample Sizes.''} \url{https://doi.org/10.7275/E4R6-DJ05}.

\leavevmode\vadjust pre{\hypertarget{ref-yarberry2021}{}}%
Yarberry, William. 2021. {``Lubridate: Date and Time Processing.''} In \emph{{CRAN} Recipes}, 109--60. Apress. \url{https://doi.org/10.1007/978-1-4842-6876-6_3}.

\leavevmode\vadjust pre{\hypertarget{ref-ziliak2008}{}}%
Ziliak, Stephen T. 2008. {``Retrospectives: Guinnessometrics: The Economic Foundation of {``}Student's{''} {\emph{t}}.''} \emph{Journal of Economic Perspectives} 22 (4): 199--216. \url{https://doi.org/10.1257/jep.22.4.199}.

\end{CSLReferences}

\end{document}
