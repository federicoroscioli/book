<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="8.4 Classification And Regression Trees | Introduction to Data Analysis with R" />
<meta property="og:type" content="book" />




<meta name="author" content="Federico Roscioli" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="8.4 Classification And Regression Trees | Introduction to Data Analysis with R">

<title>8.4 Classification And Regression Trees | Introduction to Data Analysis with R</title>

<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="book_assets/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="book_assets/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="book_assets/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="book_assets/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="book_assets/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface" id="toc-preface">Preface</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction">Introduction</a></li>
<li class="part"><span><b>I The R code</b></span></li>
<li class="has-sub"><a href="1-installation.html#installation" id="toc-installation"><span class="toc-section-number">1</span> Installation</a>
<ul>
<li><a href="1.1-introductory-activities.html#introductory-activities" id="toc-introductory-activities"><span class="toc-section-number">1.1</span> Introductory activities</a></li>
<li><a href="1.2-visualization-suggestions.html#visualization-suggestions" id="toc-visualization-suggestions"><span class="toc-section-number">1.2</span> Visualization suggestions</a></li>
<li><a href="1.3-the-workspace.html#the-workspace" id="toc-the-workspace"><span class="toc-section-number">1.3</span> The workspace</a></li>
</ul></li>
<li class="has-sub"><a href="2-a-b-c.html#a-b-c" id="toc-a-b-c"><span class="toc-section-number">2</span> A, B, C</a>
<ul>
<li><a href="2.1-the-first-code.html#the-first-code" id="toc-the-first-code"><span class="toc-section-number">2.1</span> The first code</a></li>
<li><a href="2.2-indexing.html#indexing" id="toc-indexing"><span class="toc-section-number">2.2</span> Indexing</a></li>
<li><a href="2.3-the-first-function.html#the-first-function" id="toc-the-first-function"><span class="toc-section-number">2.3</span> The first function</a></li>
<li><a href="2.4-dataset-exploration.html#dataset-exploration" id="toc-dataset-exploration"><span class="toc-section-number">2.4</span> Dataset Exploration</a></li>
<li><a href="2.5-subsetting.html#subsetting" id="toc-subsetting"><span class="toc-section-number">2.5</span> Subsetting</a></li>
<li><a href="2.6-importing-and-exporting-data.html#importing-and-exporting-data" id="toc-importing-and-exporting-data"><span class="toc-section-number">2.6</span> Importing and exporting data</a></li>
<li><a href="2.7-exercises.html#exercises" id="toc-exercises"><span class="toc-section-number">2.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="3-data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">3</span> Data Cleaning</a>
<ul>
<li><a href="3.1-variable-names.html#variable-names" id="toc-variable-names"><span class="toc-section-number">3.1</span> Variable Names</a></li>
<li class="has-sub"><a href="3.2-variable-types.html#variable-types" id="toc-variable-types"><span class="toc-section-number">3.2</span> Variable Types</a>
<ul>
<li><a href="3.2-variable-types.html#factor-variables" id="toc-factor-variables"><span class="toc-section-number">3.2.1</span> Factor variables</a></li>
<li><a href="3.2-variable-types.html#dates-and-times" id="toc-dates-and-times"><span class="toc-section-number">3.2.2</span> Dates and times</a></li>
</ul></li>
<li><a href="3.3-row-names.html#row-names" id="toc-row-names"><span class="toc-section-number">3.3</span> Row Names</a></li>
</ul></li>
<li class="has-sub"><a href="4-advanced-data-manipulation-and-plotting.html#advanced-data-manipulation-and-plotting" id="toc-advanced-data-manipulation-and-plotting"><span class="toc-section-number">4</span> Advanced Data Manipulation and Plotting</a>
<ul>
<li><a href="4.1-ifelse.html#ifelse" id="toc-ifelse"><span class="toc-section-number">4.1</span> Ifelse</a></li>
<li><a href="4.2-the-apply-family.html#the-apply-family" id="toc-the-apply-family"><span class="toc-section-number">4.2</span> The Apply family</a></li>
<li><a href="4.3-dplyr.html#dplyr" id="toc-dplyr"><span class="toc-section-number">4.3</span> Dplyr</a></li>
<li><a href="4.4-merging-datasets.html#merging-datasets" id="toc-merging-datasets"><span class="toc-section-number">4.4</span> Merging datasets</a></li>
<li><a href="4.5-melting-vs-transposing.html#melting-vs-transposing" id="toc-melting-vs-transposing"><span class="toc-section-number">4.5</span> Melting vs Transposing</a></li>
<li><a href="4.6-ggplot2.html#ggplot2" id="toc-ggplot2"><span class="toc-section-number">4.6</span> Ggplot2</a></li>
<li><a href="4.7-exercises-1.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">4.7</span> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Statistical Analysis</b></span></li>
<li class="has-sub"><a href="5-exploratory-data-analysis.html#exploratory-data-analysis" id="toc-exploratory-data-analysis"><span class="toc-section-number">5</span> Exploratory Data Analysis</a>
<ul>
<li><a href="5.1-central-tendency-measures.html#central-tendency-measures" id="toc-central-tendency-measures"><span class="toc-section-number">5.1</span> Central Tendency Measures</a></li>
<li><a href="5.2-variability-measures.html#variability-measures" id="toc-variability-measures"><span class="toc-section-number">5.2</span> Variability Measures</a></li>
<li><a href="5.3-inequality-measures.html#inequality-measures" id="toc-inequality-measures"><span class="toc-section-number">5.3</span> Inequality Measures</a></li>
<li><a href="5.4-data-visualization.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">5.4</span> Data visualization</a></li>
<li><a href="5.5-scaling-data.html#scaling-data" id="toc-scaling-data"><span class="toc-section-number">5.5</span> Scaling data</a></li>
<li><a href="5.6-probability-sampling.html#probability-sampling" id="toc-probability-sampling"><span class="toc-section-number">5.6</span> Probability Sampling</a></li>
<li><a href="5.7-exercises-2.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="6-hypothesis-testing.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">6</span> Hypothesis Testing</a>
<ul>
<li><a href="6.1-probability-distributions.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">6.1</span> Probability Distributions</a></li>
<li><a href="6.2-shapiro-wilk-test.html#shapiro-wilk-test" id="toc-shapiro-wilk-test"><span class="toc-section-number">6.2</span> Shapiro-Wilk Test</a></li>
<li><a href="6.3-one-sample-t-test.html#one-sample-t-test" id="toc-one-sample-t-test"><span class="toc-section-number">6.3</span> One-Sample T-Test</a></li>
<li><a href="6.4-unpaired-two-sample-t-test.html#unpaired-two-sample-t-test" id="toc-unpaired-two-sample-t-test"><span class="toc-section-number">6.4</span> Unpaired Two Sample T-Test</a></li>
<li><a href="6.5-mann-whitney-u-test.html#mann-whitney-u-test" id="toc-mann-whitney-u-test"><span class="toc-section-number">6.5</span> Mann Whitney U Test</a></li>
<li><a href="6.6-paired-sample-t-test.html#paired-sample-t-test" id="toc-paired-sample-t-test"><span class="toc-section-number">6.6</span> Paired Sample T-Test</a></li>
<li><a href="6.7-exercises-3.html#exercises-3" id="toc-exercises-3"><span class="toc-section-number">6.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="7-bivariate-analysis.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7</span> Bivariate Analysis</a>
<ul>
<li><a href="7.1-correlation.html#correlation" id="toc-correlation"><span class="toc-section-number">7.1</span> Correlation</a></li>
<li><a href="7.2-linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">7.2</span> Linear Regression</a></li>
<li><a href="7.3-logistic-regressions.html#logistic-regressions" id="toc-logistic-regressions"><span class="toc-section-number">7.3</span> Logistic Regressions</a></li>
<li><a href="7.4-exercises-4.html#exercises-4" id="toc-exercises-4"><span class="toc-section-number">7.4</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="8-multivariate-analysis.html#multivariate-analysis" id="toc-multivariate-analysis"><span class="toc-section-number">8</span> Multivariate Analysis</a>
<ul>
<li class="has-sub"><a href="8.1-cluster-analysis.html#cluster-analysis" id="toc-cluster-analysis"><span class="toc-section-number">8.1</span> Cluster Analysis</a>
<ul>
<li><a href="8.1-cluster-analysis.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">8.1.1</span> Hierarchical Clustering</a></li>
<li><a href="8.1-cluster-analysis.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">8.1.2</span> K-Means clustering</a></li>
<li><a href="8.1-cluster-analysis.html#the-silhouette-plot" id="toc-the-silhouette-plot"><span class="toc-section-number">8.1.3</span> The silhouette plot</a></li>
</ul></li>
<li><a href="8.2-heatmap.html#heatmap" id="toc-heatmap"><span class="toc-section-number">8.2</span> Heatmap</a></li>
<li><a href="8.3-principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.3</span> Principal Component Analysis</a></li>
<li><a href="8.4-classification-and-regression-trees.html#classification-and-regression-trees" id="toc-classification-and-regression-trees"><span class="toc-section-number">8.4</span> Classification And Regression Trees</a></li>
<li class="has-sub"><a href="8.5-composite-indicators.html#composite-indicators" id="toc-composite-indicators"><span class="toc-section-number">8.5</span> Composite Indicators</a>
<ul>
<li><a href="8.5-composite-indicators.html#mazziotta-pareto-index" id="toc-mazziotta-pareto-index"><span class="toc-section-number">8.5.1</span> Mazziotta-Pareto Index</a></li>
<li><a href="8.5-composite-indicators.html#adjusted-mazziotta-pareto-index" id="toc-adjusted-mazziotta-pareto-index"><span class="toc-section-number">8.5.2</span> Adjusted Mazziotta-Pareto Index</a></li>
</ul></li>
<li><a href="8.6-exercises-5.html#exercises-5" id="toc-exercises-5"><span class="toc-section-number">8.6</span> Exercises</a></li>
</ul></li>
<li><a href="final-remarks.html#final-remarks" id="toc-final-remarks">Final Remarks</a></li>
<li><a href="bibliography.html#bibliography" id="toc-bibliography">Bibliography</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="classification-and-regression-trees" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Classification And Regression Trees</h2>
<p>Classification and Regression Trees (CART) are simple and useful methods
for interpretation that allow to understand the underlying relationship
between one dependent variable and multiple independent variables
<span class="citation">Temkin et al. (<a href="#ref-temkin1995">1995</a>)</span>. As compared to multiple linear regression
analysis, this set of methods does not retrieve the impact of one
variable on the outcome controlling for a set of other independent
variables. It instead recursively looks at the most significant
relationship between a set of variables, subsets the given data
accordingly, and finally draws a tree. CART are a great tool for
communicating complex relationships thanks to their visual output,
however they have generally a poor predicting performance.</p>
<p>Depending on the dependent variable type it is possible to apply a
Classification (for discrete variables) or Regression (for continuous
variables) Tree. In order to build a <strong>regression tree</strong>, the algorithm
first uses recursive binary splitting to grow a large tree, stopping
only when each terminal node has fewer than some minimum number of
observations. Beginning at the top of the tree, it splits the data into
2 branches, creating a partition of 2 spaces. It then carries out this
particular split at the top of the tree multiple times and chooses the
split of the features that minimizes the Residual Sum of Squares (RSS).
Then it repeats the procedure for each subsequent split. A
<strong>classification tree</strong>, instead, is built by predicting which
observation belongs to the most commonly occurring class in the region
to which it belongs. However, in the classification setting, RSS cannot
be used as a criterion for making the binary splits. The algorithm then
uses Classification Error Rate, Gini Index or Cross-Entropy.</p>
<p>In interpreting the results of a classification tree, you are often
interested not only in the class prediction corresponding to a
particular terminal node region, but also in the class proportions among
the training observations that fall into that region. The image below
offers a clear understanding of how a classification tree must be read
<span class="citation">(<a href="#ref-lee2018">J. Lee 2018</a>)</span>. We first state our research question. The answer proposed
depend on the variables included in our data. In this case we will
accept the new job offer only if the salary is higher than $50k, it
takes less than one hour to commute, and the company offers free coffee.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-87"></span>
<img src="images/classification-tree_ygvats%20copia.jpg" alt="Classification tree explanation. Source Lee (2018)." width="80%"  />
<p class="caption">
Figure 8.8: Classification tree explanation. Source Lee (2018).
</p>
</div>
<p>The main question is when to stop splitting? Clearly, if all of the
elements in a node are of the same class it does not do us much good to
add another split. Doing so would usually decrease the power of our
model. This is known as <u>overfitting</u>. As omniscient
statisticians, we have to be creative with the rules for termination. In
fact, there is no one-size-fits-all-rule in this case, but the algorithm
provides a number of parameters that we can set. This process is called
“pruning”, as if we were pruning a tree to make it smaller and simpler.</p>
<p><strong>Manual pruning</strong>, is performed starting from fully grown (over-fitted)
trees and setting parameters such as the minimum number of observations
that must exist in a node in order for a split to be attempted
(<code>minsplit</code>), the minimum number of observations in any terminal node
(<code>minbucket</code>), and the maximum depth of any node of the final tree, with
the root node counted as depth 0 (<code>maxdepth</code>), just to mention the most
important ones. There is no rule for setting these parameters, and here
comes the art of the statistician.</p>
<p><strong>Automatic pruning</strong>, instead, is done by setting the complexity
parameter (<code>cp</code>). The complexity parameter is a combination of the size
of a tree and the ability of the tree to separate the classes of the
target variable. If the next best split in growing a tree does not
reduce the tree’s overall complexity by a certain amount, then the
process is terminated. The complexity parameter by default is set to
0.01. Setting it to a negative amount ensures that the tree will be
fully grown. But which is the right value for the complexity parameter?
Also in this case, there is not a perfect rule. The rule of thumb is to
set it to zero, and then select the complexity parameter that minimizes
the level of the cross-validated error rate.</p>
<p>In our example below, we will use the dataset <code>ptitianic</code>, from the
package <code>rpart.plot</code>. The dataset provides a list of passengers on board
fo the famous ship Titanic which sank in the North Atlantic Ocean on 15
April 1912. It tells us whether the passenger died or survived, the
passenger class, gender, age, the number of sibling or spouses aboard,
and the number of parents or children aboard. Our aim is to understand
which were the factors allowing the passenger to survive.</p>
<p>The package <code>rpart</code> allows us to run the CART algorithms
<span class="citation">(<a href="#ref-therneau2022">Therneau and Atkinson 2022</a>)</span>. The <code>rpart()</code> function needs the specification of the
formula using the same syntax used for multiple linear regressions, the
source of data, and the method (if <code>y</code> is a survival object, then
<code>method = "exp"</code>, if <code>y</code> has 2 columns then <code>method = "poisson"</code>, if <code>y</code>
is categorical then <code>method = "class"</code>, otherwise <code>method = "anova"</code>).
In the code below, the argument <code>method = "class"</code> is used given that
the outcome variable is a categorical variable. <em>It is important to set
the seed before working with rpart if we want to have coherent results,
as it runs some random sampling.</em></p>
<p>The <code>fit</code> object is a fully grown tree (<code>cp&lt;0</code>). We then create <code>fit2</code>,
which is a tree manually pruned by setting the parameters mentioned
above. Remember that it is not required to set all the parameters, one
of them could be enough. Finally, <code>fit3</code> is and automatically pruned
tree. The functions <code>printcp(fit)</code> and <code>plotcp(fit)</code>, allow us to
visualize the cross-validated error rate of the fully grown tree
(<code>fit</code>), so that we can select the value for the complexity parameter
that will minimize that value. In this case, I would pick the “elbow” of
the graph, thus <code>cp=0.094</code>. In order to apply a new complexity parameter
to the fully grown tree, either we grow the tree again as done for
<code>fit</code>, or we use the function <code>prune.rpart()</code>.</p>
<p>We then plot the tree using <code>fancyRpartPlot()</code> from the package <code>rattle</code>
<span class="citation">(<a href="#ref-williams2011">Williams 2011</a>)</span>. The graph produced displays the number of the node on
top of each node, the predicted class (yes or no), the number of
miss-classified observations, the percentage of observations in the
predicted class for this node. As we see here, in this case, pruning
using the automatic method retrieved a poor tree with only one split,
whether the manually pruned tree is richer and allows us to interpret
the result.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="8.4-classification-and-regression-trees.html#cb70-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb70-2"><a href="8.4-classification-and-regression-trees.html#cb70-2" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb70-3"><a href="8.4-classification-and-regression-trees.html#cb70-3" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb70-4"><a href="8.4-classification-and-regression-trees.html#cb70-4" tabindex="-1"></a></span>
<span id="cb70-5"><a href="8.4-classification-and-regression-trees.html#cb70-5" tabindex="-1"></a><span class="co"># set the seed in order to have replicability of the model</span></span>
<span id="cb70-6"><a href="8.4-classification-and-regression-trees.html#cb70-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>, <span class="at">kind =</span> <span class="st">&quot;Mersenne-Twister&quot;</span>, <span class="at">normal.kind =</span>  <span class="st">&quot;Inversion&quot;</span>)</span>
<span id="cb70-7"><a href="8.4-classification-and-regression-trees.html#cb70-7" tabindex="-1"></a></span>
<span id="cb70-8"><a href="8.4-classification-and-regression-trees.html#cb70-8" tabindex="-1"></a><span class="co"># fully grown tree</span></span>
<span id="cb70-9"><a href="8.4-classification-and-regression-trees.html#cb70-9" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(</span>
<span id="cb70-10"><a href="8.4-classification-and-regression-trees.html#cb70-10" tabindex="-1"></a>        <span class="fu">as.factor</span>(survived) <span class="sc">~</span> ., </span>
<span id="cb70-11"><a href="8.4-classification-and-regression-trees.html#cb70-11" tabindex="-1"></a>        <span class="at">data =</span> ptitanic, </span>
<span id="cb70-12"><a href="8.4-classification-and-regression-trees.html#cb70-12" tabindex="-1"></a>        <span class="at">method =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb70-13"><a href="8.4-classification-and-regression-trees.html#cb70-13" tabindex="-1"></a>        <span class="at">cp=</span><span class="sc">-</span><span class="dv">1</span></span>
<span id="cb70-14"><a href="8.4-classification-and-regression-trees.html#cb70-14" tabindex="-1"></a>)</span>
<span id="cb70-15"><a href="8.4-classification-and-regression-trees.html#cb70-15" tabindex="-1"></a></span>
<span id="cb70-16"><a href="8.4-classification-and-regression-trees.html#cb70-16" tabindex="-1"></a><span class="co"># manually pruned tree</span></span>
<span id="cb70-17"><a href="8.4-classification-and-regression-trees.html#cb70-17" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(</span>
<span id="cb70-18"><a href="8.4-classification-and-regression-trees.html#cb70-18" tabindex="-1"></a>        <span class="fu">as.factor</span>(survived) <span class="sc">~</span> ., </span>
<span id="cb70-19"><a href="8.4-classification-and-regression-trees.html#cb70-19" tabindex="-1"></a>        <span class="at">data =</span> ptitanic, </span>
<span id="cb70-20"><a href="8.4-classification-and-regression-trees.html#cb70-20" tabindex="-1"></a>        <span class="at">method =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb70-21"><a href="8.4-classification-and-regression-trees.html#cb70-21" tabindex="-1"></a>        <span class="co"># min. n. of obs. that must exist in a node in order for a split </span></span>
<span id="cb70-22"><a href="8.4-classification-and-regression-trees.html#cb70-22" tabindex="-1"></a>        <span class="at">minsplit =</span> <span class="dv">2</span>, </span>
<span id="cb70-23"><a href="8.4-classification-and-regression-trees.html#cb70-23" tabindex="-1"></a>        <span class="co"># the minimum number of observations in any terminal node</span></span>
<span id="cb70-24"><a href="8.4-classification-and-regression-trees.html#cb70-24" tabindex="-1"></a>        <span class="at">minbucket =</span> <span class="dv">10</span>, </span>
<span id="cb70-25"><a href="8.4-classification-and-regression-trees.html#cb70-25" tabindex="-1"></a>        <span class="co"># max number of splits</span></span>
<span id="cb70-26"><a href="8.4-classification-and-regression-trees.html#cb70-26" tabindex="-1"></a>        <span class="at">maxdepth =</span> <span class="dv">5</span></span>
<span id="cb70-27"><a href="8.4-classification-and-regression-trees.html#cb70-27" tabindex="-1"></a>)</span>
<span id="cb70-28"><a href="8.4-classification-and-regression-trees.html#cb70-28" tabindex="-1"></a></span>
<span id="cb70-29"><a href="8.4-classification-and-regression-trees.html#cb70-29" tabindex="-1"></a><span class="co"># automatically pruned tree</span></span>
<span id="cb70-30"><a href="8.4-classification-and-regression-trees.html#cb70-30" tabindex="-1"></a><span class="co"># printing and plotting the cross-validated error rate</span></span>
<span id="cb70-31"><a href="8.4-classification-and-regression-trees.html#cb70-31" tabindex="-1"></a><span class="fu">printcp</span>(fit)</span>
<span id="cb70-32"><a href="8.4-classification-and-regression-trees.html#cb70-32" tabindex="-1"></a><span class="fu">plotcp</span>(fit)</span>
<span id="cb70-33"><a href="8.4-classification-and-regression-trees.html#cb70-33" tabindex="-1"></a></span>
<span id="cb70-34"><a href="8.4-classification-and-regression-trees.html#cb70-34" tabindex="-1"></a><span class="co"># pruning the tree accordingly by setting the cp=cpbest</span></span>
<span id="cb70-35"><a href="8.4-classification-and-regression-trees.html#cb70-35" tabindex="-1"></a>fit3 <span class="ot">&lt;-</span> <span class="fu">prune.rpart</span>(fit, <span class="at">cp=</span><span class="fl">0.094</span>)</span>
<span id="cb70-36"><a href="8.4-classification-and-regression-trees.html#cb70-36" tabindex="-1"></a></span>
<span id="cb70-37"><a href="8.4-classification-and-regression-trees.html#cb70-37" tabindex="-1"></a><span class="co"># plotting the tree</span></span>
<span id="cb70-38"><a href="8.4-classification-and-regression-trees.html#cb70-38" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(fit2, <span class="at">caption =</span> <span class="st">&quot;Classification Tree for Titanic pruned manually&quot;</span>)</span>
<span id="cb70-39"><a href="8.4-classification-and-regression-trees.html#cb70-39" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(fit3, <span class="at">caption =</span> <span class="st">&quot;Classification Tree for Titanic pruned automatically&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-89"></span>
<img src="_main_files/figure-html/unnamed-chunk-89-1.png" alt="From left: Complexity vs X-val Relative Error; The automatically pruned CART." width="50%"  /><img src="_main_files/figure-html/unnamed-chunk-89-2.png" alt="From left: Complexity vs X-val Relative Error; The automatically pruned CART." width="50%"  />
<p class="caption">
Figure 8.9: From left: Complexity vs X-val Relative Error; The automatically pruned CART.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-90"></span>
<img src="_main_files/figure-html/unnamed-chunk-90-1.png" alt="The manually pruned CART." width="80%"  />
<p class="caption">
Figure 8.10: The manually pruned CART.
</p>
</div>
<p>If we want to give an interpretation of the manually pruned tree we can
say the following. The probability of dying on board of the Titanic were
about 83% if the passenger was a male, older than 9.5 years old, and
this happened for 61% of the passengers aboard. On the contrary, there
were 93% of chances of surviving by being a woman with a passenger class
different than the 3rd. This statistic applies to 19% of the passengers
abroad.</p>
<p>The algorithm for growing a decision tree is an example of recursive
partitioning. The recursive binary splitting approach is top-down and
greedy. Top-down because it begins at the top of the tree (at which
point all observations belong to a single “region”) and then
successively splits the independent variable’ space; each split is
indicated via two new branches further down on the tree. It is greedy
because at each step of the tree-building process, the best split in
terms of minimum RSS is made at that particular step, rather than
looking ahead and picking a split that will lead to a better tree in
some future step. Each node in the tree is grown using the same set of
rules as its parent node.</p>
<p>A much more powerful use of CART (but less interpretable) is when we
have an ensemble of them. An ensemble method is an approach that
combines many simple “building ensemble block” models (in this case
trees) in order to obtain a single and potentially very powerful model.
Some examples are Bagging, Random Forest, or Boosting. However, these
methodologies are out of the scope of this book.</p>
<p> </p>
<p> </p>
<p> </p>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-lee2018" class="csl-entry">
Lee, James. 2018. <span>“R Decision Trees Tutorial.”</span> <em>Datacamp</em>. <a href="https://www.datacamp.com/tutorial/decision-trees-R">https://www.datacamp.com/tutorial/decision-trees-R</a>.
</div>
<div id="ref-temkin1995" class="csl-entry">
Temkin, Nancy R., Richard Holubkov, Joan E. Machamer, H. Richard Winn, and Sureyya S. Dikmen. 1995. <span>“Classification and Regression Trees (CART) for Prediction of Function at 1 Year Following Head Trauma.”</span> <em>Journal of Neurosurgery</em> 82 (5): 764–71. <a href="https://doi.org/10.3171/jns.1995.82.5.0764">https://doi.org/10.3171/jns.1995.82.5.0764</a>.
</div>
<div id="ref-therneau2022" class="csl-entry">
Therneau, Terry M., and Elizabeth J. Atkinson. 2022. <span>“An Introduction to Recursive Partitioning Using the RPART Routines.”</span> <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a>.
</div>
<div id="ref-williams2011" class="csl-entry">
Williams, Graham J. 2011. <em>Data Mining with Rattle and r: The Art of Excavating Data for Knowledge Discovery</em>. Use r! Springer.
</div>
</div>
<p style="text-align: center;">
<a href="8.3-principal-component-analysis.html"><button class="btn btn-default">Previous</button></a>
<a href="8.5-composite-indicators.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
