<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="8.1 Cluster Analysis | Introduction to Data Analysis with R" />
<meta property="og:type" content="book" />




<meta name="author" content="Federico Roscioli" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="8.1 Cluster Analysis | Introduction to Data Analysis with R">

<title>8.1 Cluster Analysis | Introduction to Data Analysis with R</title>

<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="book_assets/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="book_assets/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="book_assets/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="book_assets/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="book_assets/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface" id="toc-preface">Preface</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction">Introduction</a></li>
<li class="part"><span><b>I The R code</b></span></li>
<li class="has-sub"><a href="1-installation.html#installation" id="toc-installation"><span class="toc-section-number">1</span> Installation</a>
<ul>
<li><a href="1.1-introductory-activities.html#introductory-activities" id="toc-introductory-activities"><span class="toc-section-number">1.1</span> Introductory activities</a></li>
<li><a href="1.2-visualization-suggestions.html#visualization-suggestions" id="toc-visualization-suggestions"><span class="toc-section-number">1.2</span> Visualization suggestions</a></li>
<li><a href="1.3-the-workspace.html#the-workspace" id="toc-the-workspace"><span class="toc-section-number">1.3</span> The workspace</a></li>
</ul></li>
<li class="has-sub"><a href="2-a-b-c.html#a-b-c" id="toc-a-b-c"><span class="toc-section-number">2</span> A, B, C</a>
<ul>
<li><a href="2.1-the-first-code.html#the-first-code" id="toc-the-first-code"><span class="toc-section-number">2.1</span> The first code</a></li>
<li><a href="2.2-indexing.html#indexing" id="toc-indexing"><span class="toc-section-number">2.2</span> Indexing</a></li>
<li><a href="2.3-the-first-function.html#the-first-function" id="toc-the-first-function"><span class="toc-section-number">2.3</span> The first function</a></li>
<li><a href="2.4-dataset-exploration.html#dataset-exploration" id="toc-dataset-exploration"><span class="toc-section-number">2.4</span> Dataset Exploration</a></li>
<li><a href="2.5-subsetting.html#subsetting" id="toc-subsetting"><span class="toc-section-number">2.5</span> Subsetting</a></li>
<li><a href="2.6-importing-and-exporting-data.html#importing-and-exporting-data" id="toc-importing-and-exporting-data"><span class="toc-section-number">2.6</span> Importing and exporting data</a></li>
<li><a href="2.7-exercises.html#exercises" id="toc-exercises"><span class="toc-section-number">2.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="3-data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">3</span> Data Cleaning</a>
<ul>
<li><a href="3.1-variable-names.html#variable-names" id="toc-variable-names"><span class="toc-section-number">3.1</span> Variable Names</a></li>
<li class="has-sub"><a href="3.2-variable-types.html#variable-types" id="toc-variable-types"><span class="toc-section-number">3.2</span> Variable Types</a>
<ul>
<li><a href="3.2-variable-types.html#factor-variables" id="toc-factor-variables"><span class="toc-section-number">3.2.1</span> Factor variables</a></li>
<li><a href="3.2-variable-types.html#dates-and-times" id="toc-dates-and-times"><span class="toc-section-number">3.2.2</span> Dates and times</a></li>
</ul></li>
<li><a href="3.3-row-names.html#row-names" id="toc-row-names"><span class="toc-section-number">3.3</span> Row Names</a></li>
</ul></li>
<li class="has-sub"><a href="4-advanced-data-manipulation-and-plotting.html#advanced-data-manipulation-and-plotting" id="toc-advanced-data-manipulation-and-plotting"><span class="toc-section-number">4</span> Advanced Data Manipulation and Plotting</a>
<ul>
<li><a href="4.1-ifelse.html#ifelse" id="toc-ifelse"><span class="toc-section-number">4.1</span> Ifelse</a></li>
<li><a href="4.2-the-apply-family.html#the-apply-family" id="toc-the-apply-family"><span class="toc-section-number">4.2</span> The Apply family</a></li>
<li><a href="4.3-dplyr.html#dplyr" id="toc-dplyr"><span class="toc-section-number">4.3</span> Dplyr</a></li>
<li><a href="4.4-merging-datasets.html#merging-datasets" id="toc-merging-datasets"><span class="toc-section-number">4.4</span> Merging datasets</a></li>
<li><a href="4.5-melting-vs-transposing.html#melting-vs-transposing" id="toc-melting-vs-transposing"><span class="toc-section-number">4.5</span> Melting vs Transposing</a></li>
<li><a href="4.6-ggplot2.html#ggplot2" id="toc-ggplot2"><span class="toc-section-number">4.6</span> Ggplot2</a></li>
<li><a href="4.7-exercises-1.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">4.7</span> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Statistical Analysis</b></span></li>
<li class="has-sub"><a href="5-exploratory-data-analysis.html#exploratory-data-analysis" id="toc-exploratory-data-analysis"><span class="toc-section-number">5</span> Exploratory Data Analysis</a>
<ul>
<li><a href="5.1-central-tendency-measures.html#central-tendency-measures" id="toc-central-tendency-measures"><span class="toc-section-number">5.1</span> Central Tendency Measures</a></li>
<li><a href="5.2-variability-measures.html#variability-measures" id="toc-variability-measures"><span class="toc-section-number">5.2</span> Variability Measures</a></li>
<li><a href="5.3-inequality-measures.html#inequality-measures" id="toc-inequality-measures"><span class="toc-section-number">5.3</span> Inequality Measures</a></li>
<li><a href="5.4-data-visualization.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">5.4</span> Data visualization</a></li>
<li><a href="5.5-scaling-data.html#scaling-data" id="toc-scaling-data"><span class="toc-section-number">5.5</span> Scaling data</a></li>
<li><a href="5.6-probability-sampling.html#probability-sampling" id="toc-probability-sampling"><span class="toc-section-number">5.6</span> Probability Sampling</a></li>
<li><a href="5.7-exercises-2.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="6-hypothesis-testing.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">6</span> Hypothesis Testing</a>
<ul>
<li><a href="6.1-probability-distributions.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">6.1</span> Probability Distributions</a></li>
<li><a href="6.2-shapiro-wilk-test.html#shapiro-wilk-test" id="toc-shapiro-wilk-test"><span class="toc-section-number">6.2</span> Shapiro-Wilk Test</a></li>
<li><a href="6.3-one-sample-t-test.html#one-sample-t-test" id="toc-one-sample-t-test"><span class="toc-section-number">6.3</span> One-Sample T-Test</a></li>
<li><a href="6.4-unpaired-two-sample-t-test.html#unpaired-two-sample-t-test" id="toc-unpaired-two-sample-t-test"><span class="toc-section-number">6.4</span> Unpaired Two Sample T-Test</a></li>
<li><a href="6.5-mann-whitney-u-test.html#mann-whitney-u-test" id="toc-mann-whitney-u-test"><span class="toc-section-number">6.5</span> Mann Whitney U Test</a></li>
<li><a href="6.6-paired-sample-t-test.html#paired-sample-t-test" id="toc-paired-sample-t-test"><span class="toc-section-number">6.6</span> Paired Sample T-Test</a></li>
<li><a href="6.7-exercises-3.html#exercises-3" id="toc-exercises-3"><span class="toc-section-number">6.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="7-bivariate-analysis.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7</span> Bivariate Analysis</a>
<ul>
<li><a href="7.1-correlation.html#correlation" id="toc-correlation"><span class="toc-section-number">7.1</span> Correlation</a></li>
<li><a href="7.2-linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">7.2</span> Linear Regression</a></li>
<li><a href="7.3-logistic-regressions.html#logistic-regressions" id="toc-logistic-regressions"><span class="toc-section-number">7.3</span> Logistic Regressions</a></li>
<li><a href="7.4-exercises-4.html#exercises-4" id="toc-exercises-4"><span class="toc-section-number">7.4</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="8-multivariate-analysis.html#multivariate-analysis" id="toc-multivariate-analysis"><span class="toc-section-number">8</span> Multivariate Analysis</a>
<ul>
<li class="has-sub"><a href="8.1-cluster-analysis.html#cluster-analysis" id="toc-cluster-analysis"><span class="toc-section-number">8.1</span> Cluster Analysis</a>
<ul>
<li><a href="8.1-cluster-analysis.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">8.1.1</span> Hierarchical Clustering</a></li>
<li><a href="8.1-cluster-analysis.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">8.1.2</span> K-Means clustering</a></li>
<li><a href="8.1-cluster-analysis.html#the-silhouette-plot" id="toc-the-silhouette-plot"><span class="toc-section-number">8.1.3</span> The silhouette plot</a></li>
</ul></li>
<li><a href="8.2-heatmap.html#heatmap" id="toc-heatmap"><span class="toc-section-number">8.2</span> Heatmap</a></li>
<li><a href="8.3-principal-component-analysis.html#principal-component-analysis" id="toc-principal-component-analysis"><span class="toc-section-number">8.3</span> Principal Component Analysis</a></li>
<li><a href="8.4-classification-and-regression-trees.html#classification-and-regression-trees" id="toc-classification-and-regression-trees"><span class="toc-section-number">8.4</span> Classification And Regression Trees</a></li>
<li class="has-sub"><a href="8.5-composite-indicators.html#composite-indicators" id="toc-composite-indicators"><span class="toc-section-number">8.5</span> Composite Indicators</a>
<ul>
<li><a href="8.5-composite-indicators.html#mazziotta-pareto-index" id="toc-mazziotta-pareto-index"><span class="toc-section-number">8.5.1</span> Mazziotta-Pareto Index</a></li>
<li><a href="8.5-composite-indicators.html#adjusted-mazziotta-pareto-index" id="toc-adjusted-mazziotta-pareto-index"><span class="toc-section-number">8.5.2</span> Adjusted Mazziotta-Pareto Index</a></li>
</ul></li>
<li><a href="8.6-exercises-5.html#exercises-5" id="toc-exercises-5"><span class="toc-section-number">8.6</span> Exercises</a></li>
</ul></li>
<li><a href="final-remarks.html#final-remarks" id="toc-final-remarks">Final Remarks</a></li>
<li><a href="bibliography.html#bibliography" id="toc-bibliography">Bibliography</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="cluster-analysis" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Cluster Analysis</h2>
<p>Cluster analysis is an exploratory data analysis tool for solving
classification problems. Its objective is to sort observations into
groups, or clusters, so that the degree of association is strong between
members of the same cluster and weak between members of different
clusters. Each cluster thus describes, in terms of the data collected,
the class to which its members belong; and this description may be
abstracted through use from the particular to the general class or type.
Cluster analysis is thus a tool of discovery. It may reveal associations
and structure in data which, though not previously evident, nevertheless
are sensible and useful once found. The results of cluster analysis may
contribute to the definition of a formal classification scheme, such as
a taxonomy for related animals, insects or plants; or suggest
statistical models with which to describe populations; or indicate rules
for assigning new cases to classes for identification and diagnostic
purposes; or provide measures of definition, size and change in what
previously were only broad concepts; or find exemplars to represent
classes. Whatever business you’re in, the chances are that sooner or
later you will run into a classification problem.</p>
<p>Cluster analysis includes a broad suite of techniques designed to find
groups of similar items within a data set. Partitioning methods divide
the data set into a number of groups predesignated by the user.
Hierarchical cluster methods produce a hierarchy of clusters from small
clusters of very similar items to large clusters that include more
dissimilar items <span class="citation">(<a href="#ref-abdi2010">Abdi and Williams 2010</a>)</span>. Both clustering and PCA seek to simplify
the data via a small number of summaries, but their mechanisms are
different: PCA looks to find a low-dimensional representation of the
observations that explain a good fraction of the variance; clustering
looks to find homogeneous subgroups among the observations.</p>
<p>As mentioned, when we cluster the observations of a data set, we seek to
partition them into distinct groups so that the observations within each
group are quite similar to each other, while observations in different
groups are quite different from each other. Of course, to make this
concrete, we must define what it means for two or more observations to
be similar or different.</p>
<p>In order to define the similarity between observations we need a
“metric”. The Eucludean distance is the most common metric used in
cluster analysis, but many others exist (see the help in the <code>dist()</code>
function for more detail). We also need to choose which algorithm we
want to apply in order for the computer to assign the observations to
the right cluster.</p>
<p>Remember that Clustering has to be performed on <u>continuous scaled
data</u>. If the variables you want to analyze are categorical,
you should use scaled dummies.</p>
<p> </p>
<div id="hierarchical-clustering" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Hierarchical Clustering</h3>
<p>Hierarchical methods usually produce a graphical output known as a
dendrogram or tree that shows this hierarchical clustering structure.
Some hierarchical methods are divisive, that progressively divide the
one large cluster comprising all of the data into two smaller clusters
and repeat this process until all clusters have been divided. Other
hierarchical methods are agglomerative and work in the opposite
direction by first finding the clusters of the most similar items and
progressively adding less similar items until all items have been
included into a single large cluster. Hierarchical methods are
particularly useful in that they are not limited to a predetermined
number of clusters and can display similarity of samples across a wide
range of scales.</p>
<p>Bottom-up or agglomerative clustering is the most common type of
hierarchical clustering, and refers to the fact that a dendrogram is
built starting from the leaves and combining clusters up to the trunk.
As we move up the tree, some leaves begin to fuse into branches. These
correspond to observations that are similar to each other. As we move
higher up the tree, branches themselves fuse, either with leaves or
other branches. The earlier (lower in the tree) fusions occur, the more
similar the groups of observations are to each other. On the other hand,
observations that fuse later (near the top of the tree) can be quite
different. <strong>The height of this fusion, as measured on the vertical
axis, indicates how different the two observations are.</strong></p>
<p>Hierarchical clustering allows also to select the method you want to
apply. <em>Ward’s</em> minimum variance method aims at finding compact,
spherical clusters. The <em>complete linkage</em> method finds similar
clusters. The <em>single linkage</em> method (which is closely related to the
minimal spanning tree) adopts a ‘friends of friends’ clustering
strategy. The other methods can be regarded as aiming for clusters with
characteristics somewhere between the single and complete link methods.</p>
<p>The first step in order to proceed with hierarchical cluster analysis is
to compute the “distance matrix”, which represents how distance are the
observations among themselves. For this step, as mentioned before, the
Euclidean distance is one of the most common metrics used. We then
properly run the hierarchical cluster analysis (function <code>hclust()</code>)
specifying the method for complete linkage. Finally we plot the
dendogram. <em>It is important that each row of the dataset has a name
assigned (see <a href="3.3-row-names.html#row-names">Row Names</a>)</em>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="8.1-cluster-analysis.html#cb60-1" tabindex="-1"></a><span class="co"># Euclidean distance</span></span>
<span id="cb60-2"><a href="8.1-cluster-analysis.html#cb60-2" tabindex="-1"></a>dist <span class="ot">&lt;-</span> <span class="fu">dist</span>(swiss, <span class="at">method=</span><span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb60-3"><a href="8.1-cluster-analysis.html#cb60-3" tabindex="-1"></a><span class="co"># Hierarchical Clustering with hclust</span></span>
<span id="cb60-4"><a href="8.1-cluster-analysis.html#cb60-4" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist, <span class="at">method=</span><span class="st">&quot;complete&quot;</span>)</span>
<span id="cb60-5"><a href="8.1-cluster-analysis.html#cb60-5" tabindex="-1"></a><span class="co"># Plot the result</span></span>
<span id="cb60-6"><a href="8.1-cluster-analysis.html#cb60-6" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">hang=</span><span class="sc">-</span><span class="dv">1</span>, <span class="at">cex=</span>.<span class="dv">5</span>)</span>
<span id="cb60-7"><a href="8.1-cluster-analysis.html#cb60-7" tabindex="-1"></a><span class="fu">rect.hclust</span>(hc, <span class="at">k=</span><span class="dv">3</span>, <span class="at">border=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:fig2"></span>
<img src="_main_files/figure-html/fig2-1.png" alt="Dendogram plot." width="100%"  />
<p class="caption">
Figure 8.1: Dendogram plot.
</p>
</div>
<p>The last line of code adds the rectangles highlighting 3 clusters. The
number of cluster is a personal choice, there is no strict rule about
how to identify them. The common rule of thumb is to look at the height
(vertical axes of the dendogram) and cut it where the highest jump
occurs between the branches. In this case it corresponds to 3 clusters.</p>
<p>Because of its agglomerative nature, clusters are sensitive to the order
in which samples join, which can cause samples to join a grouping to
which it does not actually belong. In other words, if groups are known
beforehand, those same groupings may not be produced from cluster
analysis. Cluster analysis is sensitive to both the distance metric
selected and the criterion for determining the order of clustering.
Different approaches may yield different results. Consequently, the
distance metric and clustering criterion should be chosen carefully. The
results should also be compared to analyses based on different metrics
and clustering criteria, or to an ordination, to determine the
robustness of the results.Caution should be used when defining groups
based on cluster analysis, particularly if long stems are not present.
Even if the data form a cloud in multivariate space, cluster analysis
will still form clusters, although they may not be meaningful or natural
groups. Again, it is generally wise to compare a cluster analysis to an
ordination to evaluate the distinctness of the groups in multivariate
space. <em>Transformations may be needed to put samples and variables on
comparable scales; otherwise, clustering may reflect sample size or be
dominated by variables with large values.</em></p>
<p> </p>
</div>
<div id="k-means-clustering" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> K-Means clustering</h3>
<p>K-means clustering is a simple and elegant approach for partitioning a
data set into K distinct, non-overlapping clusters. To perform K-means
clustering, we must <u>first specify the desired number of clusters
K</u>; then the K-means algorithm assigns each observation to
exactly one of the K clusters. The idea behind K-means clustering is
that a good clustering is one for which the within-cluster variation is
as small as possible. The K-Means algorithm, in an iteratively way,
defines a centroid for each cluster, which is a point (imaginary or
real) at the center of a cluster, and adjusts it until there is no
possible change anymore. The metric used is the Squared Sum of Euclidean
distances. <u>The main limitation of K-means is understanding which is the
right k prior to the analysis</u>. Also, K-means is an
algorithm that tends to perform well only with spherical clusters, as it
looks for centroids.</p>
<p>The function <code>kmeans()</code> allows to run K-Means clustering given the
preferred number of clusters (centers). The results can be appreciated
by plotting the clusters using the <code>fviz_cluster()</code> function from the
package factoextra <span class="citation">(<a href="#ref-kassambara">Kassambara and Mundt, n.d.</a>)</span>. <em>Note that in order to plot the
clusters from K-means the function automatically reduces the
dimensionality of the data via PCA and selects the first two
components.</em></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="8.1-cluster-analysis.html#cb61-1" tabindex="-1"></a><span class="co"># calculate the k-means for the preferred number of clusters</span></span>
<span id="cb61-2"><a href="8.1-cluster-analysis.html#cb61-2" tabindex="-1"></a>kc <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(swiss, <span class="at">centers=</span><span class="dv">3</span>)</span>
<span id="cb61-3"><a href="8.1-cluster-analysis.html#cb61-3" tabindex="-1"></a><span class="fu">library</span>(factoextra) </span></code></pre></div>
<pre><code>## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="8.1-cluster-analysis.html#cb63-1" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data=</span>swiss, <span class="at">cluster=</span>kc<span class="sc">$</span>cluster))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig3"></span>
<img src="_main_files/figure-html/fig3-1.png" alt="K-means clustering." width="80%"  />
<p class="caption">
Figure 8.2: K-means clustering.
</p>
</div>
<p> </p>
</div>
<div id="the-silhouette-plot" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> The silhouette plot</h3>
<p>Silhouette analysis can be used to study the separation distance between
the resulting clusters. This analysis is usually carried out prior to
any clustering algorithm <span class="citation">(<a href="#ref-syakur2018">Syakur et al. 2018</a>)</span>. In fact, the silhouette plot
displays a measure of how close each point in one cluster is to points
in the neighboring clusters and thus provides a way to assess parameters
like number of clusters visually. This measure has a range of <span class="math inline">\(-1, 1\)</span>.
The value of <em>k</em> that maximizes the silhouette width is the one
minimizing the distance within the clusters and maximizing the distance
between them. However, it is important to remark that <u>the silhouette
plot analysis provides just a rule of thumb for cluster
selection</u>.</p>
<p>In the case of the <code>swiss</code> dataset, the silhouette plot suggests the
presence of only two clusters both using hierarchical clustering and
K-Means. As we have seen previously this is not properly true.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="8.1-cluster-analysis.html#cb64-1" tabindex="-1"></a><span class="co">#silhouette method</span></span>
<span id="cb64-2"><a href="8.1-cluster-analysis.html#cb64-2" tabindex="-1"></a><span class="fu">library</span>(factoextra) </span>
<span id="cb64-3"><a href="8.1-cluster-analysis.html#cb64-3" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(swiss, <span class="at">FUN =</span> hcut)</span>
<span id="cb64-4"><a href="8.1-cluster-analysis.html#cb64-4" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(swiss, <span class="at">FUN =</span> kmeans)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:fig4"></span>
<img src="_main_files/figure-html/fig4-1.png" alt="From left: Hierarchical clustering silhouette plot; K-means clustering silhouette plot." width="50%"  /><img src="_main_files/figure-html/fig4-2.png" alt="From left: Hierarchical clustering silhouette plot; K-means clustering silhouette plot." width="50%"  />
<p class="caption">
Figure 8.3: From left: Hierarchical clustering silhouette plot; K-means clustering silhouette plot.
</p>
</div>
<p> </p>
<p> </p>
<p> </p>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-abdi2010" class="csl-entry">
Abdi, Hervé, and Lynne J. Williams. 2010. <span>“Principal Component Analysis: Principal Component Analysis.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 2 (4): 433–59. <a href="https://doi.org/10.1002/wics.101">https://doi.org/10.1002/wics.101</a>.
</div>
<div id="ref-kassambara" class="csl-entry">
Kassambara, Alboukadel, and Fabian Mundt. n.d. <span>“Factoextra : Extract and Visualize the Results of Multivariate Data Analyses.”</span> <a href="https://rpkgs.datanovia.com/factoextra/index.html">https://rpkgs.datanovia.com/factoextra/index.html</a>.
</div>
<div id="ref-syakur2018" class="csl-entry">
Syakur, M A, B K Khotimah, E M S Rochman, and B D Satoto. 2018. <span>“Integration k-Means Clustering Method and Elbow Method for Identification of the Best Customer Profile Cluster.”</span> <em>IOP Conference Series: Materials Science and Engineering</em> 336 (April): 012017. <a href="https://doi.org/10.1088/1757-899X/336/1/012017">https://doi.org/10.1088/1757-899X/336/1/012017</a>.
</div>
</div>
<p style="text-align: center;">
<a href="8-multivariate-analysis.html"><button class="btn btn-default">Previous</button></a>
<a href="8.2-heatmap.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
