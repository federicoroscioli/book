[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"real goal manual teach R language per se, \nallow students manage basic concepts order able\nexplore analyze data using R. course student \ncapable clearly understand R code someone else wrote \ncustomize code according need. means \npossibility explore different complex solutions \nstudent’s problems exploring new packages paving way \nbecome statistician!also created also ad-hoc R playground accessible\n. R\nplayground allows student exercise order reinforce \nknowledge R data analysis. Exercises fundamental order \nfix knowledge acquired class. platform structured \nway manual order linear learning process. \nfinally want remark code provide “best easiest\nversion solution”, hope appreciate . fact, \nR, possible thing 1000 different ways, \nlooking internet can confirmation .website free use, licensed GNU General\nPublic License 2.0. ’d like pdf copy book, can\ndownload \n.","code":""},{"path":"index.html","id":"author","chapter":"Preface","heading":"Author","text":"Federico Roscioli PhD Student Economics Finance \nUniversity Rome Tor Vergata. international experience \nconsultant various International Organizations Research\nInstitutes background freelance photojournalist. research\ninterests include: development issues, environmental sustainability,\npoverty, social protection, food security nutrition.www.federicoroscioli.com","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"want something, learn everything.\nJohn Carroll, “Minimal manual”Many people think R statistics system. prefer consider \nenvironment within statistical techniques implemented. R\ncan extended (easily) via packages allowing execute various\nstatistical graphical techniques, including linear nonlinear\nmodeling, classical statistical tests, spatial time-series analysis,\nclassification, clustering, others.first time coding, R may slow tedious \nbeginning. pass lot time trying understand R\ngiving error instead solution problem. Stack\nOverflow R Help become \nbest friends. software going seem judge mistakes\nharshly. read word “syntax error” lot. \nprobably grow kind hate creature lives inside \ncomputer like . think creature \nvalue judging code. crash laptop loose \ndata… Ok, ok! ’m bit exaggerating… don’t want scare \nmuch, yes, worst case scenario many \ncolleagues already passed , survived!fact , R language , , practiced \nmuch possible order learn . R “” coding\nlanguage, able speak Rish friends\n(fortunately!) write letter pals. option \npractice computer (also group prefer). \nsyntax errors get mean R thinks ’re bad. \njudgment ability statistician. Syntax errors mean\nR lost doesn’t better words tell . Beside \nfact can allow marvelous things, R understands \ntalk language. , suggestion get \nerror :Read loud error try make sense problem\n(usually R suggest ).Read loud error try make sense problem\n(usually R suggest ).Check carefully code, may missing comma \nmisspelled capital letter.Check carefully code, may missing comma \nmisspelled capital letter.Copy paste error google look answers. works \nmay think.Copy paste error google look answers. works \nmay think.Contact .Contact .steps almost always work, general want never\nloose effort. figure , sure . \nneed build experience first. like learn ride \nbicycle, beginning fall lot, end \nlove cycling never want get bike.final wish : first contact R, “lost”,\n“lost”, “lost”, “lost”, “lost”, “oh wow, love !”. hope \ngo exact feeling., please, fun, curious, exercise lot!manual composed two main sections divided chapters. \nfirst section introduce reader basic characteristics \nsoftware required, installation configuration, \nlinguistic focus, aims teaching use R order \nvisualize, explore, understand, manipulate create data. \nprogressive way, starting simple computations ending\ncomplex data manipulation plotting. second section,\ninstead, applied statistics, exploratory\nanalysis, introduction hypothesis testing \nimplications, bivariate multivariate analysis methods, \nprincipal component analysis classification regression trees.","code":""},{"path":"installation.html","id":"installation","chapter":"1 Installation","heading":"1 Installation","text":"   ","code":""},{"path":"installation.html","id":"introductory-activities","chapter":"1 Installation","heading":"1.1 Introductory activities","text":"First need install software allow us work\nR language (R Core Team 2022). need install two\nsoftware: R R-Studio. first compiler R language, \nneed install never use directly. latter \ninterface software runs top R allows us \nfacilitation suggestions working. Within manual \nuse R R-Studio synonyms, referring always \nuse R-Studio.Additional R R-Studio need install packages. \npackages extension software bring additional\nfunctions /data.install software, follow 9 steps :Download R installer CRAN.\nFigure 1.1: Select version R according operating system.\nRun installer keeping default settings. admin rights computer, please ask Support give full permissions R directories. Otherwise able install packages afterwards.Run installer keeping default settings. admin rights computer, please ask Support give full permissions R directories. Otherwise able install packages afterwards.Download R-Studio installer R-Studio.Download R-Studio installer R-Studio.\nFigure 1.2: Select version R-Studio according operating system.\ninstallation R completed (), run R-Studio installer keeping default settings.installation R completed (), run R-Studio installer keeping default settings.Run R-Studio. open window like one image .Run R-Studio. open window like one image .\nFigure 1.3: R-Studio.\nleft hand window, sign “>”, type “4+5” (without quotes) hit enter. output line reading “\\([1]\\) 9” appear. means R R-Studio working properly. successful, please contact .left hand window, sign “>”, type “4+5” (without quotes) hit enter. output line reading “\\([1]\\) 9” appear. means R R-Studio working properly. successful, please contact .Go Tools -> Install Packages install packages:“gt”, “readxl”. See image .Go Tools -> Install Packages install packages:“gt”, “readxl”. See image .Check packages installed typing “library(gt)” (without quotes) prompt press enter.Finally type “sessioninfo()” (without quotes) check gt installed.    ","code":""},{"path":"installation.html","id":"visualization-suggestions","chapter":"1 Installation","heading":"1.2 Visualization suggestions","text":"Following visualization suggestions may explore.\nPersonally, find really helpful.Setting work-space:\nView -> Panes -> Panes Layout\nclockwise top-left : Source, Environment,\nFiles, Console\nSetting work-space:View -> Panes -> Panes Layoutclockwise top-left : Source, Environment,\nFiles, ConsoleSetting color style code:\nTools > Global Options -> Appearance -> Editor Theme ->\nXcode\nSetting color style code:Tools > Global Options -> Appearance -> Editor Theme ->\nXcode    ","code":""},{"path":"installation.html","id":"the-workspace","chapter":"1 Installation","heading":"1.3 The workspace","text":"source text file extension .R can saved \nopened every version R R-Studio. file allow us \nrun rerun bunch code, modify details made mistake\nwant change something. always best friend.Environment place R saves temporarily data\ntell “” save. environment never clean \ncontain essential objects need (maybe happen \ngreat programmer, now). see \ndata-sets, variables, vectors, etc…bottom right part screen devoted many things.\nViewer, Plots Help activated automatically \nshow requested output. Packages, instead, useful \ninstall new packages.Console can write code saved,\ntemporary history console . space \nalso R give us feedback inputs form results,\nwarnings errors.","code":""},{"path":"a-b-c.html","id":"a-b-c","chapter":"2 A, B, C","heading":"2 A, B, C","text":"   ","code":""},{"path":"a-b-c.html","id":"the-first-code","chapter":"2 A, B, C","heading":"2.1 The first code","text":"can start exploring big potential R typing small basic\nmathematical operations. suggestion always work source\nfile1 run commands line line using “Run” button (\nControl+Enter Windows, Command+Enter Mac).Now can create “objects”. means storing numbers series\nnumbers within memory computer (Environment), \nable use objects analysis. can also create \nvector assigning one value object. purpose,\nneed use combine function c() put \nparentheses values separated comma. want \ncreate vector comprising ordered series integer numbers,\nshortcut: can use colon symbol starting\nending numbers. can also operations values \nobjects. Remember assign object another\ncontent, loose original one. R “undo”\nbutton.can see syntax used scope. can see, within\ncode comments. Comments great tool allow us\nunderstand code stands , let people (us)\nunderstand better. placing one () hashtag (#) within \ncode, transform whole line comment R skip\nline running.notations <- = equivalent assigning values \nobject, however suggestion always use former assign\nvalues, order avoid confusion equal sign. can see,\nstore something Environment, R showing us.\nneed “call” order see inside object.Read code , guess result running \ncode.   ","code":"\n1 + 1\n2 * 5\n# = and <- are the same, but I prefer <- \nx = 3\nx <- 5\n\n# with the == sign you ask R if a condition is true\nx == 7\n\n# single value assignment\nx <- \"hello\"\n\nx <- 2 * 5\nx\n\n# vector assignment\ny <- c(3, 4, 2, 4, 5, 1)\n\ny == 4\n\n# character vector\nt <- c(\"hello\", \"how\", \"are you\", \"?\")\nt\n\n# oredered integer series\ny <- c(1:10)\n# operations\nx * 5\n\n# what will happen here?\nx * y\nx - y\ny - x"},{"path":"a-b-c.html","id":"indexing","chapter":"2 A, B, C","heading":"2.2 Indexing","text":"indexing system allows us select specific part data. \n, need first call name object want search\n(y), put searching instruction squared\nparenthesis [ ] right . example, selecting first, second\nfourth value y[c(1,2,4)]; values bigger 3\ny[y>3], can give multiple conditions together y[y<3 & y>10],\n. Another common useful instruction select \nvalues one object \\(y\\) present another object \\(z\\)\ny[y %% z].2Try following code guess result running\n.recognize indexing system immediate may result\nconfusing majority. suggestion exercise lot .\nDon’t shy creating objects sorts select whatever \nwant. Later chapter Advanced Data Manipulation \nPlotting see two friendly methods sub-setting (\nsubset() function package dplyr), , please, \nunderestimate power indexing system.Now master numeric vectors, time learn R able \nmanage many different types data. precisely vector can :\ncharacter, numeric (real decimal), integer, logical, \ncomplex. used vectors however, character (like one\ncalled t first code chunk), numeric logical (TRUE \nFALSE). Another interesting type factor vector, see\naction Data Cleaning. aware vector composed\ncharacters numbers, considered automatically \ncharacter vector! applies also numbers comma\ninstead dot decimals!  \n ","code":"\ny <- c(30:70)\n# partial selection\ny[c(1,3,15)]\ny[c(1:3,5)]\nz <- y[1:4]\n# subtractive selection\ny[-4]\ny[-c(1:3,5)]\n\n# assigning values only to a subset of observations\ny[c(1,3,15)] <- 3\ny\n\n# equality selection\ny[y==3]\ny[y==3 & y==45]\ny[y==3 | y==45]\n\nz <- c(30:50)\ny[y %in% z]"},{"path":"a-b-c.html","id":"the-first-function","chapter":"2 A, B, C","heading":"2.3 The first function","text":"first R function life mean. running \nsuggest explore Help documentation function typing \nconsole ?mean. Within help page, appear right\nside work-space, find instructions use\nfunction. “options” specified within normal parenthesis\n() called arguments. default value \nothers , instead needed order run function. \ncase x needed: object want compute \nmean. help provides exhaustive explanation function\nworks, plus cites references examples. importantly, \nhelp available function package, , 99% \ncases, provide mentioned information.Functions works calling (aware R case sensitive) \nputting normal parenthesis required arguments.Try following code try guess result.happens compute mean vector \nmissing observations (NA)? Missing observations zeros, \nsum computed. reason, add na.rm=TRUE\nargument mean function, R strips missing observations \napplying function, gives us result. aware \nstripping missing observation, vector smaller\nlength, , may implications want per-capita\naverage example.  \n ","code":"\nmean(y)\nmean(z)\n\n# what will happen here?\nmean(y[1:4])\n\n# calculate the mean manually\nsum(y)/length(y)\n\np <- c(1, NA, 2, 4, NA, 7, 9, 30, NA)\nmean(p)\n# why is it NA?\n\nmean(p, na.rm = TRUE)\n# check in the help what na.rm does!\nmean(p[!is.na(p)])\n\n# we can see how many values p has, and how many of them are NA (Please note that \n# for NA we do not use the normal equality expressions)\nlength(p)\nlength(p[is.na(p)])\n\n# the function class tells us which is the type of data in the vector\nclass(y)\nclass(t)"},{"path":"a-b-c.html","id":"dataset-exploration","chapter":"2 A, B, C","heading":"2.4 Dataset Exploration","text":"dataset matrix \\(r*c\\), \\(r\\) rows representing observations\n\\(c\\) columns representing variables. words, dataset \ncomposed \\(r\\) horizontal vectors, \\(c\\) longitudinal vectors. \nmind, can now load dataset start exploring \ndimensions.scope use dataset embedded R called\nmtcars. order better understand mtcars refers , type\n?mtcars Console read carefully help says .know working data car design performances\nbelonging 1973 favorite hobby, R dataset \ncleaned, ready use, amazing documentation attached, \nperfect learning purposes.following code presents us exploratory activities \ncarried get hands new data order \nunderstand shape.first explore internal structure dataset (str(mtcars)),\nmeaning: kind dataset (yes, multiple types),\nnumber observations (rows) variables (columns), name\ntype variable small preview. dataset \n“matrix”, “data frame”, “thible”, etc… go \ndetails type dataset, suggestion always use data\nframe, one flexible complete form \nbi-dimensional data. order convert data frame whatever kind \ndataset, use .data.frame() function.3Another important skill acquire capability create dataset\nzero. , exist millions ways . \nexamples create dataset.   ","code":"\n# loading an internal dataset\ndata(\"mtcars\")\n\n# structure of the dataset\nstr(mtcars)\n\n# visualizing the first 6 rows and the last 6\nhead(mtcars)\ntail(mtcars)\n\n# variables names\ncolnames(mtcars)\n\n# number of rows and number of columns\nnrow(mtcars)\nncol(mtcars)\ndim(mtcars)\n# by specifying it columnwise\npeople <- data.frame(name = c(\"Mary\", \"Mike\", \"Greg\"),\n                     age = c(44, 52, 46),\n                     IQ = c(160, 95, 110))\n\n# by creating some vectors and then binding them togheter column-wise\nname <- c(\"Mary\", \"Mike\", \"Greg\")\nage <- c(44, 52, 46)\nIQ <- c(160, 95, 110)\npeople2 <- data.frame(cbind(name, age, IQ))"},{"path":"a-b-c.html","id":"subsetting","chapter":"2 A, B, C","heading":"2.5 Subsetting","text":"Now can create complex objects, need able “destroy”\n, meaning split subsets according interesting\ncharacteristics (James et al. 2021). Sub-setting heart data\nmanipulation basis data analysis. Via subsetting can\nanalyze data one group observations within dataset\naccording interesting characteristics define. example,\nconsumption (mpg) manual cars vs automatic cars., use indexing expressions used \nIndexing chapter, time specify two dimensions\n(rows columns) separated comma within squared brackets\n[rows,columns]. Remember specify either rows \ncolumns, R consider whole set corresponding dimension.Explore code . Use str() function, specifying new\nobject created line code, order understand \nhappened.want subset one single column dataset can use $\nsign.4 opens opportunities specifying peculiar\ncharacteristics want retain dataset.also easier (discursive) way: subset() function.\nfunction takes 3 arguments: data frame want subsetted, \nrows corresponding condition want subsetted, \ncolumns want returned. argument drop=TRUE allows us drop\nrow names vector final output. course can input\nmultiple conditions select multiple columns.code leads result last lines \nprevious code chunk.   ","code":"\n# to subset we use [rows,columns]\na <- mtcars[2, 5]\nstr(a)\n\n# what will happen here?\nb <- mtcars[3,]\nstr(b)\n\n# and here?\nc <- mtcars[,3]\nstr(c)\n\n# we want to see the first 5 rows and the column from 3 to 8\nmtcars[1:5, 3:8]\n# to call a singe variable we use the $ sign\nmtcars$cyl\n# it is the same as\nmtcars[,2]\n\n# equality subsetting: we want all the data of the cars with 4 cylinders\nmtcars[mtcars$cyl == 4,]\n\n# we want the values of the firt clumn (mpg) of cars with 4 cylinders\nmtcars[mtcars$cyl == 4, 1]\n# it is the same as\nmtcars$mpg[mtcars$cyl == 4] # I prefer this\n\n# subsetting consumption per type of transmission\n# automatic cars\nmpg.at <- mtcars$mpg[mtcars$am == 0]\n# manual cars\nmpg.mt <- mtcars$mpg[mtcars$am == 1]\n# subsetting consumption per type of transmission\nmpg.at2 <- subset(mtcars, am == 0, select = \"mpg\", drop=TRUE)\nmpg.mt2 <- subset(mtcars, am == 1, select = \"mpg\", drop=TRUE)\n\n# multiple conditions and columns\nmix <- subset(mtcars, am==0 & cyl==4, select=c(\"mpg\", \"hp\"))"},{"path":"a-b-c.html","id":"importing-and-exporting-data","chapter":"2 A, B, C","heading":"2.6 Importing and exporting data","text":"course none expects us work internal datasets, \nkeep data computer, need know import \nexport files containing data. R super powerful tool \ncleaning data, important data import follow \nleast basic rule one observation per row one variable per\ncolumn (meaning variables : “income2021”,\n“income2020”, etc; one column years one income).\nrules clean data available Data Cleaning.first thing working external files set \nworking directory within code (finction setwd()). tell R\nlook files put new ones. Imaging \nshare code someone else, person \nchange working directory path running code\nsuccessfully.order import external data R, go File -> Import\nDataset select format dataset import. R can read \ncommon data formats (text based like csv txt, Excel,\nStata, SAS, SPSS)(Wickham Bryan 2022), use packages can\nextend importing capacity spacial data, formats . \nselected appropriate format, new window appear asking\nselect file set options need order read\nproperly. Finally, suggestion copy code appear \nConsole paste Source. way able \nreload file anytime without losing time windows clicks.seen Installation chapter install packages,\nhowever, order avoid overloading computers, R activates \nsmall set default packages open . means \nusing content (function, data, object) additional package, \nneed activate using function library(). package \nactivated, remain active whole session work, \nneed call .teaching career, seen many students huge list\npackages called beginning code (useless\npurposes). method allowed avoid remembering \npurpose package, also overcharged computers\noften leaded software crashes data losses. , please, avoid\n!Download wine dataset, change directory folder put downloaded\nfile, try import (text based csv file). Try village dataset, aware one Excel format.end first chapter talk also save \nfinal output work. common way export dataframe\nexcel file. order , need use package\n“xlsx” (Dragulescu Arendt 2020). Note always important set \nworking directory order tell R put exported file.\nbeginning code, need repeat\n.   ","code":"\n# setting the working directory\nsetwd(\"/Users/federicoroscioli/Desktop\")\n# or setting the working directory as the same where the Source file is stored\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\n# for text based datasets (csv)\nwine <- read.csv(\"Wine.csv\")\n\n# for excel datasets you need the readxl package\nlibrary(readxl)\nvillage <- read_excel(\"Village.xlsx\")\n# exporting to excel\nlibrary(\"xlsx\")\nwrite.xlsx(mtcars, file = \"mtcars.xlsx\")\n\n# exporting to csv\nwrite.csv(mtcars, file = \"mtcars.csv\")"},{"path":"a-b-c.html","id":"exercises","chapter":"2 A, B, C","heading":"2.7 Exercises","text":"James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). \nIntroduction Statistical Learning (Vol. 103). Springer New York.\nAvailable . Chapter 2.4 exercises 8, 9, 10.R playground, , B, C","code":""},{"path":"data-cleaning.html","id":"data-cleaning","chapter":"3 Data Cleaning","heading":"3 Data Cleaning","text":"Happy families alike; every unhappy family unhappy \nway.\nLev TolstoyIt often said 80% data analysis spent process \ncleaning preparing data. Data cleaning just one step, \nmust repeated many times course analysis new problems\narise new data collected. tidy data standard designed\nfacilitate initial exploration analysis data, \nsimplify development data analysis tools work well together.dataset collection values, usually either numbers (\nquantitative) strings (qualitative). statistical datasets \nrectangular tables made rows columns. columns almost\nalways labeled rows sometimes labeled. Values organized\ntwo ways. Every value belongs variable (column) \nobservation (row). variable contains values measure \nunderlying attribute (like height, temperature, duration) across\nunits. observation contains values measured unit\n(like person, day, race) across attributes.tidy data (Wickham 2014):1. variable forms column.2. observation forms row.3. type observational unit forms table.Tidy data makes easy analyst computer extract needed\nvariables provides standard way structuring dataset.\nalso particularly well suited vectorized programming\nlanguages like R, layout ensures values different\nvariables observation always paired. order\nvariables observations affect analysis, good ordering\nmakes easier scan raw values.five common problems messy datasets :Column names values codes variable names.Column names values codes variable names.Multiple variables stored one column.Multiple variables stored one column.Variables stored rows columns.Variables stored rows columns.Multiple types observational units stored table\n(.e. people household).Multiple types observational units stored table\n(.e. people household).single observational unit stored multiple tables.single observational unit stored multiple tables.Variables types miss-specified due inputational errors (comma\ndecimals, written comments within numeric columns).Variables types miss-specified due inputational errors (comma\ndecimals, written comments within numeric columns).   ","code":""},{"path":"data-cleaning.html","id":"variable-names","chapter":"3 Data Cleaning","heading":"3.1 Variable Names","text":"Working external code data, often see variable names object\nnames \\(X, y, xs, x1, x2, tp, tn, clf, reg, xi, yi, ii\\). \ncreates problem need analyze specific variable, \nunderstand results get. fact, go back \ncode-book understand content variable. put frankly,\npeople (included) terrible naming objects, especially \nneed invent lot names, bigger effort needed\n(Koehrsen 2019).three fundamental ideas keep mind naming variables:variable name must describe information represented \nvariable. variable name tell us specifically words \nvariable stands .variable name must describe information represented \nvariable. variable name tell us specifically words \nvariable stands .code read others us future. \nprioritize easy understood, rather quickly \nwritten.code read others us future. \nprioritize easy understood, rather quickly \nwritten.Adopt standard conventions naming can make one global\ndecision instead multiple local decisions throughout \nprocess.Adopt standard conventions naming can make one global\ndecision instead multiple local decisions throughout \nprocess.order change names variables can use following\ncode. Basically function colnames() retrieves names \nvariables character vector can change seen \nIndexing.   ","code":"\n# retreiving variable names\ncolnames(mtcars)\n\n# changing the name of the variable mpg to consumption\ncolnames(mtcars)[1] <- \"consumption\""},{"path":"data-cleaning.html","id":"variable-types","chapter":"3 Data Cleaning","heading":"3.2 Variable Types","text":"Variable types important tell R interpret \nvariable. example, never want R treat numeric variable \ncharacter one able use variable \ncalculations.Fortunately, import dataset R-Studio automatically recognizes\ntype variable. However, sometimes inputation\nmistakes software overcome . refer cases\nnumber comma instead dot decimals, \ncell comments numeric variable, etc. cases R\nread variable character variable. function str(),\nallows us recognize possible problem listing variables\ntypes. see missclassified variable can immediately\nexplore via summary() /table(), find error, correct\n. Finally need tell R right type \nvariable, don’t get error, means fixed (look \nexample ).Remember data types R apposite function \nconverts data type. list used: .character(),\n.numeric(), .factor(), .data.frame(), etc… ","code":"\n# creating an age vector\nage <- c(22, 12, \"didn't answer\", 30)\n\n# we need it numeric, but it is character (chr)\nstr(age)\n\n# we find the problem: there is a comment!\ntable(age)\n\n# we change the value where there was the comment and input a missing value\nage[age==\"didn't answer\"] <- NA\n\n# we change the vector type to numeric\nage <- as.numeric(age)"},{"path":"data-cleaning.html","id":"factor-variables","chapter":"3 Data Cleaning","heading":"3.2.1 Factor variables","text":"cases (especially plotting data), important \nfactor variables recognized R treated accordingly. Factor\nvariables dichotomous (female male, 1 0, yes )\ncategorical (hair color, rooftop type, vehicle type, etc…).dummy variable variable indicates whether observation \nparticular characteristic. dummy variable can assume values\n0 1, 0 indicates absence property, 1 indicates\npresence . values 0/1 can seen /yes \n/.following code creates character variable composed ten random\n“yes” “”. , , turns variable factor variable \nspecifies levels. comparing summary variables, see\nimmediately differently R behaves dealing factor\ncompared character variable. relevel() function reorders\nvariable want. ","code":"\nyesno <- sample(c(\"yes\",\"no\"), size=10, replace=TRUE)\nstr(yesno)\nsummary(yesno)\n\n# transform the variable in factor\nyesnofac = factor(yesno, levels=c(\"yes\",\"no\"))\nstr(yesnofac)\nsummary(yesnofac)\nlevels(yesnofac)\n\n# relevel the variable\nrelevel(yesnofac, ref=\"no\")\nas.numeric(yesnofac)"},{"path":"data-cleaning.html","id":"dates-and-times","chapter":"3 Data Cleaning","heading":"3.2.2 Dates and times","text":"R can calculations using dates times . may sound\nwired, sometimes need subtract 180 days date variable \nwithout functionality quite complex task. Dates,\nhowever, automatically recognized software \nneed external package called lubridate()(Yarberry 2021).Following small example. functions today() now() retrieve\ncurrent date two different levels detail. function ymd()\ntransforms numeric character variables containing year, month, \nday date variable. Note lubridate() package long\nlist functions according format date needed (check \nLubridate cheatsheet).   ","code":"\nlibrary(lubridate)\n# today's date\ntoday()\nnow()\n\n# creating a numeric vector of dates with year, month and day\nx <- c(20220321, 20220322, 20220320)\nstr(x)\n\n# transforming it in a date type vector \ny <- ymd(x)\nstr(y)\n\n# subtracting 180 days to y and x\nx-180\ny-180"},{"path":"data-cleaning.html","id":"row-names","chapter":"3 Data Cleaning","heading":"3.3 Row Names","text":"Row names often important. cases \ndataset column representing id observation. However,\ncomes Multivariate Analysis methods, need \nid observation one column, row name. \nway R able plot data automatically referring \nname row.code gives us way copy first column dataset \nrow names delete column , order \nfully numeric dataset. use dataset created \nDataset Exploration.","code":"\n# creatng the dataset\npeople <- data.frame(name = c(\"Mary\", \"Mike\", \"Greg\"),\n                     age = c(44, 52, 46),\n                     IQ = c(160, 95, 110))\n\n# inputting rownames from the column \"name\"\nrownames(people) <- people$name\n\n# deleting the clumn \"name\"\npeople <- people[,-1]"},{"path":"advanced-data-manipulation-and-plotting.html","id":"advanced-data-manipulation-and-plotting","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4 Advanced Data Manipulation and Plotting","text":"time become pro! Well, becoming “pro” R doesn’t strictly\ncomplex things, become crazy freaks \nleaving laptop squeaking like lab rats. Becoming pro means\nable things another way. Sometimes easier way, \ntimes complex way allows us reach higher level \noutput.Now asking: “easier ways, learn \ncomplex part beginning?” Well, Michael Jordan answer :\n“Get fundamentals level everything \nrise.” , basically, need grammar order write sentence.Now, put “pro” hat !   ","code":""},{"path":"advanced-data-manipulation-and-plotting.html","id":"ifelse","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.1 Ifelse","text":"ifelse() function allows us give R multiple inputs according \none conditions. function used lot cleaning data\nwant create new variables.example, want generate variable equal 1 car\nconsumes less 20, equal 0 cases. can\ncreate variable one condition. want \ncreate new variable two levels. Finally, may want \ncorrect values according conditions.Within ifelse() function required specify \ncondition(s), comma, value give condition(s) true,\ncomma, value give condition(s) false.case categorical variable multiple\ncategories (.e. color car), /want create dummies\nmultiple categorical variables (.e. variables: red,\ngreen, blue, etc), suggest use package\nfastDummies(Kaplan 2022). Note argument select_columns\ncan specify multiple columns using c(). argument\nremove_selected_columns removes original categorical variable\ntransformation.   ","code":"\n# creating a new var conditionally\nmtcars$newvar <- ifelse(mtcars$mpg < 20, 1, 0)\n\n# creating a new var with more conditions\nmtcars$newvar <- ifelse(mtcars$mpg > 20 & mtcars$cyl < 4, 1, 0)\n\n# creating a new var with more and more conditions in nested way\nmtcars$newvar <- ifelse(mtcars$mpg > 20, 100,\n                        ifelse(mtcars$mpg < 17, 10 ,\n                                ifelse(mtcars$cyl == 6, NA , mtcars$mpg )))\n# correctin a variable\nmtcars$mpg <- ifelse(mtcars$cyl>5, NA, mtcars$mpg)\n#this is the same as\nmtcars$mpg[mtcars$cyl>5] <- NA\nlibrary(fastDummies)\ndummy_cols(mtcars, select_columns = \"cyl\", remove_selected_columns = TRUE)"},{"path":"advanced-data-manipulation-and-plotting.html","id":"the-apply-family","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.2 The Apply family","text":"apply() function great facilitator. applies function \nchoice whole dataset row, column\n(Sovansky Winter 2022). means , using function, can\ncompute mean (complex calculus) variables \ndata! can create summary data.Within apply() function impute data want \nmanipulate, comma, want apply function row-wise (1) \ncolumn-wise (2), comma, function want apply data.   ","code":"\n############## apply\n# calculate the mean per each column\napply(mtcars, 2, mean)\n\n# filter out rows where the mean of the row is lower than 20\nmtcars[-apply(mtcars, 1, mean) < 20,]\n\n# create statistical summary\nStat<-cbind(\n  apply(mtcars[,c(\"mpg\", \"cyl\", \"disp\", \"hp\",\"drat\")],2,mean), \n  apply(mtcars[,c(\"mpg\", \"cyl\", \"disp\", \"hp\",\"drat\")],2,sd),\n  apply(mtcars[mtcars$am==0,c(\"mpg\", \"cyl\", \"disp\", \"hp\",\"drat\")],2,mean),\n  apply(mtcars[mtcars$am==1,c(\"mpg\", \"cyl\", \"disp\", \"hp\",\"drat\")],2,mean)\n)\ncolnames(Stat)<- c(\"Mean\", \"s.d.\", \"Mean Automatic\", \"Mean Manual\")\nround(Stat,2)"},{"path":"advanced-data-manipulation-and-plotting.html","id":"dplyr","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.3 Dplyr","text":"Following powerful things, dplyr package\n(Wickham Bryan 2022). Well, one favorite packages R. fact,\ndplyr allows us almost seen now \neasier , sometimes, intuitive way.Dplyr grammar data manipulation, providing consistent set \nverbs help us solve common data manipulation challenges.\nBasically, dplyr allows us execute multiple functions cascade. \nusing symbol %>% end line, tell R code\ncontinues another function (fact R gives us indent \nfollowing line)(Wickham Bryan 2022).code allows us see glance big potential \npackage. First filter Star Wars characters skin color;\nsecondly compute Body Mass Index (bmi) characters, \nselect columns original data; finally \nspecies, compute number characters average mass, \nfilter groups one character average\nmass 50., using dplyr, one command, can ask R group data \ncategorical variable (group_by()), subset data according \nconditions (filter()), select variables retrieved\n(select()), manipulate current variables create new ones\n(mutate()), summarize data (summarize()), etc… course, \ncan alternatively done using indexing system \nmultiple functions, live choice.   ","code":"\n# loading the package and data\nlibrary(dplyr)\ndata(starwars)\n\n# subsetting by skin color\nstarwars %>%\n        filter(skin_color==\"light\")\n\n# creating a new variable and subsetting some columns\nstarwars %>%\n        mutate(bmi = mass/((height/100)^2)) %>%\n        select(name:mass, bmi)\n\n# creating a new dataset, summarizing the data grouped by species and filtering\n# the most interesting information\nprova <- starwars %>%\n        group_by(species) %>%\n        summarise(n = n(), \n                  mass = mean(mass, na.rm = TRUE)) %>%\n        filter(n > 1, \n               mass > 50)"},{"path":"advanced-data-manipulation-and-plotting.html","id":"merging-datasets","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.4 Merging datasets","text":"R can load multiple dataset data formats memory, \nuse together. However, sometimes want data \ndifferent sources within object. entails merging datasets\ntogether. may want outer join, left join, cross join. \noften, may sure want! suggestion first\nclear mind reading following options:Outer join merges two datasets specified variable \nkeeps observations (= TRUE).Outer join merges two datasets specified variable \nkeeps observations (= TRUE).Left outer merges observations object \\(y\\) \\(x\\) \ncorresponding \\(id\\) variable (corresponding means \nvariable name type).Left outer merges observations object \\(y\\) \\(x\\) \ncorresponding \\(id\\) variable (corresponding means \nvariable name type).Right outer merges observations \\(x\\) \\(y\\) \ncorresponding \\(id\\) variable (corresponding means \nvariable name type).Right outer merges observations \\(x\\) \\(y\\) \ncorresponding \\(id\\) variable (corresponding means \nvariable name type).Cross cartesian product one kind join row \none dataset joined . dataset size\n\\(m\\) join dataset size \\(n\\), get\ndataset \\(m*n\\) number rows.Cross cartesian product one kind join row \none dataset joined . dataset size\n\\(m\\) join dataset size \\(n\\), get\ndataset \\(m*n\\) number rows.Within merge() function asked specify dataset \\(x\\),\ncomma dataset $y$, comma, name \\(id\\) variable, comma,\nadditional arguments depending merge interested .   ","code":"\n#Outer join: \nmerge(x = df1, y = df2, by = \"CustomerId\", all = TRUE)\n\n#Left outer: \nmerge(x = df1, y = df2, by = \"CustomerId\", all.x = TRUE)\n\n#Right outer: \nmerge(x = df1, y = df2, by = \"CustomerId\", all.y = TRUE)\n\n#Cross join: \nmerge(x = df1, y = df2, by = NULL)"},{"path":"advanced-data-manipulation-and-plotting.html","id":"melting-vs-transposing","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.5 Melting vs Transposing","text":"wrote previous chapter important rule , \ndataset, row observation column \nrepresent variable. Now time break rule! fact,\nsometimes need different shape dataset, usually want \nplot multidimensional data.Melting data sounds strange, however want \nwide dataset (lot variables) want transform \n“long” dataset (variables), retaining \ninformation.Please remember: melting transposing! Transposing (t()\nfunction) useful data columns need \nrearrange rows, melting (melt()) creates “long” dataset\nvariables column. Note melt() function\nbelongs package reshape2(Anderson 2022).code generates dataset, transposes , melts .\nmelt() function requires us specify data want melt,\ncomma, name variable(s) want retain . \nsee following chapter use melted data.   ","code":"\n# creating data\npeople <- data.frame(name = c(\"Mary\", \"Mike\", \"Greg\"),\n                     surname = c(\"Wilson\", \"Jones\", \"Smith\"),\n                     age = c(44, 52, 46),\n                     IQ = c(160, 95, 110))\n\n# transpose\nt(people)\n\n# melt\nlibrary(reshape2)\nmelt(people, id = \"name\")\n\n# for two id variables\nmelt(people, id = c(\"name\", \"surname\"))"},{"path":"advanced-data-manipulation-and-plotting.html","id":"ggplot2","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.6 Ggplot2","text":"Ggplot2 one famous packages R, probably also one\nused. allows us draw almost beautiful graphs\nR known worldwide (Wickham 2016; Chang 2018). main\nadvantage ggplot2 deep underlying grammar, lets us\ncreate simple, expressive descriptive code plotting. Plots can\nbuilt iteratively, edited second moment.ggplot() function tells R generate Cartesian axes \nplace graph, yet graph yet. Within line\nusually specify dataset ggplot take data,\nvariables corresponding Cartesian axes (using aes()).\nspecified ggplot() function can put + sign\nexpress intention put additional layer top \nCartesian space using dedicated functions: geom_point() \nscatter plot, geom_smooth() regression line, geom_bar() \ngeom_col() bar chart, just mention (\ndescription meaning mentioned graph refer \nData visualization chapter).can add layers graph, personalize \nthings want. code generates two types regression lines\n(details [Linear Regression]), divides data plots\naccording variable (facets_grid()). can see, tend \nassign plot object, unless strictly necessary. \nway working, free prefer.\nFigure 4.1: top-left clockwise: Regression line GAM; Linear regression line; Facets.\nFollowing example application need melting data \nscope plotting multidimensional phenomenons. case, \ndataset students want plot scores, want\nable clearly see trend one ? solution \npaired (specifying position = 'dodge') bar chart .\nFigure 4.2: Paired barchart.\nable personalize almost everything like using ggplot2.\nGgplot2 Cheatsheet gives us important options. strongly suggest look\nalso R Graph Gallery \nincludes advanced graphs done using ggplot2 similar\npackages, provides code replicating one .\nFigure 4.3: top-left clockwise: Labeled scatterplot; Violin plot.\nFinally, want show outliers \ndata, need plot main trend excluding . \ndifferent versions ggplot code put limit y axes (\ndone x axes using xlim() instead ylim()).\nTry code see difference.   ","code":"\nlibrary(ggplot2)\n# No Plot Yet!\nggplot(data=mtcars, aes(x=mpg, y=hp))\n\n# First scatterplot\nggplot(data=mtcars, aes(x=mpg, y=hp)) + \n  geom_point()\n\n# we can assign our graph to an object\ng <- ggplot(data=mtcars, aes(x=mpg, y=hp)) + \n  geom_point()\nlibrary(ggplot2)\n# Adding More Layers: regression line\nggplot(data=mtcars, aes(x=mpg, y=hp)) + \n  geom_point() + \n  geom_smooth() # Generalized additive models\n\nggplot(data=mtcars, aes(x=mpg, y=hp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") #linear regression\n\n# Adding More Layers: Facets\nggplot(data=mtcars, aes(x=mpg, y=hp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_grid(. ~ cyl)\n# creating data\npeople <- data.frame(name = c(\"Mary\", \"Mike\", \"Greg\"),\n                     surname = c(\"Wilson\", \"Jones\", \"Smith\"),\n                     age = c(44, 52, 46),\n                     IQ = c(160, 95, 110))\n\n# melting data\nlibrary(reshape2)\nmelted_people <- melt(people, id = c(\"name\", \"surname\"))\n\n# plotting\nggplot(data = melted_people, aes(x = name, y = value, fill = variable)) +\n  geom_col(position = 'dodge')\n# labeled points\nggplot(mtcars, aes(x = hp, y = mpg, size = wt)) +\n  geom_point(aes(color = as.factor(cyl)), alpha=.7) +\n  geom_text(aes(label = row.names(mtcars)), size = 3, nudge_y = -.7) +\n  theme_bw(base_family = \"Times\")\n\n# Violin plot\ndata(\"InsectSprays\")\nggplot(InsectSprays, aes(spray, count, fill=spray)) +\n  geom_violin(colour=\"black\") +\n  xlab(\"Type of spray\") +\n  ylab(\"Insect count\") +\n  theme_minimal()\n# creating data\ntestdat <- data.frame(x = 1:100, \n                      y = rnorm(100))\ntestdat[20,2] <- 100  # Outlier!\n\n# normal plot\nggplot(testdat, aes(x = x, y = y)) + \n        geom_line()\n\n# excluding the values outside the ylim range\nggplot(testdat, aes(x = x, y = y)) + \n        geom_line() + \n        ylim(-3, 3)\n\n# zooming in a portion of the graph\nggplot(testdat, aes(x = x, y = y)) + \n        geom_line() + \n        coord_cartesian(ylim = c(-3, 3))"},{"path":"advanced-data-manipulation-and-plotting.html","id":"exercises-1","chapter":"4 Advanced Data Manipulation and Plotting","heading":"4.7 Exercises","text":"Exercises:R playground -\nAdvanced Data Manipulation\n","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"5 Exploratory Data Analysis","heading":"5 Exploratory Data Analysis","text":"point forward, stop focusing R, finally\nstart acting statisticians. perform various level data\nanalysis applying different methodologies. course, R \ntool, facilitator, center \nstage. fully concentrate statistical methods, \napplication interpretation.Following six steps data analysis. section explores \nsteps 4 6, given first two scope \nbook third treated previous section.\nFigure 5.1: six steps data analysis.\nExploratory data analysis usually performed first\ncontact clean new data. course, first\nimpression always right one, give proper\nchance can save us lot work headaches later . \nphase perform summary statistics (central tendency \nvariability measures), basic plotting, inequality measures, \ncorrelations. Exploratory analysis differs dataset\nexploration performed Chapter 1, latter focuses \nshape data (type, dimensions, etc) former focuses\ncontent.   ","code":""},{"path":"exploratory-data-analysis.html","id":"central-tendency-measures","chapter":"5 Exploratory Data Analysis","heading":"5.1 Central Tendency Measures","text":"measure central tendency single value attempts \ndescribe set data identifying central position within \nset data. mean, median mode valid measures central\ntendency, different conditions, measures central\ntendency become appropriate use others (Lane et al. 2003; Manikandan 2011; Laerd Statistics, n.d.).arithmetic mean (average) popular \nwell known measure central tendency. mean equal sum \nvalues divided number observations (Equation\n(5.1)).\\[\\begin{equation}\n\\bar x = \\frac{\\sum{x}}{n}\n\\tag{5.1}\n\\end{equation}\\]One important properties minimizes error \nprediction value data set, given big enough sample size\n(details property discussed [Normal\nDistribution] chapter). Another important property mean \nincludes every value data set part calculation.\nimplies, also, sensitive presence outliers \nskewed distributions (.e., frequency distribution data \nskewed). cases median better indicator \ncentral location.geometric mean defined \\(n^{th}\\) root product \n\\(n\\) values (Equation (5.2)).\\[\\begin{equation}\n\\bar x_{g} = \\sqrt[n^{th}]{x_1*x_2*...*x_n}\n\\tag{5.2}\n\\end{equation}\\]geometric mean used values considered lower\nequal zero. often used set numbers whose values \nmeant multiplied together exponential nature, \nset growth figures: values human population, interest rates\nfinancial investment time.median middle score set data \narranged order magnitude. means, also, median \nequal value corresponding 50th percentile \ndistribution. number observations even, median \nsimple average middle two numbers.mode frequent score data set. histogram\nrepresents highest point (see later \nchapter). can, therefore, sometimes consider mode \npopular option. implies mode always unique\nvalue, fact (particularly continuous data) can bi-modal,\ntri-modal, multi-modal distributions. Moreover, mode \nprovide us good measure central tendency \ncommon mark far away rest data.middle value less used measure. represents value\nexactly center minimum maximum values\n(Equation (5.3)), regardless distribution data Thus \nsensitive outliers.\\[\\begin{equation}\n\\frac{max-min}{2}\n\\tag{5.3}\n\\end{equation}\\]function summary() gives us brief overview minimum,\nmaximum, first third quantile, median mean variable, \nsingle variable. course measures can also computed\nindependently using dedicated functions.following code computes summary statistics, arithmetic mean,\ngeometric mean (dedicated function ), median, mode\n(function DescTools allows us retrieve also multi-modal\nvalues)(Signorelli 2021), middle value. options \nfunctions , please, look help.   ","code":"\n# to do statistical summaries of the whole dataset or of a single variable\nsummary(mtcars)\nsummary(mtcars$cyl)\n\n# arithmetic mean\nmean(mtcars$mpg)\n\n# geometric mean\nexp(mean(log(mtcars$mpg)))  \n\n# median\nmedian(mtcars$mpg)\n\n# mode\nlibrary(DescTools)\nMode(mtcars$mpg)\n\n#middle value\n(max(mtcars$mpg)-min(mtcars$mpg))/2"},{"path":"exploratory-data-analysis.html","id":"variability-measures","chapter":"5 Exploratory Data Analysis","heading":"5.2 Variability Measures","text":"terms variability, spread, dispersion synonyms, refer \ndisperse distribution . measures complete information\ngiven central tendency measures, order better understand \ndata analyzing. example gives us idea unequal\ndistributions may central tendency measures, thus,\nconsidering lead mistaken evaluations.Central tendency measures samples b.\nFigure 5.2: left: Histograms samples b.\nrange simplest measure variability calculate, \none probably encountered many times life. range \nmaximum value minus minimum value (Equation (5.4)).\\[\\begin{equation}\nrange=max-min\n\\tag{5.4}\n\\end{equation}\\]interquartile range (IQR) range central 50% \nvalues distribution (Equation (5.5)).\\[\\begin{equation}\nIQR=75^{th} percentile - 25^{th} percentile\n\\tag{5.5}\n\\end{equation}\\]variability can also defined terms close values \ndistribution middle distribution. Using mean\nmeasure referencing middle distribution, \nvariance defined average squared difference scores\nmean (Equation (5.6)).\\[\\begin{equation}\n\\sigma^2=\\frac{\\sum{(X-\\bar x)^2}}{N}\n\\tag{5.6}\n\\end{equation}\\]standard deviation simply square root variance\n(Equation (5.7)). especially useful measure variability \ndistribution normal approximately normal (see [Normal\nDistribution]) proportion distribution within given\nnumber standard deviations mean can calculated.\\[\\begin{equation}\n\\sigma=\\sqrt{\\sigma^2}\n\\tag{5.7}\n\\end{equation}\\]coefficient variation (CV) represents ratio \nstandard deviation mean (Equation (5.8)), useful\nstatistic comparing degree variation data relative \ndifferent unit measures. fact, CV variability\nmeasure (mentioned ) standardized, thus \ndepend unit measure.\\[\\begin{equation}\nCV=\\frac{\\sigma}{\\bar{x}}\n\\tag{5.8}\n\\end{equation}\\]following code computes mentioned variability measures \nconsumption variable mtcars dataset. last lines \ncan find also code compile frequency table function\ntable().simple code allows create dataframe, also \nway create summary table. data.frame() function \nfilled column wise, specifying column/variable name \ncontent. can play order create personalized table.\nstargazer() function, instead presents statistics \nsummary() function, cooler style (Torres-Reyna 2014; Hlavac 2022).   ","code":"\n# range\nmax(mtcars$mpg) - min(mtcars$mpg)\n\n# quantile distribution\nquantile(mtcars$mpg)\n\n# interquantile range\nquantile(mtcars$mpg, probs = .75) - quantile(mtcars$mpg, probs = .25)\n\n# variance\nvar(mtcars$mpg)\n\n# standard deviation\nsd(mtcars$mpg)\n\n# coefficent of variation\nsd(mtcars$mpg)/mean(mtcars$mpg)\n\n# frequency tables\ntable(mtcars$cyl)\ntable(mtcars$cyl, mtcars$am)\n# create a table storing your values\ndata.frame(obs = c(\"Miles per Gallon\", \"Number of Cylinders\"),\n           mean = c(mean(mtcars$mpg), mean(mtcars$cyl)),\n           median = c(median(mtcars$mpg), median(mtcars$cyl)),\n           sd = c(sd(mtcars$mpg), sd(mtcars$cyl))\n           )\n\n# create a table storing basic summary statistics\nlibrary(stargazer)\nstargazer(mtcars, type = \"text\", digits=2)"},{"path":"exploratory-data-analysis.html","id":"inequality-measures","chapter":"5 Exploratory Data Analysis","heading":"5.3 Inequality Measures","text":"Another set analysis methods related variability \ninequality measures. Inequality can defined : “scalar numerical\nrepresentation interpersonal differences income (example)\nwithin given population.” use word “scalar” implies \ndifferent features inequality compressed single\nnumber single point scale.inequality ? strict statistical rule says must \nquantitative variable transferable among population interest,\nincome, apples, cars, etc… However, economics, inequality\nmeasurements used also items, like carbon emissions.Gini coefficient, Gini Index, widely used measure\ninequality policy-related discussions. measure \nstatistical dispersion prominently used measure inequality\nincome distribution inequality wealth distribution. \ndefined ratio values 0 1. Thus, low Gini\ncoefficient indicates equal income wealth distribution, \nhigh Gini coefficient indicates unequal distribution. Zero\ncorresponds perfect equality (everyone exactly \nincome) 1 corresponds perfect inequality (one person \nincome, everyone else zero income). Note \nGini coefficient requires one negative net income \nwealth.geometrical terms, Gini coefficient can thought \nratio area lies line perfect equality (\ndiagonal) Lorenz curve total area line \nequality (see Figure 5.3). formula used compute Gini\ncoefficient:\\[\\begin{equation}\nG(x)=1-\\frac{2}{N-1}\\sum^{N-1}_{=1}{q_i}\n\\tag{5.9}\n\\end{equation}\\]N population size, q cumulative relative\nincome.\nFigure 5.3: Graphical representation Gini index.\nRun code calculate Gini coefficient x \ny objects. can either use function package DescTools\n(Signorelli 2021), function package labstatR\n(Iacus Masarotto 2020) (remember R case-sensitive!). expect \nvalue Gini?Try understand function rep() works using help.mentioned, Gini Index nowadays recognized standard,\nnevertheless limitations. fact, Gini sensitive \nchanges middle distribution, tails. \neconomics, study inequality, often interested \ntails behavior (namely top bottom 10% distribution).Palma Ratio particular specification within family \ninequality measures known inter-decile ratios (Cobham, Schlögl, Sumner 2016). \nspecifically, ratio national income shares top 10%\nhouseholds bottom 40%. , thus, tells us many times \nincome top 10% population higher \nbottom 40% (Equation (5.10)).\\[\\begin{equation}\nPalma = \\frac {top10}{bottom40}\n\\tag{5.10}\n\\end{equation}\\]code computes Palma Ratio \\(x\\) \\(z\\)\ndistributions created previously. done manually, \ncomputing top 10% income bottom 40% income \ncumulative frequencies computed function gini() package\nlabstatR (Iacus Masarotto 2020). Confront Gini Index Palma Ratio, \nfind different results? ?   ","code":"\n# generate vector (of incomes)\nx <- c(541, 1463, 2445, 3438, 4437, 5401, 6392, 8304, 11904, 22261)\ny <- c(rep(1000, 10))\nz <- c(541, 1463, 2445, 3438, 3438, 3438, 3438, 3438, 11904, 22261)\n\n# compute Gini coefficient\nlibrary(DescTools)\nGini(x)\nGini(y)\nGini(z)\n# or\nlibrary(labstatR)\ngini(x)\nlibrary(labstatR)\n# extracting the cumulative frequencies from the function gini by labstatR\nq <- gini(x)$Q\n\n# computing the Palma Ratio on the cumulative frequencies\n(quantile(q, probs = 1, type=3)-quantile(q, probs = .9, type=3))/\n  quantile(q, probs = .4, type=3)\n\n# extracting the cumulative frequencies from the function gini by labstatR\nq2 <- gini(y)$Q\n\n# computing the Palma Ratio on the cumulative frequencies\n(quantile(q2, probs = 1, type=3)-quantile(q2, probs = .9, type=3))/\n  quantile(q2, probs = .4, type=3)\n\n# extracting the cumulative frequencies from the function gini by labstatR\nq3 <- gini(z)$Q\n\n# computing the Palma Ratio on the cumulative frequencies\n(quantile(q3, probs = 1, type=3)-quantile(q3, probs = .9, type=3))/\n  quantile(q3, probs = .4, type=3)"},{"path":"exploratory-data-analysis.html","id":"data-visualization","chapter":"5 Exploratory Data Analysis","heading":"5.4 Data visualization","text":"Visualizing data another way explore data, better, \ncomplement quantitative exploratory analysis carried .\nfact, saw example Figure 5.4, simple histogram can\ntell us whether arithmetic mean can give us realistic\nrepresentation data, analysis needed. \nsection see beautiful graphs R recognized\nworldwide (invite explore Ggplot2), fast \ndirty graphs potential , least exploratory\nanalysis phase.histogram plot allows inspection data \nunderlying distribution (e.g., normal distribution), outliers, skewness,\netc. bars histogram equally spaced bars, height \nbin reflects frequency value distribution.box plot, also called “box whisker plot”, way show\nspread centers data set. box plot way show five\nnumber summary chart. main part chart (“box”) shows\nmiddle portion data : interquartile range. \nends box, ” find first quartile (25% mark) \nthird quartile (75% mark). far bottom chart \nminimum (smallest number set) far top maximum\n(largest number set). Finally, median\n(mean!) represented \nhorizontal bar center box.scatter plot bi-dimensional plot dots represent\nvalues two different numeric variables Cartesian space. \nposition dot indicates individual data point respect\ntwo variables selected. Scatter plots used observe\nrelationships two numeric variables (deepen use\nlater chapter).\nFigure 5.4: Plot examples. top-left clowise: Histogram; Density plot; Scatterplot; Boxplot.\ncode provided part may look bit complex \nseen far. first draw histogram density plot (\nlinear representation distribution). , draw \nscatter plots, add “stilish” (still basic) arguments \nlinear regression line. Note formula used abline()\nfunction something explore better Linear\nRegression, now notice y ~ x + z + e stands \ny = x + z + e classic linear expression algebra.5 \nsubset consumption (mpg) data automatic manual cars \nmtcars dataset order study differences among box\nplot. can see manual cars (1 plot) present higher miles\nper gallon (thus lower consumption) automatic cars (0 plot),\ntwo distributions overlaps tails. following\ngraph, appreciate continuous variable (horse power) \ndistributed among cars grouped discrete variable (cylinders). \nway, studying relationship variable mixed\ntypes. Note , order tell R variable discrete, \nused function .factor() (details Factor\nvariables). Moreover, specified \\(x\\) \\(y\\) axes labels using\nxlab ylab arguments.Try code, personalize , check help supplementary\noptions.   ","code":"\n# histogram\nhist(mtcars$mpg)\n\n# density plot\nplot(density(mtcars$mpg), type = \"l\")\n\n# scatterplot of cyl vs hp\nplot(mtcars$cyl, mtcars$hp)\n\n# why do we get an error here?\nplot(mpg, hp)\n\n# stylish scatterplot\nplot(mtcars$hp,\n     mtcars$mpg,\n     # adding titles and axes names\n     ylab = \"Miles per Gallon\",\n     xlab = \"Horse Power\",\n     main = \"Is consumption related to Horse Power?\")\n\n# adding a regression line on the scatterplot\nabline(lm(mpg ~ hp, data = mtcars), col = \"blue\")\n\n# subset automatic cars consumption\nmpg.at <- mtcars$mpg[mtcars$am == 0]\n# and manual cars consumption\nmpg.mt <- mtcars$mpg[mtcars$am == 1]\n\n# boxplot\nboxplot(mpg.mt, mpg.at)\n\n# boxplot with a categorical variable\nplot(as.factor(mtcars$cyl), mtcars$hp, \n     # adding axes names\n     xlab=\"Cylinders\", ylab=\"Horse Power\")"},{"path":"exploratory-data-analysis.html","id":"scaling-data","chapter":"5 Exploratory Data Analysis","heading":"5.5 Scaling data","text":"Scaling, standardizing, data useful technical trick \nstatisticians economists use order better use data. \ncommon situations scaling required visualization,\ninterpretation, comparison.Log scaling typically used plotting regression analysis.\nway displaying numerical data wide range \nvalues compact way. fact, typically, log scaling needed,\nlargest numbers data hundreds even thousands times\nlarger smallest numbers. example, often exponential\ngrowth curves displayed log scale, otherwise \nincrease quickly fit within small graph allow complete\nanalysis variation. R function log().Ranking replaces value assumed unit, order\nnumber (rank) unit placed list according \nspecific indicator. two units assume value, \ngive average rank positions \ncase different values. transformation ranks purifies\nindicators unit measure, preserve \nrelative distance different units.basic form rank() function produces vector contains\nrank values vector evaluated \nlowest value rank 1 second-lowest value \nrank 2.Relative indices respect range (Min-Max) purifies \ndata unit measure features within \nspecific range (.e. \\(0, 1\\))(Equation (5.11)). Min-Max scaling important want\nretain distance data points.\\[\\begin{equation}\nr_{ij}=\\frac{x_{ij}-\\min_ix_{ij}}{\\max_ix_{ij}-\\min_ix_{ij}}\n\\tag{5.11}\n\\end{equation}\\]point z-score standardization change data \ncan described normal distribution (Equation (5.12)). Normal distribution, \nGaussian distribution, specific statistical distribution \nequal observations fall mean, mean \nmedian , observations closer mean\n(details see [Normal Distribution]).\\[\\begin{equation}\nz_{ij}=\\frac{x_{ij}-\\bar x_j}{\\sigma_j}\n\\tag{5.12}\n\\end{equation}\\]scale() function, default settings, calculate mean\nstandard deviation entire vector, normalize element\nvalues subtracting mean dividing standard\ndeviation (Equation (5.12)). resulting distribution mean\nequal 0 standard deviation equal 1.index numbers, value assumed unit divided \nreference value belonging distribution calculated \n(generally mean maximum)(Equation (5.13)). normalization allows\ndelete unit measure keep relative distance among\nunits. denominator maximum obtain values less\nequal 100.\\[\\begin{equation}\nI_{ij}=\\frac{x_{ij}}{x^*_{oj}}*100\n\\tag{5.13}\n\\end{equation}\\]percentage transformation value unit divided\nsum values (Equation (5.14)). sum normalized\nvalues equal 100.\\[\\begin{equation}\np_{ij}=\\frac{x_{ij}}{\\sum^n_{=1}x_{ij}}*100\n\\tag{5.14}\n\\end{equation}\\]   ","code":"\n# creating data\nx = c(5,100,4000,7,1000,350000,25000)\n\n# plotting data\nplot(x)\n\n# log scaling\nscaled_x <- log(x)\n\n# plotting scaled data\nplot(log(scaled_x))\n# creating data\nx = c(5,1,4,7,10,35,25)\n\n# scale ranking data\nscaled_x <- rank(x)\n# creating data\nx = c(5,1,4,7,10,35,25)\n\n# min-max scaling data\nscaled_x <- (x-min(x))/(max(x)-min(x))\n# creating data\nx = c(5,1,4,7,10,35,25)\n\n# z-score scaling data\nscaled_x <- scale(x)\nmean(scaled_x)\nsd(scaled_x)\n# creating data\nx = c(5,1,4,7,10,35,25)\n\n# indexing with mean\nscaled_x <- x/mean(x)\n\n# indexing with maximum\nscaled_x <- x/max(x)\n# creating data\nx = c(5,1,4,7,10,35,25)\n\n# percentage indexing\nscaled_x <- x/sum(x)"},{"path":"exploratory-data-analysis.html","id":"probability-sampling","chapter":"5 Exploratory Data Analysis","heading":"5.6 Probability Sampling","text":"Sampling allows statisticians draw conclusions whole \nexamining part. enables us estimate characteristics \npopulation directly observing portion entire population.\nResearchers interested sample , can \nlearned survey—information can applied \nentire population.Simple Random sampling powerful tool, can use random\nsubset data, generate random distributions precise\ncharacteristics. member population equal chance \nincluded sample. Also, combination members \npopulation equal chance composing sample. two\nproperties defines simple random sampling. Simple random\nsampling can done without replacement. sample \nreplacement means possibility sampled\nobservation may selected twice . Usually, simple random\nsampling approach conducted without replacement \nconvenient gives precise results.Simple random sampling easiest method sampling \ncommonly used. Advantages technique \nrequire additional information sampled unit, \ncontext (.e. geographic area, gender, age, etc…). Also, since\nsimple random sampling simple method theory behind \nwell established, standard formulas exist determine sample size,\nestimates , formulas easy use.hand, making use context information method\nmay sometimes result less efficient equally representing strata\npopulation, particularly smaller samples. Finally, \nplanning survey just sampling already collected data, simple\nrandom sampling may result expensive unfeasible method large\npopulations elements must identified labeled prior \nsampling. can also expensive personal interviewers required\nsince sample may geographically spread across population.can calculate probability given observation selected.\nSince know sample size (\\(n\\)) total population (\\(N\\)),\ncalculating probability included sample becomes \nsimple matter division (Equation (5.15)).\\[\\begin{equation}\np=\\frac{n}{N}*100\n\\tag{5.15}\n\\end{equation}\\]R use sample() function. function requires us \nspecify total population sampled (.e. vector 1 \n1 million), comma, sample size interested , comma, \nwant replacement happen (TRUE) (FALSE).order reproducible code (one main reasons\nusing R working tools), want able select \nrandom sample . may sound complex, . \nable select random sample today tomorrow, \nanalysis change slightly every time run code. order \nsolve problem, use set.seed() function \nbeginning work. fact setting seed, tell R set\nrandom number generator use. Please aware different\nversions Random Number Generator select different samples\ngiven seed. collaborate people, specify \nversion (see code ).code , run first line multiple times see\ndifferent samples drawn. instead, run set.seed line plus \nsampling, always sample. last line code,\ngives us example randomly select 4 rows mtcars\ndataset (32 rows) using combination indexing system \nsample() function. fact, square brackets : \nvector row numbers (generated sample() function), comma\nnothing (means want keep columns \ndataset). can run just sample function first see \nresulting vector (sample(1:32, 4, replace=FALSE)), run \nwhole line (samp_data <- mtcars[sample(1:32, 4, replace=FALSE),]). \ncode subsets mtcars lines randomly sampled.stratified sampling, population divided homogeneous,\nmutually exclusive groups called strata, independent samples\nselected stratum. need create strata? \nmany reasons, main one can make sampling\nstrategy efficient. fact, mentioned earlier need\nlarger sample get accurate estimation characteristic\nvaries greatly one unit characteristic\n. example, every person population \nsalary, sample one individual enough get precise\nestimate average salary.idea behind efficiency gain obtained \nstratification. create strata within units share similar\ncharacteristics (e.g., income) considerably different units\nstrata (e.g., occupation, type dwelling) \nneed small sample stratum get precise estimate total\nincome stratum. combine estimates get \nprecise estimate total income whole population. \nuse simple random sampling approach whole population without\nstratification, sample need larger total \nstratum samples get estimate total income level\nprecision.sampling methods mentioned section (others \nexist) can used sample within stratum. sampling method\nsize can vary one stratum another, since stratum\nbecomes independent population. simple random sampling used\nselect sample within stratum, sample design called\nstratified simple random sampling. population can stratified \nvariable available units sampling frame prior\nsampling (.e., age, gender, province residence, income, etc.).code applies Stratified Simple Random Sampling Star\nWars dataset. Using Dplyr package (Wickham Bryan 2022), able \nstratify data one variable (eye color), sample randomly 40%\navailable population stratum (function sample_frac()),\n2 observations per stratum (function sample_n()).   ","code":"\n# random sampling 4 numbers out of the series from 1 to 32\nsample(1:32, 4, replace=FALSE)\n\n# set seed allows us to reproduce the random operations we do locally\nset.seed(1234)\nsample(1:32, 4, replace=FALSE)\n\n# specifying the Random Number Generator version, allows everyone to have the same sample. \nset.seed(123, kind = \"Mersenne-Twister\", normal.kind =  \"Inversion\")\n\n# sampling the random observations selected\nset.seed(1234)\nsamp_data <- mtcars[sample(1:32, 4, replace=FALSE),]\n# loading the package and data\nlibrary(dplyr)\n\nstarwars %>%\n        # stratifying by eye color\n        group_by(gender) %>%\n        # setting the sample size\n        sample_frac(.4) %>%\n        ungroup\n\nstarwars %>%\n        # stratifying by eye color\n        group_by(gender) %>%\n        # setting the sample size\n        sample_n(2) %>%\n        ungroup"},{"path":"exploratory-data-analysis.html","id":"exercises-2","chapter":"5 Exploratory Data Analysis","heading":"5.7 Exercises","text":"R playground,\nExploratory data analysis","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"6 Hypothesis Testing","heading":"6 Hypothesis Testing","text":"Hypothesis testing vital process inferential statistics \ngoal use sample data draw conclusions entire\npopulation. hypothesis test evaluates two mutually exclusive\nstatements population determine statement best\nsupported sample data. two statements called null\nhypothesis alternative hypothesis.need test? mentioned, almost always statistics, \ndealing sample instead full population. huge\nbenefits working samples usually impossible \ncollect data entire population. However, trade \nworking manageable sample need account \nsampling error. sampling error gap sample\nstatistic population parameter. Unfortunately, value \npopulation parameter unknown usually unknowable.\nHypothesis tests provide rigorous statistical framework draw\nconclusions entire population based representative\nsample.example, given sample mean 350USD hypothetical\npopulation mean 300USD, can estimate much probability\npopulation mean 300USD true (p-value), thus \nsampling error, using T distribution (similar \nNormal Distribution, fattier tails). follow four\nsteps:First, define null (\\(\\bar i_n = \\bar i_N\\)) alternative\nhypotheses (\\(\\bar i_n \\neq \\bar i_N\\)). Much emphasis \nclassical statistics focuses testing single null\nhypothesis, H0: average income two groups\n. course, probably like discover \ndifference average income two groups.\nreasons become clear, construct null\nhypothesis corresponding difference.Next, construct test statistic summarizes strength \nevidence null hypothesis.compute p-value quantifies probability \nobtained comparable extreme value test\nstatistic \\(H_0\\) (Lee 2019).Finally, based predefined significance level want \nreach (alpha), decide whether reject \\(H_0\\)\npopulation mean 300.Hypothesis tests 100% accurate use random sample\ndraw conclusions entire populations. perform \nhypothesis test, two types errors related drawing \nincorrect conclusion.Type error: rejects null hypothesis true (false\npositive).Type error: rejects null hypothesis true (false\npositive).Type II error: fails reject null hypothesis false (\nfalse negative).Type II error: fails reject null hypothesis false (\nfalse negative).significance level, also known alpha, evidentiary\nstandard researcher sets study. defines \nstrongly sample evidence must contradict null hypothesis \ncan reject null hypothesis entire population. \nstrength evidence defined probability rejecting \nnull hypothesis true (Frost, n.d.). words, \nprobability say effect effect. \npractical terms, \\(alpha=1-pvalue\\). instance, p-value lower \n0.05 signifies 95% chances detecting difference two\nvalues. Higher significance levels require stronger sample evidence \nable reject null hypothesis. example, statistically\nsignificant 0.01 requires substantial evidences \n0.05 significance level.mentioned, significance level set researcher. However\nstandard values conventionally applied \ndifferent scientific context, follow conventions.Significal levels conventionFollowing see different type tests, implementation\nusing R. suggestion, working really small numbers (\np-values), suppress scientific notation order \nclearer sense coefficient scale. Use following code.  \n ","code":"\n#suppress scientific notation\noptions(scipen = 9999)"},{"path":"hypothesis-testing.html","id":"probability-distributions","chapter":"6 Hypothesis Testing","heading":"6.1 Probability Distributions","text":"probability distributions can classified discrete\nprobability distributions continuous probability distributions,\ndepending whether define probabilities associated discrete\nvariables continuous variables. discrete probability\ndistribution, possible value discrete random variable can \nassociated non-zero probability. Two famous\ndistributions associated categorical data Binomial \nPoisson Distributions.random variable continuous variable, probability\ndistribution called continuous probability distribution. \ndistribution differs discrete probability distribution \nprobability continuous random variable assume \nparticular value zero, function shows density \nvalues data called Probability Density Function, sometimes\nabbreviated pdf. Basically, function represents outline \nhistogram. probability random variable assumes value\n\\(\\) \\(b\\) equal area density function\nbounded \\(\\) \\(b\\) (Damodaran, n.d.).\nFigure 6.1: left: density distribution outline hisogram; probability random variable \\(X\\) higher equal 1.5.\nFigure 6.1, shaded area graph represents probability\nrandom variable \\(X\\) higher equal \\(1.5\\). \ncumulative probability, also called p-value (please, remember \ndefinition, key order understand remaining\nchapter). However, probability \\(X\\) exactly equal \n\\(1.5\\) zero.normal distribution, also known Gaussian distribution, \nimportant probability distribution statistics \nindependent, continuous, random variables. people recognize \nfamiliar bell-shaped curve statistical reports (Figure 6.1).normal distribution continuous probability distribution \nsymmetrical around mean, observations cluster around\ncentral peak, probabilities values away \nmean taper equally directions.Extreme values tails distribution similarly unlikely.\nnormal distribution symmetrical, symmetrical\ndistributions normal. important probability\ndistribution statistics accurately describes \ndistribution values many natural phenomena (Frost, n.d.). \nprobability distribution, parameters normal distribution\ndefine shape probabilities entirely. normal distribution \ntwo parameters, mean standard deviation.Despite different shapes, forms normal distribution \nfollowing characteristic properties.’re symmetric bell curves. Gaussian distribution \nskewed.’re symmetric bell curves. Gaussian distribution \nskewed.mean, median, mode equal.mean, median, mode equal.One half population less mean, half\ngreater mean.One half population less mean, half\ngreater mean.Empirical Rule allows determine proportion \nvalues fall within certain distances mean.Empirical Rule allows determine proportion \nvalues fall within certain distances mean.normally distributed data, standard deviation becomes\nparticularly valuable. fact, can use determine \nproportion values fall within specified number standard\ndeviations mean. example, normal distribution, 68% \nobservations fall within +/- 1 standard deviation mean\n(Figure 6.2). property part Empirical Rule, \ndescribes percentage data fall within specific numbers\nstandard deviations mean bell-shaped curves. \nreason, statistics, dealing large-enough dataset, \nnormal distribution assumed, even perfect.\nFigure 6.2: Empirical Rule.\nstandard normal distribution special case normal\ndistribution mean 0 standard deviation 1. \ndistribution also known Z-distribution. value \nstandard normal distribution known standard score, Z-score.\nstandard score represents number standard deviations \nmean specific observation falls. standard normal\ndistribution also result z-score standardization process\n(see Scaling data)(Frost, n.d.). , compare means \ndistributions two elements (weight apple pears), \nstandardizing can (Figure 6.3).\nFigure 6.3: comparison Apples Pears standardized.\n   ","code":""},{"path":"hypothesis-testing.html","id":"shapiro-wilk-test","chapter":"6 Hypothesis Testing","heading":"6.2 Shapiro-Wilk Test","text":"Shapiro-Wilk Test way tell random sample comes \nnormal distribution. test gives us W value p-value. null\nhypothesis population normally distributed. Thus, \nget significant p-value (<0.1), reject \\(H_0\\), \nconsider distribution normal. test limitations,\nimportantly bias sample size. larger sample,\nlikely get statistically significant result.important know random sample comes normal\ndistribution? , hypothesis hold, able\nuse characteristics t-distribution order make \ninference , instead using T-Test, use Wilcoxon\nRank Sum test Mann-Whitney test.order perform Shapiro-Wilk Test R, need use \nfollowing code.  \n ","code":"\n# Shapiro Test of normality\n# H0: the variable is normally distributed\n# Ha: the variable is not normally distributed\nshapiro.test(mtcars$mpg)\nhist(mtcars$mpg)"},{"path":"hypothesis-testing.html","id":"one-sample-t-test","chapter":"6 Hypothesis Testing","heading":"6.3 One-Sample T-Test","text":"One-Sample T-Test used compare mean one sample \ntheoretical/hypothetical mean (previous example). can apply\ntest according one three different type hypotheses\navailable, depending research question:null hypothesis (\\(H_0\\)) sample mean equal \nknown mean, alternative (\\(H_a\\)) \n(argument “two.sided”).null hypothesis (\\(H_0\\)) sample mean lower-equal\nknown mean, alternative (\\(H_a\\)) bigger\n(argument “greater”).null hypothesis (\\(H_0\\)) sample mean bigger-equal\nknown mean, alternative (\\(H_a\\)) smaller\n(argument “less”).code , given average consumption 20.1 miles per\ngallon, want test cars’ consumption average lower\n22 miles per gallon (\\(H_a\\)). fact true, \nretrieve significant p-value (<0.05), thus reject \\(H_0\\). using\npackage gginfernece (Bratsas, Foudouli, Koupidis 2020), can also plot test.\nFigure 6.4: Single Sample T-Test plotted gginference package.\n\nFigure 6.5: Single Sample T-Test output example.\noutput test R gives us many information. \nimportant : data using (mtcars$mpg), t\nstatistics (lower 2, significant p-value), \np-value, alternative hypothesis (\\(H_a\\)), 95% confidence interval\n(boundaries 95% possible averages), sample\nestimate mean.  \n ","code":"\nmean(mtcars$mpg)\n\n# H0: cars consumption is on average 22 mpg or higher\n# Ha: cars consumption is on average lower than 22 mpg\nt.test(mtcars$mpg, mu = 22, alternative = \"less\")\n#We reject H0. The average car consumption is significantly lower than 22 mpg.\n\n# plot the t-test \nlibrary(gginference)\nggttest(t.test(mtcars$mpg, mu = 22, alternative = \"less\"))"},{"path":"hypothesis-testing.html","id":"unpaired-two-sample-t-test","chapter":"6 Hypothesis Testing","heading":"6.4 Unpaired Two Sample T-Test","text":"One common tests statistics Unpaired Two Sample\nT-Test, used determine whether means two groups equal \n(Ziliak 2008). assumption test \ngroups sampled normal distributions equal variances. \nnull hypothesis (\\(H_0\\)) two means equal, \nalternative (\\(H_a\\)) .example, can compare average income two group \n100 people. First, explore descriptive statistics two\ngroups order idea. see difference \ntwo groups 0.69. Note, set seed \ngenerate data randomly Normal distribution using \nfunction rnorm().proceeding test, verify assumption \nnormality Shapiro-Wilk Test explained . However, \ndata normally distributed construction, skip \nstep.state hypotheses, run Unpaired Two Sample\nT-Test. R function One-Sample T-Test, \nspecifying different arguments. can plot test using \npackage gginfernece (Bratsas, Foudouli, Koupidis 2020).\nFigure 6.6: Single Sample T-Test plotted gginference package.\n\nFigure 6.7: Two sample T-test output example.\nOne-Sample T-Test, R gives us set information \ntest. important : data using, t\nstatistics (lower 2, significant p-value), \np-value, alternative hypothesis (\\(H_a\\)), 95% confidence interval\n(boundaries 95% possible difference \naverages two groups), sample estimates mean. Note\ncompute difference two averages “\nhand”, case needed.want extract coefficients, p-value \ninformation given t-test order build table, must\nassign test object first, extract data. \ncode run Unpaired Two Sample T-Test manual \nautomatic average consumption weight cars. \ncreate table average difference p-value \ntests. Note t-test formula written different way \ncode chunk , however meaning two expressions \n.  \n ","code":"\nset.seed(1234)\nincome_a <- rnorm(100, mean=20, sd=3)\nincome_b <- rnorm(100, mean=18.8, sd=1)\n\nboxplot(income_a, income_b)\nmean(income_a)-mean(income_b)\n# H0: the two groups have the same income on average\n# Ha: the two groups have the different income on average\nt.test(income_a, income_b) # reject H0\n# We accept Ha. There is a SIGNIFICANT mean income difference (0.67)\n# between the two groups.\n\nlibrary(gginference)\nggttest(t.test(income_a, income_b))\n# create a table\nt <- t.test(mpg ~ am, data=mtcars)\ns <- t.test(wt ~ am, data=mtcars)\n\ntaboft <- data.frame(\"coef\"=c(\"difference\", \"p-value\"),\n                     \"mpg\"= c(t$estimate[1]-t$estimate[2], t$p.value),\n                     \"wt\" = c(s$estimate[1]-s$estimate[2], s$p.value))\ntaboft"},{"path":"hypothesis-testing.html","id":"mann-whitney-u-test","chapter":"6 Hypothesis Testing","heading":"6.5 Mann Whitney U Test","text":"case variable want compare normally distributed\n(aka: Shapiro test gives us significant p-value), can run \nMann Whitney U Test, also known Wilcoxon Rank Sum test (Winter 2013).\nused test whether two samples likely derive \npopulation (.e., two populations shape).\nscholars interpret test comparing medians \ntwo populations. Recall Unpaired Two Sample T-Test compares \nmeans independent groups. contrast, research hypotheses\ntest stated follows:\\(H_0\\): two populations equal versus\\(H_a\\): two populations equal.procedure test involves pooling observations \ntwo samples one combined sample, keeping track sample \nobservation comes , ranking lowest highest.order run , thing need change name \nformula test, written code .  \n ","code":"\nwilcox.test(mpg.at, mpg.mt)"},{"path":"hypothesis-testing.html","id":"paired-sample-t-test","chapter":"6 Hypothesis Testing","heading":"6.6 Paired Sample T-Test","text":"Paired Sample T-Test statistical procedure used determine\nwhether mean difference two sets observations zero.\nCommon applications Paired Sample T-Test include case-control\nstudies repeated-measures designs (time series). fact, \nsubject entity must measured twice, resulting pairs \nobservations. example, suppose interested evaluating\neffectiveness company training program. measure \nperformance sample employees completing \nprogram, analyze differences using Paired Sample T-Test see\nimpact.code generates dataset values. \ncompute difference two measures run \nShapiro-Wilk Test order verify normality data\ndistribution. Finally, can run proper Paired Sample T-Test \ndata (Paired Mann Whitney U Test, data normally\ndistributed).   ","code":"\ndf <- data.frame(\"before\" = c(200.1, 190.9, 192.7, 213, 241.4, 196.9, \n                              172.2, 185.5, 205.2, 193.7),\n                 \"after\" = c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, \n                             383.9, 392.3, 352.2))\n\n# compute the difference\ndf$difference <- df$before - df$after\n# H0: the variables are normally distributed\nshapiro.test(df$difference) # accept H0\nhist(df$difference)\n\nt.test(df$before, df$after, paired = TRUE)"},{"path":"hypothesis-testing.html","id":"exercises-3","chapter":"6 Hypothesis Testing","heading":"6.7 Exercises","text":"","code":""},{"path":"bivariate-analysis.html","id":"bivariate-analysis","chapter":"7 Bivariate Analysis","heading":"7 Bivariate Analysis","text":"   ","code":""},{"path":"bivariate-analysis.html","id":"correlation","chapter":"7 Bivariate Analysis","heading":"7.1 Correlation","text":"now point imaginary boat towards Relationship Islands.\nsee one thing vary, perceive changing regard, \nsun setting, price goods increasing, alternation \ngreen red lights intersection. Therefore, two things\ncovary two possibilities. One change first\nconcomitant change second, change child’s\nage covaries height. older, taller. higher\nmagnitudes one thing occur along higher magnitudes another\nlower magnitudes also co-occur, things vary\ntogether positively, denote situation positive\ncovariation positive correlation. second possibility \ntwo things vary inversely oppositely. , higher magnitudes\none thing go along lower magnitudes vice\nversa. , denote situation negative covariation \nnegative correlation. seems clear enough, order \nsystematic correlation definition needed.start concept covariance, represents \ndirection linear relationship two variables. direction\nmean variables directly proportional inversely\nproportional . Thus, increasing value one\nvariable positive negative impact value \nvariable. values covariance can number \ntwo opposite infinities. ’s important mention covariance\nmeasures direction relationship two variables\nmagnitude, correlation used.probability theory statistics, correlation, also called\ncorrelation coefficient, indicates strength direction \nlinear relationship two random variables (Davis 2021; Madhavan 2019; Wilson 2014). general statistical usage, correlation\nco-relation refers departure two variables \nindependence. broad sense several coefficients,\nmeasuring degree correlation, adapted nature data. \nbest known Pearson product-moment correlation coefficient\n(\\(\\rho\\)), used linearly related variables obtained\ndividing covariance two variables (\\(\\sigma_{xy}\\)) \nproduct standard deviations (Equation (7.1).\\[\\begin{equation}\n\\rho=\\frac{\\sigma_{xy}}{\\sigma_x*\\sigma_y}\n\\tag{7.1}\n\\end{equation}\\]second measure Spearman’s rank correlation coefficient\n(\\(\\rho_{R(x),R(y)}\\)) nonparametric measure rank\ncorrelation defined Pearson correlation coefficient rank\nvariables (Equation (7.2)).\\[\\begin{equation}\n\\rho_{R(x),R(y)}=\\frac{cov{(R(x),R(y))}}{\\sigma_{R(x)}*\\sigma_{R(y)}}\n\\tag{7.2}\n\\end{equation}\\]Pearson’s correlation assesses linear relationships, Spearman’s\ncorrelation assesses monotonic relationships (whether linear ). \n, thus, fundamental theoretical assumption taken choosing \nright measure. Two variables \\(x\\) \\(y\\) positively correlated \n\\(x\\) increases \\(y\\) increases \\(x\\) decreases \\(y\\)\ndecreases . correlation instead negative variable \\(x\\)\nincreases variable \\(y\\) decreases vice-versa. sign \n\\(\\rho\\) depends covariance (\\(\\sigma_{xy}\\)). correlation\ncoefficient varies -1 (perfect negative linear correlation) \n1 (perfect positive linear correlation), equal zero \nvariables independent.code allows compute correlation coefficient \ntwo variables, correlation matrix, table showing\ncorrelation coefficients pair variables. Note \nfirst line code suppresses scientific notation, allowing \nresults future calculation expressed decimals even \nreally really small (big) values.correlation matrix may result dispersive difficult \nstudy, especially high number variables, \npossibility visualize . fact, variables correlated,\n“scatter” points trend known: trend linear \ncorrelation linear.code provides examples. function pairs()\ninternal R gives us basic graph, function\ncorrplot() belongs corrplot package provides \nstylish customizable graph (Wei Simko 2021). Finally, interesting\n(advanced) version pair plot offered GGally package\nfunction ggpairs()(Emerson et al. 2012). function allows us \ndraw correlation matrix can include whatever kind value \ngraph want inside cell.last measure relationship talk chi-squared\ntest (also known \\(\\chi^2\\) test). hypothesis test\nstatistics comes play dealing contingency tables \nrelatively large sample sizes. simpler terms, chi-squared test \nprimarily employed assess whether ’s relationship two\ncategorical variables terms impact test\nstatistic. purpose test evaluate likely \nobserved frequencies assuming null hypothesis (\\(H_0\\)) \ntrue. Test statistics follow \\(\\chi^2\\) distribution occur \nobservations independent.code computes \\(\\chi^2\\) test number cilinders \ncar presence manual transmission. Since get p-Value\nless significance level 0.05, reject null hypothesis\nconclude two variables fact dependent.Cramér’s V (sometimes referred Cramer’s \\(\\phi\\)). \nmeasure association two categorical variables (\ncategorical) based Pearson’s \\(\\chi^2\\) statistic published\nHarald Cramér 1946. Cramér’s V, gives us method can used\nwant study intercorrelation two discrete\nvariables, may used variables two levels. \nvaries 0 (corresponding association variables) \n1 (complete association) can reach 1 variable \ncompletely determined . Thus tell us \ndirection association.   ","code":"\n# simple covariance\ncov(mtcars$hp, mtcars$mpg)\n# suppress scientific notation\noptions(scipen = 9999)\n\n# Correlation coefficients\ncor(mtcars$mpg, mtcars$disp, method = \"pearson\")\ncor(mtcars$mpg, mtcars$disp, method = \"spearman\")\n\n# Pearson correlation matrix\ncor(mtcars)\n# basic pair plot\npairs(mtcars)\n\n# the corrplot version\nlibrary(corrplot)\ncorrplot(cor(mtcars))\n\n# the ggally version\nlibrary(GGally)\nggpairs(mtcars)\n# H0: The two variables are independent.\n# H1: The two variables relate to each other.\nchisq.test(mtcars$cyl, mtcars$am)\nlibrary(DescTools)\nCramerV(mtcars$cyl, mtcars$am)"},{"path":"bivariate-analysis.html","id":"linear-regression","chapter":"7 Bivariate Analysis","heading":"7.2 Linear Regression","text":"Linear regression examines relation dependent variable\n(response variable) specified independent variables (explanatory\nvariables). mathematical model relationship \nregression equation. dependent variable modelled random\nvariable uncertainty value, given value \nindependent variable. regression equation contains estimates \none hypothesized regression parameters (“constants”). \nestimates constructed using data variables, \nsample. estimates measure relationship dependent\nvariable independent variables. also allow\nestimating value dependent variable given value \nrespective independent variable.Uses regression include curve fitting, prediction (including\nforecasting time-series data), modelling causal relationships, \ntesting scientific hypotheses relationships variables.\nHowever, must always keep mind correlation imply\ncausation. fact, study causality concerned \nstudy potential causal mechanisms variation amongst \ndata (Imbens Rubin 2015).difference correlation regression whether \nfirst \\(x\\) \\(y\\) level, latter one \\(x\\) affect\n\\(y\\), way around. important theoretical\nimplications selection \\(x\\) \\(y\\). general form \nsimple linear regression (7.3):\\[\\begin{equation}\ny=\\beta_0+\\beta_1x+e\n\\tag{7.3}\n\\end{equation}\\]\\(\\beta_0\\) intercept, \\(\\beta_1\\) slope, \\(e\\) \nerror term, picks unpredictable part dependent\nvariable \\(y\\). sometimes describe 1.1 saying \nregressing \\(y\\) \\(x\\). error term \\(e\\) usually posited \nnormally distributed. \\(x\\)’s \\(y\\)’s data quantities \nsample population question, \\(\\beta_0\\) \\(\\beta_1\\) \nunknown parameters (“constants”) estimated data.\nEstimates values \\(\\beta_0\\) \\(\\beta_1\\) can derived \nmethod ordinary least squares. method called\n“least squares”, estimates \\(\\beta_0\\) \\(\\beta_1\\) minimize\nsum squared error estimates given data set (Equation (7.4)), thus\nminimizing:\\[\\begin{equation}\nRSS=e_1^2+e_2^2+...+e_n^2\n\\tag{7.4}\n\\end{equation}\\]estimates often denoted \\(\\hat\\beta_0\\) \\(\\hat\\beta_1\\) \ncorresponding Roman letters. can shown least squares\nestimates given \\[\\begin{equation}\n\\hat\\beta_1=\\frac{\\sum_{=1}^N(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{=1}^N(x_i-\\bar{x})^2} \\\\\n\\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\bar{x}\n\\end{equation}\\]\\(\\bar{x}\\) mean (average) \\(x\\) values \\(\\bar{y}\\)\nmean \\(y\\) values.\nFigure 7.1: Plot residuals regression line.\nsaid, building first regression model, important\nidea theoretical relationship variable\nwant study. case dataset cars, know speed\nimpact breaking distance car (variable dist) \nphysics studies. can thus say dist dependent variable\n(\\(y\\)), speed independent variable (\\(x\\)).can fit model following way. Inside lm() function\nplace dependent variable ~ independent variable, comma dataset\ncontained. Note formula 1.1 lm()\nfunction written y ~ x.6 code chunk draws \nsummary plot relationship speed car \nbreaking distance.output summary regression model (Figure 7.2) must \nread following way order brief idea model\nbuilt. first step look Multiple R-squared, \nnumber tells us percentage data explained model\n(65.1% case), thus significant model . \nmodel appreciable power explain data, analyze \ncoefficients’ estimates significance level. see \nspeed p-value lower 0.001 (thus highly significant7) \none unit increase speed means 3.9 units increase distance.\nHowever, also slightly significant intercept, \\(\\beta_0\\),\nmeans factors present model\nexplain behavior dependent variable.\ntheoretical perspective makes sense, fact, type \ntires, weight car, weather conditions (etc..) \nadditional factors missing model improve \nunderstanding phenomenon.\nFigure 7.2: Linear regression model summary output.\nWhether simple linear regression useful approach predicting \nresponse basis one single independent variable, often \none independent variable (\\(x\\)) influence dependent\nvariable (\\(y\\)). Instead fitting separate simple linear regression\nmodel \\(x\\), better approach extend simple linear\nregression model. can giving independent variable\n(\\(x\\)) separate slope coefficient single model. general,\nsuppose \\(p\\) distinct independent variables (\\(x\\)). \nmultiple linear regression model takes form (7.5):\\[\\begin{equation}\ny\\approx\\beta_0+\\beta_1x_1+\\beta_2x_2...+\\beta_px_p+e\n\\tag{7.5}\n\\end{equation}\\]\\(x_p\\) represents \\(p^{th}\\) independent variable, \\(\\beta_p\\)\nquantifies association variable dependent\nvariable \\(y\\). interpret \\(\\beta_p\\) average effect \\(y\\) \none unit increase \\(x_p\\), holding independent variables\nfixed. words, still effect \nincrease one independent variable (\\(x\\)) dependent variable\n(\\(y\\)), “controlling” factors.order include independent variables model, use\nplus sign. formula R y ~ x1 + x2 + x3. \nwant use variables present dataset independent\nvariables, formula R y ~ . dot stands \n“everything else”. Another possibility interaction term.\ninteraction effect exists effect independent variable\ndependent variable changes, depending value(s) one \nindependent variables (.e. \\(y=x*z\\)). However, interaction\nterms scope manual.make example, can use dataset swiss, reports Swiss\nfertility socioeconomic data year 1888. Following \nmodels different number variables used.perform multiple linear regression, usually interested \nanswering important questions order reach goal: find \nmodel, lower number independent variables best explains\noutcome. questions :least one \\(x_1, x_2, . . . ,x_p\\) useful explaining\nindependent variable \\(y\\)?least one \\(x_1, x_2, . . . ,x_p\\) useful explaining\nindependent variable \\(y\\)?\\(x_1, x_2, . . . ,x_p\\) help explain \\(y\\), \nsubset sufficient?\\(x_1, x_2, . . . ,x_p\\) help explain \\(y\\), \nsubset sufficient?well model fit data?well model fit data?order answer questions need comparative\nanalysis models. Analysis Variance (ANOVA) consists\ncalculations provide information levels variability\nwithin regression model form basis tests significance. \ncan thus use function anova() order compare multiple\nregression models. ANOVA applied practice, actually\nbecomes variable selection method: full model significantly\ndifferent (null hypothesis rejected), variable/s added full\nmodel /considered useful prediction. Statistic textbooks\ngenerally recommend test every predictor significance.   ","code":"\n# linear regression\nfit <- lm(dist ~ speed, data = cars)\nsummary(fit)\n\nlibrary(ggplot2)\nggplot(cars, aes(dist, speed))+\n        geom_point(size=3)+\n        geom_smooth(method=\"lm\")\n# multiple linear regression\nfit2 <- lm(Fertility ~ ., data = swiss)\nsummary(fit2)\n\nfit3 <- lm(Fertility ~ Education + Agriculture, data = swiss)\nsummary(fit3)\n# ANOVA testing\nanova(fit2, fit3)"},{"path":"bivariate-analysis.html","id":"logistic-regressions","chapter":"7 Bivariate Analysis","heading":"7.3 Logistic Regressions","text":"Whether independent variables can either continuous categorical,\ndependent variable logical (1,0 TRUE,FALSE), \nrun different kind regression model: logistic model (\nlogit model). model gives us probability one event (\ntwo alternatives) taking place log-odds (logarithm \nodds) event linear combination one \nindependent variables.Using glm() function (Generalized Linear Models), argument\nfamily=\"binomial\", can fit logistic regression model.output (Figure 7.3) must interpreted follows. increase \nmiles per gallons (mpg) increases probability car \nautomatic transmission 31%, increase statistically\nsignificant (p-value<0.01). case Multiple\nR-squared assess significance model, \nlook Residual deviance tells us well response variable\ncan predicted model p predictor variables. lower \nvalue, better model able predict value response\nvariable.\nFigure 7.3: Logistic regression model summary output.\n   ","code":"\nfit4 <- glm(am ~ mpg, family=\"binomial\", mtcars)\nsummary(fit4)\n\n# plotting the logit model\nlibrary(ggplot2)\nggplot(mtcars, aes(x=mpg, y=am)) + \n  geom_point() +\n  stat_smooth(method=\"glm\", color=\"green\", se=FALSE, \n              method.args = list(family=binomial))"},{"path":"bivariate-analysis.html","id":"exercises-4","chapter":"7 Bivariate Analysis","heading":"7.4 Exercises","text":"James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). \nIntroduction Statistical Learning (Vol. 103). Springer New York.\nAvailable . Chapter 3.6 3.7\nexercises 8 15.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). \nIntroduction Statistical Learning (Vol. 103). Springer New York.\nAvailable . Chapter 3.6 3.7\nexercises 8 15.R playground,\nsection 7 - T-tests RegressionsR playground,\nsection 7 - T-tests Regressions","code":""},{"path":"multivariate-analysis.html","id":"multivariate-analysis","chapter":"8 Multivariate Analysis","heading":"8 Multivariate Analysis","text":"Often bivariate analysis enough, particularly true \nhistorical period, data availability issue anymore.\nOne common problems instead analyze big (huge)\ndataset. Multivariate analysis methods give us possibility \nsomehow reduce dimensionality data, allowing clearer\nunderstanding relationships present .   ","code":""},{"path":"multivariate-analysis.html","id":"cluster-analysis","chapter":"8 Multivariate Analysis","heading":"8.1 Cluster Analysis","text":"Cluster analysis exploratory data analysis tool solving\nclassification problems. objective sort observations \ngroups, clusters, degree association strong \nmembers cluster weak members different\nclusters. cluster thus describes, terms data collected,\nclass members belong; description may \nabstracted use particular general class type.\nCluster analysis thus tool discovery. may reveal associations\nstructure data , though previously evident, nevertheless\nsensible useful found. results cluster analysis may\ncontribute definition formal classification scheme, \ntaxonomy related animals, insects plants; suggest\nstatistical models describe populations; indicate rules\nassigning new cases classes identification diagnostic\npurposes; provide measures definition, size change \npreviously broad concepts; find exemplars represent\nclasses. Whatever business ’re , chances sooner \nlater run classification problem.Cluster analysis includes broad suite techniques designed find\ngroups similar items within data set. Partitioning methods divide\ndata set number groups predesignated user.\nHierarchical cluster methods produce hierarchy clusters small\nclusters similar items large clusters include \ndissimilar items (Abdi Williams 2010). clustering PCA seek simplify\ndata via small number summaries, mechanisms \ndifferent: PCA looks find low-dimensional representation \nobservations explain good fraction variance; clustering\nlooks find homogeneous subgroups among observations.mentioned, cluster observations data set, seek \npartition distinct groups observations within \ngroup quite similar , observations different\ngroups quite different . course, make \nconcrete, must define means two observations \nsimilar different.order define similarity observations need \n“metric”. Eucludean distance common metric used \ncluster analysis, many others exist (see help dist()\nfunction detail). also need choose algorithm \nwant apply order computer assign observations \nright cluster.Remember Clustering performed continuous scaled\ndata. variables want analyze categorical,\nuse scaled dummies. ","code":""},{"path":"multivariate-analysis.html","id":"hierarchical-clustering","chapter":"8 Multivariate Analysis","heading":"8.1.1 Hierarchical Clustering","text":"Hierarchical methods usually produce graphical output known \ndendrogram tree shows hierarchical clustering structure.\nhierarchical methods divisive, progressively divide \none large cluster comprising data two smaller clusters\nrepeat process clusters divided. \nhierarchical methods agglomerative work opposite\ndirection first finding clusters similar items \nprogressively adding less similar items items \nincluded single large cluster. Hierarchical methods \nparticularly useful limited predetermined\nnumber clusters can display similarity samples across wide\nrange scales.Bottom-agglomerative clustering common type \nhierarchical clustering, refers fact dendrogram \nbuilt starting leaves combining clusters trunk.\nmove tree, leaves begin fuse branches. \ncorrespond observations similar . move\nhigher tree, branches fuse, either leaves \nbranches. earlier (lower tree) fusions occur, \nsimilar groups observations . hand,\nobservations fuse later (near top tree) can quite\ndifferent. height fusion, measured vertical\naxis, indicates different two observations .Hierarchical clustering allows also select method want \napply. Ward’s minimum variance method aims finding compact,\nspherical clusters. complete linkage method finds similar\nclusters. single linkage method (closely related \nminimal spanning tree) adopts ‘friends friends’ clustering\nstrategy. methods can regarded aiming clusters \ncharacteristics somewhere single complete link methods.first step order proceed hierarchical cluster analysis \ncompute “distance matrix”, represents distance \nobservations among . step, mentioned , \nEuclidean distance one common metrics used. \nproperly run hierarchical cluster analysis (function hclust())\nspecifying method complete linkage. Finally plot \ndendogram. important row dataset name\nassigned (see Row Names).\nFigure 8.1: Dendogram plot.\nlast line code adds rectangles highlighting 3 clusters. \nnumber cluster personal choice, strict rule \nidentify . common rule thumb look height\n(vertical axes dendogram) cut highest jump\noccurs branches. case corresponds 3 clusters.agglomerative nature, clusters sensitive order\nsamples join, can cause samples join grouping \nactually belong. words, groups known\nbeforehand, groupings may produced cluster\nanalysis. Cluster analysis sensitive distance metric\nselected criterion determining order clustering.\nDifferent approaches may yield different results. Consequently, \ndistance metric clustering criterion chosen carefully. \nresults also compared analyses based different metrics\nclustering criteria, ordination, determine \nrobustness results.Caution used defining groups\nbased cluster analysis, particularly long stems present.\nEven data form cloud multivariate space, cluster analysis\nstill form clusters, although may meaningful natural\ngroups. , generally wise compare cluster analysis \nordination evaluate distinctness groups multivariate\nspace. Transformations may needed put samples variables \ncomparable scales; otherwise, clustering may reflect sample size \ndominated variables large values. ","code":"\n# Euclidean distance\ndist <- dist(swiss, method=\"euclidean\")\n# Hierarchical Clustering with hclust\nhc <- hclust(dist, method=\"complete\")\n# Plot the result\nplot(hc, hang=-1, cex=.5)\nrect.hclust(hc, k=3, border=\"red\")"},{"path":"multivariate-analysis.html","id":"k-means-clustering","chapter":"8 Multivariate Analysis","heading":"8.1.2 K-Means clustering","text":"K-means clustering simple elegant approach partitioning \ndata set K distinct, non-overlapping clusters. perform K-means\nclustering, must first specify desired number clusters\nK; K-means algorithm assigns observation \nexactly one K clusters. idea behind K-means clustering \ngood clustering one within-cluster variation \nsmall possible. K-Means algorithm, iteratively way,\ndefines centroid cluster, point (imaginary \nreal) center cluster, adjusts \npossible change anymore. metric used Squared Sum Euclidean\ndistances. main limitation K-means understanding \nright k prior analysis. Also, K-means \nalgorithm tends perform well spherical clusters, \nlooks centroids.function kmeans() allows run K-Means clustering given \npreferred number clusters (centers). results can appreciated\nplotting clusters using fviz_cluster() function \npackage factoextra (Kassambara Mundt, n.d.). Note order plot \nclusters K-means function automatically reduces \ndimensionality data via PCA selects first two\ncomponents.\nFigure 8.2: K-means clustering.\n ","code":"\n# calculate the k-means for the preferred number of clusters\nkc <- kmeans(swiss, centers=3)\nlibrary(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_cluster(list(data=swiss, cluster=kc$cluster))"},{"path":"multivariate-analysis.html","id":"the-silhouette-plot","chapter":"8 Multivariate Analysis","heading":"8.1.3 The silhouette plot","text":"Silhouette analysis can used study separation distance \nresulting clusters. analysis usually carried prior \nclustering algorithm (Syakur et al. 2018). fact, silhouette plot\ndisplays measure close point one cluster points\nneighboring clusters thus provides way assess parameters\nlike number clusters visually. measure range \\(-1, 1\\).\nvalue k maximizes silhouette width one\nminimizing distance within clusters maximizing distance\n. However, important remark silhouette\nplot analysis provides just rule thumb cluster\nselection.case swiss dataset, silhouette plot suggests \npresence two clusters using hierarchical clustering \nK-Means. seen previously properly true.\nFigure 8.3: left: Hierarchical clustering silhouette plot; K-means clustering silhouette plot.\n   ","code":"\n#silhouette method\nlibrary(factoextra) \nfviz_nbclust(swiss, FUN = hcut)\nfviz_nbclust(swiss, FUN = kmeans)"},{"path":"multivariate-analysis.html","id":"heatmap","chapter":"8 Multivariate Analysis","heading":"8.2 Heatmap","text":"Heatmap two-way display data matrix individual\ncells displayed colored rectangles. color cell \nproportional position along color gradient. Usually, \ncolumns (variables) matrix shown columns heat\nmap rows matrix shown rows heat map, \nexample . order rows determined performing\nhierarchical cluster analyses rows (even possible \nappreciate corresponding dendogram side heatmap). \ntends position similar rows together plot. order \ncolumns determined similarly. Usually, clustered Heatmap made \nvariables similar scales, scores tests. \nvariables different scales, data matrix must first scaled\nusing standardization transformation z-scores proportion \nrange.heatmap can see V. Geneve proper outlier \nterms Education share people involved agricultural\nsector. advanced Heatmaps, please visit link.\nFigure 8.4: Heatmap.\n   ","code":"\n#heatmap\ndataMatrix <- as.matrix(swiss)\nheatmap(dataMatrix, cexCol=.8)"},{"path":"multivariate-analysis.html","id":"principal-component-analysis","chapter":"8 Multivariate Analysis","heading":"8.3 Principal Component Analysis","text":"Principal Component Analysis (PCA) way identifying patterns \ndata, expressing data way highlight \nsimilarities differences Jolliffe Cadima (2016). Since patterns\ndata can hard find data high dimension, luxury\ngraphical representation available, PCA powerful tool \nanalysing data. main advantage PCA \nfound patterns data, compress data (ie. reducing\nnumber dimensions) without much loss information.goal PCA reduce dimensionality data \nretaining much possible variation present \ndataset.PCA :statistical technique used examine interrelations among \nset variables order identify underlying structure \nvariables.non-parametric analysis answer unique independent\nhypothesis data distribution.two properties can regarded weaknesses well strengths.\nSince technique non-parametric, prior knowledge can \nincorporated. Moreover, PCA data reduction often incurs loss \ninformation.assumptions PCA:Linearity. Assumes data set linear\ncombinations variables.importance mean covariance. \nguarantee directions maximum variance contain good\nfeatures discrimination.large variances important dynamics. Assumes\ncomponents larger variance correspond interesting\ndynamics lower ones correspond noise.first principal component can equivalently defined direction\nmaximizes variance projected data. second \nrepresent direction maximizes variance projected\ndata, given first component, thus uncorrelated \n. components. computed \nprincipal components, can plot order \nproduce low-dimensional views data. generally, \ninterested knowing proportion variance explained \nprincipal component analyse ones maximize .important remember PCA performed continuous\nscaled data. variables want analyze \ncategorical, use scaled dummies correspondence analysis.\nAnother fundamental aspect row dataset must \nname assigned , otherwise see names corresponding\nobservation plot. See Scaling data Row Names \ninformation procedure.Using codes , able reduce dimensionality \nswiss dataset. dataset presents percentage values, thus \nvariables already continuous scale. Moreover,\nobservation (village) row named accordingly, \nneed transformation prior analysis. One sure\ntwo aspect, can start analysis studying \ncorrelation different variables compose dataset. \nknow PCA works best correlated\nvariables can “grouped” within principal component \nalgorithm.next step properly run PCA’s algorithm assign \nobject. values scale, better set \nargument scale equal TRUE. argument sets PCA work \ncorrelation matrix, instead covariance matrix, allowing \nstart values centered around 0 scale. \nobject created prcomp() “list”. list can contain dataframes,\nvectors, variables, etc… order explore inside list\ncan use $ sign [] (nested square brackets). \nsummary scree plot (command fviz_eig() package\nfactoextra) first thing look tell us \nmuch variance explained component (Kassambara Mundt, n.d.). \nhigher first components, accurate PCA . \ncase, first two components retain 73.1% total\nvariability within data.\nFigure 8.5: Screeplot.\nfinal step plot graph variables, positively\ncorrelated variables point side plot, \nnegatively correlated variables point opposite sides graph. \ncan see Education positively correlated PC2, \nFertility Catholic negatively correlated dimension\nthus also Education. result confirms already saw\ncorrelation matrix .graph individuals, instead, tells us observations (\nvillages case) related components. Thus, can\nconclude saying V. de Geneve peculiar characteristics\ncompared villages, fact highest\neducation level lowest fertility share catholic people.biplot overlays previous two graphs allowing immediate\ninterpretation. However many variables observations, \nplot can messy analyzed.\nFigure 8.6: top-left clockwise: Graph variables, positive correlated variables point side plot; Graph individuals, individuals similar profile grouped together; Biplot individuals variables.\nrobustness check, also better understand algorithm\n, can compare rotation axis pca\nlooking pairs plot. pair graph PCA expect \nsee relationship principal component, \naim PCA algorithm.\nFigure 8.7: top: Pairs graph PCA; Pairs graph PCA.\n   ","code":"\n# Correlation Matrix\ncor(swiss)\nlibrary(factoextra)\n#running the PCA\npca_swiss <- prcomp(swiss, scale = TRUE)\nsummary(pca_swiss)\n\n#visualizing the PCA\nfviz_eig(pca_swiss)\n# Graph of variables\nfviz_pca_var(\n        pca_swiss,\n        col.var = \"contrib\",\n        repel = TRUE     # Avoid text overlapping\n)\n\n# Graph of individuals\nfviz_pca_ind(\n        pca_swiss,\n        col.ind = \"cos2\",\n        repel = TRUE\n)\n\n# Biplot of individuals and variables\nfviz_pca_biplot(pca_swiss, repel = TRUE)\n# Pairs before PCA\npairs(swiss, panel=panel.smooth, col=\"#6da7a7\")\n\n# Pair after PCA\npairs(pca_swiss$x, panel=panel.smooth, col=\"#6da7a7\")"},{"path":"multivariate-analysis.html","id":"classification-and-regression-trees","chapter":"8 Multivariate Analysis","heading":"8.4 Classification And Regression Trees","text":"Classification Regression Trees (CART) simple useful methods\ninterpretation allow understand underlying relationship\none dependent variable multiple independent variables\nTemkin et al. (1995). compared multiple linear regression\nanalysis, set methods retrieve impact one\nvariable outcome controlling set independent\nvariables. instead recursively looks significant\nrelationship set variables, subsets given data\naccordingly, finally draws tree. CART great tool \ncommunicating complex relationships thanks visual output,\nhowever generally poor predicting performance.Depending dependent variable type possible apply \nClassification (discrete variables) Regression (continuous\nvariables) Tree. order build regression tree, algorithm\nfirst uses recursive binary splitting grow large tree, stopping\nterminal node fewer minimum number \nobservations. Beginning top tree, splits data \n2 branches, creating partition 2 spaces. carries \nparticular split top tree multiple times chooses \nsplit features minimizes Residual Sum Squares (RSS).\nrepeats procedure subsequent split. \nclassification tree, instead, built predicting \nobservation belongs commonly occurring class region\nbelongs. However, classification setting, RSS \nused criterion making binary splits. algorithm \nuses Classification Error Rate, Gini Index Cross-Entropy.interpreting results classification tree, often\ninterested class prediction corresponding \nparticular terminal node region, also class proportions among\ntraining observations fall region. image \noffers clear understanding classification tree must read\n(J. Lee 2018). first state research question. answer proposed\ndepend variables included data. case \naccept new job offer salary higher $50k, \ntakes less one hour commute, company offers free coffee.\nFigure 8.8: Classification tree explanation. Source Lee (2018).\nmain question stop splitting? Clearly, \nelements node class us much good \nadd another split. usually decrease power \nmodel. known overfitting. omniscient\nstatisticians, creative rules termination. \nfact, one-size-fits--rule case, algorithm\nprovides number parameters can set. process called\n“pruning”, pruning tree make smaller simpler.Manual pruning, performed starting fully grown (-fitted)\ntrees setting parameters minimum number observations\nmust exist node order split attempted\n(minsplit), minimum number observations terminal node\n(minbucket), maximum depth node final tree, \nroot node counted depth 0 (maxdepth), just mention \nimportant ones. rule setting parameters, \ncomes art statistician.Automatic pruning, instead, done setting complexity\nparameter (cp). complexity parameter combination size\ntree ability tree separate classes \ntarget variable. next best split growing tree \nreduce tree’s overall complexity certain amount, \nprocess terminated. complexity parameter default set \n0.01. Setting negative amount ensures tree \nfully grown. right value complexity parameter?\nAlso case, perfect rule. rule thumb \nset zero, select complexity parameter minimizes\nlevel cross-validated error rate.example , use dataset ptitianic, \npackage rpart.plot. dataset provides list passengers board\nfo famous ship Titanic sank North Atlantic Ocean 15\nApril 1912. tells us whether passenger died survived, \npassenger class, gender, age, number sibling spouses aboard,\nnumber parents children aboard. aim understand\nfactors allowing passenger survive.package rpart allows us run CART algorithms\n(Therneau Atkinson 2022). rpart() function needs specification \nformula using syntax used multiple linear regressions, \nsource data, method (y survival object, \nmethod = \"exp\", y 2 columns method = \"poisson\", y\ncategorical method = \"class\", otherwise method = \"anova\").\ncode , argument method = \"class\" used given \noutcome variable categorical variable. important set\nseed working rpart want coherent results,\nruns random sampling.fit object fully grown tree (cp<0). create fit2,\ntree manually pruned setting parameters mentioned\n. Remember required set parameters, one\nenough. Finally, fit3 automatically pruned\ntree. functions printcp(fit) plotcp(fit), allow us \nvisualize cross-validated error rate fully grown tree\n(fit), can select value complexity parameter\nminimize value. case, pick “elbow” \ngraph, thus cp=0.094. order apply new complexity parameter\nfully grown tree, either grow tree done \nfit, use function prune.rpart().plot tree using fancyRpartPlot() package rattle\n(Williams 2011). graph produced displays number node \ntop node, predicted class (yes ), number \nmiss-classified observations, percentage observations \npredicted class node. see , case, pruning\nusing automatic method retrieved poor tree one split,\nwhether manually pruned tree richer allows us interpret\nresult.\nFigure 8.9: left: Complexity vs X-val Relative Error; automatically pruned CART.\n\nFigure 8.10: manually pruned CART.\nwant give interpretation manually pruned tree can\nsay following. probability dying board Titanic \n83% passenger male, older 9.5 years old, \nhappened 61% passengers aboard. contrary, \n93% chances surviving woman passenger class\ndifferent 3rd. statistic applies 19% passengers\nabroad.algorithm growing decision tree example recursive\npartitioning. recursive binary splitting approach top-\ngreedy. Top-begins top tree (\npoint observations belong single “region”) \nsuccessively splits independent variable’ space; split \nindicated via two new branches tree. greedy\nstep tree-building process, best split \nterms minimum RSS made particular step, rather \nlooking ahead picking split lead better tree \nfuture step. node tree grown using set \nrules parent node.much powerful use CART (less interpretable) \nensemble . ensemble method approach \ncombines many simple “building ensemble block” models (case\ntrees) order obtain single potentially powerful model.\nexamples Bagging, Random Forest, Boosting. However, \nmethodologies scope book.   ","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(rattle)\n\n# set the seed in order to have replicability of the model\nset.seed(123, kind = \"Mersenne-Twister\", normal.kind =  \"Inversion\")\n\n# fully grown tree\nfit <- rpart(\n        as.factor(survived) ~ ., \n        data = ptitanic, \n        method = \"class\",\n        cp=-1\n)\n\n# manually pruned tree\nfit2 <- rpart(\n        as.factor(survived) ~ ., \n        data = ptitanic, \n        method = \"class\",\n        # min. n. of obs. that must exist in a node in order for a split \n        minsplit = 2, \n        # the minimum number of observations in any terminal node\n        minbucket = 10, \n        # max number of splits\n        maxdepth = 5\n)\n\n# automatically pruned tree\n# printing and plotting the cross-validated error rate\nprintcp(fit)\nplotcp(fit)\n\n# pruning the tree accordingly by setting the cp=cpbest\nfit3 <- prune.rpart(fit, cp=0.094)\n\n# plotting the tree\nfancyRpartPlot(fit2, caption = \"Classification Tree for Titanic pruned manually\")\nfancyRpartPlot(fit3, caption = \"Classification Tree for Titanic pruned automatically\")"},{"path":"multivariate-analysis.html","id":"composite-indicators","chapter":"8 Multivariate Analysis","heading":"8.5 Composite Indicators","text":"composite indicator formed individual indicators combined\nsingle index, based underlying model \nmultidimensional concept measured. indicators \nmake composite indicator referred components component\nindicators, variability represents implicit weight \ncomponent within final composite indicator Mazziotta Pareto (2020). One common advanced methods build \ncomposite indicator use scoring system, flexible\neasily interpretable measure Mazziotta Pareto (2020).\ncan expressed relative absolute measure, depending \nmethod chosen, , cases, composite index can compared\ntime. examples Mazziotta-Pareto Index (relative\nmeasure), Adjusted Mazziotta-Pareto Index (absolute\nmeasure)(Mazziotta Pareto 2020), arithmetic mean z-scores (relative\nmeasure), arithmetic mean Min-Max (OECD JRC 2008). Scores\ncan also clustered classified ranking regions.\nFurthermore, scoring system allows use components \noriginal form, thus keeping eventual weighting balancing \n. Finally, reference value facilitates interpretation \nscores (.e., score 90 given average 100 means \nregion 10 points average). caveat can find \nlack unit measure final indicator thus \nimpossibility meaningful single value world\naggregation. ","code":""},{"path":"multivariate-analysis.html","id":"mazziotta-pareto-index","chapter":"8 Multivariate Analysis","heading":"8.5.1 Mazziotta-Pareto Index","text":"Mazziotta–Pareto index (MPI) composite index summarizing \nset individual indicators assumed fully\nsubstitutable. based non-linear function , starting \narithmetic mean normalized indicators, introduces penalty\nunits unbalanced values indicators\n(De Muro, Mazziotta, Pareto 2011). MPI best solution static analysis.Given matrix \\(Y=y_{ij}\\) \\(n\\) rows (statistical units) \\(m\\)\ncolumns (individual indicators), calculate normalized matrix (8.1)\n\\(Z=z_{ij}\\) follows:\\[\\begin{equation}\nz_{ij}=100\\pm\\frac{y_{ij}-M_{y_j}}{S_{y_j}}*10\n\\tag{8.1}\n\\end{equation}\\]\\(M_{y_j}\\) \\(S_{y_j}\\) , respectively, mean standard\ndeviation indicator \\(j\\) sign \\(\\pm\\) ‘polarity’ \nindicator \\(j\\), .e., sign relation indicator\n\\(j\\) phenomenon measured (\\(+\\) individual indicator\nrepresents dimension considered positive \\(-\\) represents \ndimension considered negative).aggregate normalized data. Denoting \n\\(M_{z_i},S_{z_i},cv_{z_i}\\), respectively, mean, standard deviation,\ncoefficient variation normalized values unit \\(\\),\ncomposite index given (8.2):\n\\[\\begin{equation}\nMPI^\\pm_i= M_{z_i}*(+cv^2_{z_i})=M_{z_i}\\pm S_{z_i}*cv_{z_i}\n\\tag{8.2}\n\\end{equation}\\]sign \\(\\pm\\) depends kind phenomenon \nmeasured. composite index ‘increasing’ ‘positive’, .e.,\nincreasing values index correspond positive variations \nphenomenon (e.g., socio-economic development), \\(MPI^-\\) used. \ncontrary, composite index ‘decreasing’ ‘negative’,\n.e., increasing values index correspond negative variations\nphenomenon (e.g., poverty), \\(MPI^+\\) used. cases,\nunbalance among indicators negative effect value\nindex.example , usa dataset present Compind\npackage (Fusco, Vidoli, Sahoo 2018). first input rownames (needed \nnames countries final table), state index\npolarity. normalize data using function\nnormalise_ci selecting variables interest, polarity \nrespect phenomeon interest, method use (see \nhelp available methods). finally compute MPI using \nfunction ci_mpi specifying penality. ","code":"\nlibrary(Compind)\n# loading data\ndata(EU_NUTS1)\n\n# inputting rownames\nEU_NUTS1 <- data.frame(EU_NUTS1)\nrownames(EU_NUTS1) <- EU_NUTS1$NUTS1\n\n# Unsustainable Transport Index (NEG)\n\n# Normalization (roads are negative, trains are positive)\ndata_norm <- normalise_ci(EU_NUTS1,c(2:3),\n                          # roads are negative, railrads positive\n                          polarity = c(\"NEG\",\"POS\"),\n                          # z-score method\n                          method = 1)\n\n# Aggregation using MPI (the index is negative)\nCI <- ci_mpi(data_norm$ci_norm, \n             penalty=\"NEG\")\n\n# Table containing Top 5 Unsustainable Transport Index\npander(data.frame(AMPI=head(sort(CI$ci_mpi_est, decreasing = T),5)), \n       caption = \"Top 5 unsustainable Index\")\n\n# Table containing Bottom 5 Unsustainable Transport Index (aka most sustainable)\npander(data.frame(AMPI=sort(tail(sort(CI$ci_mpi_est, decreasing = T),5))), \n       caption = \"Bottom 5 unsustainable Index\")"},{"path":"multivariate-analysis.html","id":"adjusted-mazziotta-pareto-index","chapter":"8 Multivariate Analysis","heading":"8.5.2 Adjusted Mazziotta-Pareto Index","text":"study consider composite indicator Adjusted\nMazziotta Pareto Index (AMPI) methodology. AMPI non-compensatory\n(partially compensatory) composite index allows \ncomparability data units time\n(Mazziotta Pareto 2016). variant MPI, based \nrescaling individual indicators using Min-Max transformation\n(De Muro, Mazziotta, Pareto 2011).apply AMPI, original indicators normalized using Min-Max\nmethodology goalposts. compared common MPI, \nMin-Max normalization technique enables us compare data time,\nwhereas z-score normalization used MPI . Given matrix\n\\(X=\\{x_{ij}\\}\\) \\(n\\) rows (units) \\(m\\) columns (indicators), \ncalculate normalized matrix \\(R=\\{r_{ij}\\}\\) follows (8.3):\n\\[\\begin{equation}\nr_{ij}=\\frac{x_{ij} -\\min x_j}{\\max x_j - \\min x_j} *60+70\n\\tag{8.3}\n\\end{equation}\\]\\(x_{ij}\\) value indicator \\(j\\) unit \\(\\) \n\\(\\min x_j\\) \\(\\max x_j\\) ‘goalposts’ indicator \\(j\\). \nindicator \\(j\\) negative polarity, complement \\((1)\\) \ncalculated respect \\(200\\). facilitate interpretation \nresults, ‘goalposts’ can fixed 100 represents reference\nvalue (e.g., average given year). simple procedure \nsetting ‘goalposts’ following. Let \\(\\inf x_j\\) \\(\\sup x_j\\)\noverall minimum maximum indicator \\(j\\) across units\ntime periods considered. Denoting \\(\\text{ref } x_j\\) \nreference value indicator \\(j\\), ‘goalposts’ defined (8.4):\\[\\begin{equation*}\n    \\begin{cases}\n      \\min x_j= \\text{ref } x_j - (\\sup x_j - \\inf x_j)/2\\\\\n      \\max x_j= \\text{ref } x_j + (\\sup x_j - \\inf x_j)/2\n    \\end{cases}\\\n\\tag{8.4}\n\\end{equation*}\\]\\end{equation*}normalized values fall approximately range (70; 130),\n100 represents reference value.normalized indicators can aggregated. Denoting \n\\(M_{r_i}\\) \\(S_{r_i}\\), respectively, mean standard deviation\nnormalized values unit \\(\\), generalized form AMPI\ngiven (8.5):\\[\\begin{equation}\nAMPI_i^{+/-}=M_{r_i} \\pm S_{r_i}*cv_i\n\\tag{8.5}\n\\end{equation}\\]\\(cv_i=\\frac{S_{r_i}}{M_{r_i}}\\) coefficient variation \nunit \\(\\) sign \\(\\pm\\) depends kind phenomenon \nmeasured. composite index increasing positive, ,\nincreasing index values corresponds positive variations \nphenomenon (example, well-), \\(AMPI^-\\) used. Vice versa,\ncomposite index decreasing negative, , increasing\nindex values correspond negative variations phenomenon (e.g.\nwaste), \\(AMPI^+\\) used. approach characterized use\nfunction (product \\(S_{r_i}*cv_i\\)) penalize units \nunbalanced values normalized indicators. ‘penalty’ based\ncoefficient variation zero values equal. \npurpose favor units , mean equal, greater\nbalance among different indicators. Therefore, AMPI characterized\ncombination ‘mean effect’ (\\(M_{r_i}\\)) ‘penalty effect’\n(\\(S_{r_i}*cv_i\\)) unit stands relation ‘goalposts’.example , usa dataset present Compind\npackage (Fusco, Vidoli, Sahoo 2018). reshape data long format, allowing us \ndataset one variable per column. use function\nci_ampi standardize data compite score \ntime. function set dataset, variables interest,\nvalues corresponding variables used reference\n(goalposts), time variable, polarity variables \npenality corresponding final indicator.   ","code":"\n# loading data\ndata(EU_2020)\n\n# Sustainable Employment Index (POS)\n\n# subsetting interesting variables\ndata_test <- EU_2020[,c(\"geo\",\"employ_2010\",\"employ_2011\",\"finalenergy_2010\",\n                        \"finalenergy_2011\")] \n\n# reshaping to long format\nEU_2020_long <- reshape(data_test, \n                       #our variables\n                      varying=c(2:5), \n                      direction=\"long\", \n                      #geographic variable\n                      idvar=\"geo\", \n                      sep=\"_\")\n\n# normalization and aggregation using AMPI\nCI <- ci_ampi(EU_2020_long, \n              #our variables\n              indic_col=c(3:4),\n              #goalposts\n              gp=c(50, 100), \n              #time variable\n              time=EU_2020_long$time,\n              #both variables are positive\n              polarity= c(\"POS\", \"POS\"),\n              #index is positive\n              penalty=\"POS\")\n\n# Table containing the Sustainable Employment Index scores\npander(data.frame(t(CI$ci_ampi_est)), caption = \"AMPI time series\")"},{"path":"multivariate-analysis.html","id":"exercises-5","chapter":"8 Multivariate Analysis","heading":"8.6 Exercises","text":"R playground,\nsection 6 - PCA ClusteringR playground,\nsection 6 - PCA ClusteringR playground,\nsection 8 - Classification Regression TreesR playground,\nsection 8 - Classification Regression Trees","code":""},{"path":"final-remarks.html","id":"final-remarks","chapter":"Final Remarks","heading":"Final Remarks","text":"point expect familiar Rish language\ndynamics. manual covered basic data analysis methods,\nmuch learn. Now time \nexplore new packages, new ways writing code, new statistical\ntechniques.curious fun!Federico Roscioli","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"Bibliography","heading":"Bibliography","text":"","code":""}]
